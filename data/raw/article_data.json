[
  {
    "title": "issue 294",
    "date": null,
    "url": "https://www.deeplearning.ai/the-batch/issue-294/",
    "content": "The Batch\nWeekly Issues\nissue 294\nPublishedMar 26, 2025\nPublished\n\nPublished\nMar 26, 2025\nReading time14min read\nReading time\n\nReading time\n14min read\nPublishedMar 26, 2025Reading time14min readShareLoading theElevenlabs Text to SpeechAudioNative Player...Dear friends,Fine-tuning small language models has been gaining traction over the past half year. I’d like to share my sense of when to use this technique, and also when not to, based on what I’m seeing in multiple companies.First, while fine-tuning is an important and valuable technique, many teams that are currently using it probably could get good results with simpler approaches, such as prompting (including writingmega prompts), few-shot prompting, or simple agentic workflows.Why shouldn’t these teams be fine-tuning? Because fine-tuning, which takes a pre-trained model and further trains it on data specific to an application, is relatively complex to implement. You need to collect training data, then (unless you want to implement fine-tuning yourself) find a provider to help with running fine-tuning, then find a way to deploy the fine-tuned model. Because it adds extra complexity both in training and deployment, usually I resort to this technique only after I find that prompting and simple agentic workflows are not up to a task.Having said that, there are also applications where fine-tuning is appropriate and valuable. LoRA (which learns by modifying a limited number of parameters rather than the entire model) and related methods have made fine-tuning quite affordable, particularly for small models (say, 13B or fewer parameters). And the amount of data needed to get started is less than most people think. Depending on the application, I’ve seen good results with 100 or even fewer examples. Here are a few applications where I have seen fine-tuning applied successfully:Improving accuracy of critical applications.Prompting can get you really far for many applications. But sometimes, fine-tuning helps eke out that last bit of accuracy. For example, if you are building a customer service chatbot and need it to call the right API reliably (say, to carry out transactions, issue refunds, and the like), perhaps prompting can get it to make the right API call 95% of the time. But if you struggle to raise the accuracy even with revisions to the prompt and you really need 99% accuracy, fine-tuning on a dataset of conversations and API calls might be a good way to get you there. This is particularly true for tasks where it's hard to specify, using only language, an unambiguous rule to decide what to do. For example, when a customer is frustrated, should the chatbot escalate to a manager or just issue a refund? Teams often write Standard Operating Procedures (SOPs) for human workers to follow, and these SOPs can go into the prompts of models. But if it is hard to specify an unambiguous SOP, so even humans need to see numerous examples before they can learn what to do, fine-tuning can be a good approach. For many text-classification applications fine-tuning also works well, for example, classifying medical records into diagnosis and procedure codes for health insurance claims.Learning a particular  style of communication.As I explain in “Generative AI for Everyone,” my team fine-tuned a model to sound like me. Many people (including myself) have idiosyncratic uses of language. There are certain words I tend to say and others I tend not to, and these idiosyncrasies are numerous and very difficult to specify in a text prompt. (By the way, the avatar at deeplearning.ai/avatar, built with RealAvatar, uses fine-tuning for this reason.) To get a system to communicate in a certain style, fine-tuning is often a superior solution to prompting alone.Reducing latency or cost during scale-ups.I’ve seen applications where developers have successfully prompted a large model to perform a complex task. But as usage scales up, if the large model is too slow (which often happens) or too expensive (which also happens but less frequently), the team might want to use a smaller model. If, however, the performance of the smaller model isn't good enough, then fine-tuning it can help bring it up to the performance of the larger one for that narrow application. Further, the larger model (or perhaps an agentic workflow) can also be used to generate data to help with fine-tuning the small model for that task.At the cutting edge of research, some teams are fine-tuning models to get better at a certain language. But with few exceptions, if the goal is to get an LLM to better understand a body of knowledge that is not in its training data, I find that using RAG (retrieval augmented generation) is a much simpler approach, and I still occasionally run into teams using fine-tuning for which  I think RAG would work better.Overall my sense is that, of all the teams I see using fine-tuning, perhaps 75% could get good results using simpler techniques (like prompting or agentic workflows), but in 25% of cases I know of no better way to achieve their goal.It is still technically challenging to implement fine-tuning, get the hyperparameters right, optimize the compute resources, and so on. We are lucky that more and more companies have worked hard to optimize these and provide efficient fine-tuning services. Many of them allow us to fine-tune open weights models and also download the fine-tuned weights. Some allow us to fine-tune their closed models and continue to keep the tuned weights closed. Both can be useful, but the former has obvious advantages of portability and not having to worry that the provider will stop serving a particular model, causing a critical component in our software to become deprecated.In conclusion, before fine-tuning, consider if you should be trying just a bit harder with prompting or agentic workflows, which can lead to simpler solutions that are easier to maintain. The vast majority of applications my teams build do not use any fine-tuning at all, but it’s a critical piece of a small minority of them.Keep learning!AndrewA MESSAGE FROM DEEPLEARNING.AIIn “Vibe Coding 101 with Replit,” you’ll learn to plan, prompt, and debug alongside a coding agent. Build, host, and share two real web apps in Replit’s cloud environment while developing effective development skills like writing product requirements, structuring tasks, and refining AI-generated code.Start todayNewsVision-Language, Compact and OpenGoogle updated its open-weights family of large language models to include versions that handle image and video inputs.What’s new:Google released itsGemma 3multilingual large language models with parameter counts of 1 billion, 4 billion, 12 billion, and 27 billion. While the smallest processes text only, the other three are vision-language models that are small enough to run on a consumer hardware.Input/output:Gemma 3 1B: text-in (up to 32,000 tokens), text out (up to 8,192 tokens). Gemma 3 4B, 7B, 27B: text, images/video in (up to 128,000 tokens), text out (up to 8,192 tokens). Gemma 3 27Boutputs24.61 tokens per /second, 0.68 seconds to first token.Knowledge cutoff:March 2024Architecture:Gemma 3 1B: Transformer. Gemma 3 4B, 12B, 27B: Transformer, SigLIP  vision encoder.Features:140 languages, function calling, structured output.Training data:Gemma 3 1B: 2 trillion tokens of web text, code, and mathematics. Gemma 3 4B, 12B, 27B: between 4 trillion and 14 trillion tokens of text and images.Availability/price:Weights free to download fromHugging Faceand Kaggle under alicensethat allows noncommercial and commercial uses with some restrictions. Available free via Google’s AI Studio.How it works:Gemma 3rearchitectsand refines earlier Gemma models for higher performance at lower parameter counts.To save memory, Gemma 3 interleaves five local attention layers for every global attention layer. Global attention layers attend to the entire input, while local attention layers attend to 1,024 tokens.The models were fine-tuned to encourage their outputs to match those of an unspecified larger teacher model.Gemma 3 learned via reinforcement learning in three ways. (i) The models were aligned with human preferences viareinforcement learning from human feedback(RLHF). (ii) They were fine-tuned to solve math problems via reinforcement learning, much likeDeepSeek-R1. (iii) They were trained to generate better code viareinforcement learning from execution feedback (RLEF). Specifically, over several rounds of output, RLEF tested generated code on a subset of tests, then prompted the model to fix any bugs. RLEF rewarded the models if their final output passed all tests.Performance:Gemma 3 models outperform Gemma 2 models of equal or larger size by several measures, and all sizes show a strong ability to solve mathematics word problems as measured byMATH.In Google’s tests, Gemma 3 1B performs roughly comparably to Gemma 2 2B, outperforming the larger model on LiveCodeBench (1.9 percent to 1.2 percent) and MATH (48.0 percent to 27.2 percent).Gemma 3 4B achieves roughly comparable performance to Gemma 2 9B, Llama 3.1 8B, and Qwen2.5-7B. It’s slightly behind Microsoft Phi-4 Mini (also 4 billion parameters), except on MATH, according to that company’s tests.Gemma 3 12B improves on Gemma 2 27B and compares to Gemini 1.5 Flash (in TIGER-Lab’s tests) and Anthropic Claude 3.5 Haiku (in that developer’s tests). It outperforms the larger, proprietary models on MATH.Gemma 3 27B consistently outperforms the Gemma 2 model of the same size and performs comparably to Gemini 1.5 Pro onMMLU-Pro(high-level language comprehension) 67.5 percent to 56.9 percent, onLiveCodeBench(coding) 29.7 percent to 20.4 percent, onGPQA Diamond(graduate-level domain knowledge) 42.4 percent to 34.3 percent, and on MATH 89.0 percent to 55.6 percent.Moreover, Gemma 3 27B achieves 1,338 ELO inChatbot Arena, a top-ten score that puts it ahead of OpenAI o1 and behind only DeepSeek-R1 among models with open weights.Hot on Gemma 3’s heels:Shortly after Gemma 3 became available, Mistral releasedSmall 3.1(24 billion parameters), a vision-language model with open weights, under a more permissive Apache 2.0 license.Mistral Small 3.1 is similarly multilingual and offers a 128,000 token context window.It slightly outperforms Gemma 3 27B on MMLU, MMLU-Pro, MMMU, and other selected benchmarks.It also outperforms Gemma 3 27B and other models in its size range on long-context tests. (However, Gemma 3 27B performs better in the Chatbot Arena test of human preference.)Why it matters:Gemma 3 takes advantage of a variety of techniques to raise the bar for vision-language performance in relatively small models. Knowledge distillation, multiple rounds of reinforcement learning, and fine-tuning on many languages are a powerful combination.We’re thinking:A vision-language model small enough to run on a smartphone feels increasingly close!Better Images in Fewer StepsDiffusion models usually take many noise-removal steps to produce an image, which takes time at inference. There are ways to reduce the number of steps, but the resulting systems are less effective. Researchers devised a streamlined approach that doesn’t sacrifice output quality.What’s new:Kevin Frans and colleagues at UC Berkeley introducedshortcut modelsthat learn to take larger noise-removal steps and thus require fewer steps to generate an image.Key insight:At inference, a scheduler likeEulercan enable a model to take larger steps than those it learned during training, but this approach yieldsworse performance. Alternatively distillation, in which a student model learns to remove the same amount of noise as a teacher model when it takes several steps, offers improved performance at the cost of more cumbersome development. Training the model directly to take bigger steps — that are equivalent to multiple smaller steps — enables it to maintain high performance while taking fewer steps.How it works:The authors trainedDiT-B, a diffusion transformer, to generate images like those in CelebA-HQ (celebrity faces) and ImageNet-256 (various subjects, size 256x256).The loss function included terms for flow matching and self-consistency. The flow matching term encouraged the model to learn to remove noise. The self-consistency term encouraged the model to learn how to minimize the discrepancy between the noise removed by a single big step and two smaller steps.Initially the model learned to combine two small steps into one step 2x as large. Combining two larger steps resulted in step sizes of 4x, 8x, and so on, up to 128x.At inference, the user told the model how many small steps to take, and the model computed the single-step size necessary to accomplish that.Results:The authors compared their model using 1, 4, or 128 steps to alternatives that were trained via various methods including many variants of distillation. They measured the results usingFréchet inception distance(FID), which assesses how closely generated images resemble real-world images (lower is better).On both CelebA-HQ and ImageNet-256, their model, when it took four steps, achieved the best performance. For example, on CelebA-HQ, using four steps, the shortcut model achieved 13.8 FID, while the next-best model,Reflow(another variant of distillation), achieved 18.4 FID.When it took one step, it achieved the second-best result, behindprogressive distillation, which trained a series of student models to remove the same amount of noise as a teacher model does when it takes multiple steps.Why it matters:Generating images by diffusion is typically costly, and previous approaches to cutting the cost have compromised either performance or incurred additional development expense or both. This method achieves high performance at relatively low cost.We’re thinking:As diffusion models continue to become cheaper and faster, we expect to see applications blossom!LLM Support for TutorsStudents benefit from tutoring, but training tutors is expensive. A study shows that large language models can boost tutors’ effectiveness in real time.What’s new:Rose Wang and colleagues at Stanford builtTutor CoPilot, a tool for remote, online tutors that uses GPT-4 to generate hints, explanations, questions, and other helpful responses to students.Key insight:When a student makes an error, according to previousworkby some of the same authors, effective teachers choose a strategy for addressing the mistake. The authors identified 11 strategies, such as ask a question, explain a concept, provide a hint, or encourage the student. Moreover, they found that an LLM that executed a strategy chosen by an expert teacher performed significantly better than an LLM that was prompted with a strategy chosen at random or no specific strategy. Letting inexperienced tutors choose a strategy while an LLM generates a response helps them learn how to execute the strategy. Students, in turn, benefit from responses that mimic those of an experienced teacher.How it works:The authors outfitted a remote tutoring application with GPT-4.The application included a tutor-student chat window, a problem display, and a whiteboard. The authors added a button that enabled the tutor to turn Tutor CoPilot on or off.When a tutor engaged Tutor CoPilot, the system prompted GPT-4 to behave as an experienced elementary math teacher and provided context in the form of the 10 most recent messages, the current lesson topic, and a default strategy from the list. GPT-4 responded with guidance. (To preserve the tutor’s and student’s privacy, the system redacted their names using the open source libraryEdu-ConvoKit.)The system prompted GPT-4 three times, each time changing the strategy, and presented the tutor with three potential responses.The tutor could re-generate or edit GPT-4’s responses, or select a strategy and generate a new response before adding it to the chat window.Results:The authors partnered with a virtual tutoring company and a school district in the United States for a two-month study of 874 tutors and 1,787 students between grades 3 and 8. They divided the participants into two groups. In one group, tutors conducted sessions with students as usual. In the other, tutors had access to Tutor CoPilot. The authors measured success by the percentage of students who passed a test at the end of a lesson.In the group that didn’t use Tutor CoPilot, 62 percent of students passed the test.In the group with TutorCopilot, 66 percent passed.The effect was most pronounced among the one-third of tutors who had the lowest ratings (9 percent higher) and least experience (7 percent higher).The API cost was approximately $3.31 per tutor, or roughly $20 per tutor per year.Yes, but:The authors found statistically significant improvements as measured by test results per lesson, but not in end-of-year exam results. The study’s two-month duration may account for the lack of evidence for longer-term effects.Why it matters:LLMs hold great promise for helping to educate students, but they also show potential in educating teachers. For inexperienced tutors who are learning how to interact with students, an LLM’s general knowledge and pedagogical insights gleaned from expert teachers make a powerful combination.We’re thinking:Although it relies on sophisticated technology, the authors’ approach is simple: Prompt an LLM to apply proven teaching principles. Presumably such principles apply beyond elementary math, which would make this approach useful for teaching a variety of disciplines.Faster Learning for Diffusion ModelsDiffusion transformers learn faster when they can look at embeddings generated by a pretrained model like DINOv2.What’s new:Sihyun Yu and colleagues at Korea Advanced Institute of Science and Technology, Korea University, New York University, and Scaled Foundations (a startup that builds AI for robotics) proposedRepresentation Alignment(REPA), a loss term for transformer-based diffusion.Key insight:Diffusion models learn to remove noise from images to which noise was added (and, at inference, they start with pure noise to generate a fresh image). This process can be divided into two parts: learning to (i) embed the noisy image and (ii) estimate the noise from the embedding. One way to accelerate learning is to add a loss term that encourages the diffusion model to produce embeddings that are similar to those produced by a pretrained embedding model. The diffusion model can learn to estimate the noise faster if it doesn’t need to learn how to embed an image from scratch.How it works:The authors modifiedDiT-XL/2andSiT-XL/2transformer-based latent diffusion models, a class of diffusion models that subtract noise from embeddings rather than images. They trained the models to produce images similar to ImageNet. In the process, the modified models learned to produce embeddings similar to those produced by a pretrainedDINOv2.The authors usedStable Diffusion VAE’spretrained encoder to embed an image.Given the embedding with noise added, the diffusion model learned to remove the noise according to the usual loss term.It also learned according to the REPA loss. Specifically, it learned to maximize the cosine similarity between a specially processed version of its eighth-layer embedding and the embedding produced by a pretrained DINOv2. To process its eighth-layer embedding for the REPA loss, the diffusion model fed the embedding to a vanilla neural network.At inference, given pure noise, the model removed it over several steps to produce an image embedding. Stable Diffusion VAE’s decoder converted the embedding into an image.Results:The modified DiT-XL/2 learned significantly faster than the unmodified version.In 400,000 training steps, the modified model reached 12.3Fréchet inception distance(FID) (which measures similarity between generated and non-generated images, lower is better), while the unmodified version reached 19.5 FID.The models continued to learn at different speeds as training continued. The modified DiT-XL/2  took 850,000 training steps to reach 9.6 FID, while the unmodified version took 7 million steps to reach the same number.Experiments with modified and unmodified versions of SiT-XL/2 yielded similar results.Trained to convergence, the modified models outperformed the unmodified versions. For instance, the modified  SiT-XL/2 achieved 5.9 FID (after 4 million training steps), while the unmodified version achieved 8.3 FID (after 7 million training steps).Why it matters:Diffusion models and contrastive self-supervised models like DINOv2 have fundamentally different training objectives: One produces embeddings for the purpose of image generation, while the other’s embeddings are used for tasks like classification and semantic segmentation. Consequently, they learn different aspects of data. This work proposes a novel way to combine these approaches to produce more generally useful embeddings.We’re thinking:It turns out that the REPA modification enabled diffusion models to produce embeddings better suited not only to diffusion but also to image classification and segmentation. A similar approach could lead to a more holistic framework for learning image representations.Share\nPublishedMar 26, 2025\nPublished\n\nPublished\nMar 26, 2025\nMar 26, 2025\nReading time14min read\nReading time\n\nReading time\n14min read\nShare\nShare\n\nShare\n\nLoading theElevenlabs Text to SpeechAudioNative Player...Dear friends,Fine-tuning small language models has been gaining traction over the past half year. I’d like to share my sense of when to use this technique, and also when not to, based on what I’m seeing in multiple companies.First, while fine-tuning is an important and valuable technique, many teams that are currently using it probably could get good results with simpler approaches, such as prompting (including writingmega prompts), few-shot prompting, or simple agentic workflows.Why shouldn’t these teams be fine-tuning? Because fine-tuning, which takes a pre-trained model and further trains it on data specific to an application, is relatively complex to implement. You need to collect training data, then (unless you want to implement fine-tuning yourself) find a provider to help with running fine-tuning, then find a way to deploy the fine-tuned model. Because it adds extra complexity both in training and deployment, usually I resort to this technique only after I find that prompting and simple agentic workflows are not up to a task.Having said that, there are also applications where fine-tuning is appropriate and valuable. LoRA (which learns by modifying a limited number of parameters rather than the entire model) and related methods have made fine-tuning quite affordable, particularly for small models (say, 13B or fewer parameters). And the amount of data needed to get started is less than most people think. Depending on the application, I’ve seen good results with 100 or even fewer examples. Here are a few applications where I have seen fine-tuning applied successfully:Improving accuracy of critical applications.Prompting can get you really far for many applications. But sometimes, fine-tuning helps eke out that last bit of accuracy. For example, if you are building a customer service chatbot and need it to call the right API reliably (say, to carry out transactions, issue refunds, and the like), perhaps prompting can get it to make the right API call 95% of the time. But if you struggle to raise the accuracy even with revisions to the prompt and you really need 99% accuracy, fine-tuning on a dataset of conversations and API calls might be a good way to get you there. This is particularly true for tasks where it's hard to specify, using only language, an unambiguous rule to decide what to do. For example, when a customer is frustrated, should the chatbot escalate to a manager or just issue a refund? Teams often write Standard Operating Procedures (SOPs) for human workers to follow, and these SOPs can go into the prompts of models. But if it is hard to specify an unambiguous SOP, so even humans need to see numerous examples before they can learn what to do, fine-tuning can be a good approach. For many text-classification applications fine-tuning also works well, for example, classifying medical records into diagnosis and procedure codes for health insurance claims.Learning a particular  style of communication.As I explain in “Generative AI for Everyone,” my team fine-tuned a model to sound like me. Many people (including myself) have idiosyncratic uses of language. There are certain words I tend to say and others I tend not to, and these idiosyncrasies are numerous and very difficult to specify in a text prompt. (By the way, the avatar at deeplearning.ai/avatar, built with RealAvatar, uses fine-tuning for this reason.) To get a system to communicate in a certain style, fine-tuning is often a superior solution to prompting alone.Reducing latency or cost during scale-ups.I’ve seen applications where developers have successfully prompted a large model to perform a complex task. But as usage scales up, if the large model is too slow (which often happens) or too expensive (which also happens but less frequently), the team might want to use a smaller model. If, however, the performance of the smaller model isn't good enough, then fine-tuning it can help bring it up to the performance of the larger one for that narrow application. Further, the larger model (or perhaps an agentic workflow) can also be used to generate data to help with fine-tuning the small model for that task.At the cutting edge of research, some teams are fine-tuning models to get better at a certain language. But with few exceptions, if the goal is to get an LLM to better understand a body of knowledge that is not in its training data, I find that using RAG (retrieval augmented generation) is a much simpler approach, and I still occasionally run into teams using fine-tuning for which  I think RAG would work better.Overall my sense is that, of all the teams I see using fine-tuning, perhaps 75% could get good results using simpler techniques (like prompting or agentic workflows), but in 25% of cases I know of no better way to achieve their goal.It is still technically challenging to implement fine-tuning, get the hyperparameters right, optimize the compute resources, and so on. We are lucky that more and more companies have worked hard to optimize these and provide efficient fine-tuning services. Many of them allow us to fine-tune open weights models and also download the fine-tuned weights. Some allow us to fine-tune their closed models and continue to keep the tuned weights closed. Both can be useful, but the former has obvious advantages of portability and not having to worry that the provider will stop serving a particular model, causing a critical component in our software to become deprecated.In conclusion, before fine-tuning, consider if you should be trying just a bit harder with prompting or agentic workflows, which can lead to simpler solutions that are easier to maintain. The vast majority of applications my teams build do not use any fine-tuning at all, but it’s a critical piece of a small minority of them.Keep learning!AndrewA MESSAGE FROM DEEPLEARNING.AIIn “Vibe Coding 101 with Replit,” you’ll learn to plan, prompt, and debug alongside a coding agent. Build, host, and share two real web apps in Replit’s cloud environment while developing effective development skills like writing product requirements, structuring tasks, and refining AI-generated code.Start todayNewsVision-Language, Compact and OpenGoogle updated its open-weights family of large language models to include versions that handle image and video inputs.What’s new:Google released itsGemma 3multilingual large language models with parameter counts of 1 billion, 4 billion, 12 billion, and 27 billion. While the smallest processes text only, the other three are vision-language models that are small enough to run on a consumer hardware.Input/output:Gemma 3 1B: text-in (up to 32,000 tokens), text out (up to 8,192 tokens). Gemma 3 4B, 7B, 27B: text, images/video in (up to 128,000 tokens), text out (up to 8,192 tokens). Gemma 3 27Boutputs24.61 tokens per /second, 0.68 seconds to first token.Knowledge cutoff:March 2024Architecture:Gemma 3 1B: Transformer. Gemma 3 4B, 12B, 27B: Transformer, SigLIP  vision encoder.Features:140 languages, function calling, structured output.Training data:Gemma 3 1B: 2 trillion tokens of web text, code, and mathematics. Gemma 3 4B, 12B, 27B: between 4 trillion and 14 trillion tokens of text and images.Availability/price:Weights free to download fromHugging Faceand Kaggle under alicensethat allows noncommercial and commercial uses with some restrictions. Available free via Google’s AI Studio.How it works:Gemma 3rearchitectsand refines earlier Gemma models for higher performance at lower parameter counts.To save memory, Gemma 3 interleaves five local attention layers for every global attention layer. Global attention layers attend to the entire input, while local attention layers attend to 1,024 tokens.The models were fine-tuned to encourage their outputs to match those of an unspecified larger teacher model.Gemma 3 learned via reinforcement learning in three ways. (i) The models were aligned with human preferences viareinforcement learning from human feedback(RLHF). (ii) They were fine-tuned to solve math problems via reinforcement learning, much likeDeepSeek-R1. (iii) They were trained to generate better code viareinforcement learning from execution feedback (RLEF). Specifically, over several rounds of output, RLEF tested generated code on a subset of tests, then prompted the model to fix any bugs. RLEF rewarded the models if their final output passed all tests.Performance:Gemma 3 models outperform Gemma 2 models of equal or larger size by several measures, and all sizes show a strong ability to solve mathematics word problems as measured byMATH.In Google’s tests, Gemma 3 1B performs roughly comparably to Gemma 2 2B, outperforming the larger model on LiveCodeBench (1.9 percent to 1.2 percent) and MATH (48.0 percent to 27.2 percent).Gemma 3 4B achieves roughly comparable performance to Gemma 2 9B, Llama 3.1 8B, and Qwen2.5-7B. It’s slightly behind Microsoft Phi-4 Mini (also 4 billion parameters), except on MATH, according to that company’s tests.Gemma 3 12B improves on Gemma 2 27B and compares to Gemini 1.5 Flash (in TIGER-Lab’s tests) and Anthropic Claude 3.5 Haiku (in that developer’s tests). It outperforms the larger, proprietary models on MATH.Gemma 3 27B consistently outperforms the Gemma 2 model of the same size and performs comparably to Gemini 1.5 Pro onMMLU-Pro(high-level language comprehension) 67.5 percent to 56.9 percent, onLiveCodeBench(coding) 29.7 percent to 20.4 percent, onGPQA Diamond(graduate-level domain knowledge) 42.4 percent to 34.3 percent, and on MATH 89.0 percent to 55.6 percent.Moreover, Gemma 3 27B achieves 1,338 ELO inChatbot Arena, a top-ten score that puts it ahead of OpenAI o1 and behind only DeepSeek-R1 among models with open weights.Hot on Gemma 3’s heels:Shortly after Gemma 3 became available, Mistral releasedSmall 3.1(24 billion parameters), a vision-language model with open weights, under a more permissive Apache 2.0 license.Mistral Small 3.1 is similarly multilingual and offers a 128,000 token context window.It slightly outperforms Gemma 3 27B on MMLU, MMLU-Pro, MMMU, and other selected benchmarks.It also outperforms Gemma 3 27B and other models in its size range on long-context tests. (However, Gemma 3 27B performs better in the Chatbot Arena test of human preference.)Why it matters:Gemma 3 takes advantage of a variety of techniques to raise the bar for vision-language performance in relatively small models. Knowledge distillation, multiple rounds of reinforcement learning, and fine-tuning on many languages are a powerful combination.We’re thinking:A vision-language model small enough to run on a smartphone feels increasingly close!Better Images in Fewer StepsDiffusion models usually take many noise-removal steps to produce an image, which takes time at inference. There are ways to reduce the number of steps, but the resulting systems are less effective. Researchers devised a streamlined approach that doesn’t sacrifice output quality.What’s new:Kevin Frans and colleagues at UC Berkeley introducedshortcut modelsthat learn to take larger noise-removal steps and thus require fewer steps to generate an image.Key insight:At inference, a scheduler likeEulercan enable a model to take larger steps than those it learned during training, but this approach yieldsworse performance. Alternatively distillation, in which a student model learns to remove the same amount of noise as a teacher model when it takes several steps, offers improved performance at the cost of more cumbersome development. Training the model directly to take bigger steps — that are equivalent to multiple smaller steps — enables it to maintain high performance while taking fewer steps.How it works:The authors trainedDiT-B, a diffusion transformer, to generate images like those in CelebA-HQ (celebrity faces) and ImageNet-256 (various subjects, size 256x256).The loss function included terms for flow matching and self-consistency. The flow matching term encouraged the model to learn to remove noise. The self-consistency term encouraged the model to learn how to minimize the discrepancy between the noise removed by a single big step and two smaller steps.Initially the model learned to combine two small steps into one step 2x as large. Combining two larger steps resulted in step sizes of 4x, 8x, and so on, up to 128x.At inference, the user told the model how many small steps to take, and the model computed the single-step size necessary to accomplish that.Results:The authors compared their model using 1, 4, or 128 steps to alternatives that were trained via various methods including many variants of distillation. They measured the results usingFréchet inception distance(FID), which assesses how closely generated images resemble real-world images (lower is better).On both CelebA-HQ and ImageNet-256, their model, when it took four steps, achieved the best performance. For example, on CelebA-HQ, using four steps, the shortcut model achieved 13.8 FID, while the next-best model,Reflow(another variant of distillation), achieved 18.4 FID.When it took one step, it achieved the second-best result, behindprogressive distillation, which trained a series of student models to remove the same amount of noise as a teacher model does when it takes multiple steps.Why it matters:Generating images by diffusion is typically costly, and previous approaches to cutting the cost have compromised either performance or incurred additional development expense or both. This method achieves high performance at relatively low cost.We’re thinking:As diffusion models continue to become cheaper and faster, we expect to see applications blossom!LLM Support for TutorsStudents benefit from tutoring, but training tutors is expensive. A study shows that large language models can boost tutors’ effectiveness in real time.What’s new:Rose Wang and colleagues at Stanford builtTutor CoPilot, a tool for remote, online tutors that uses GPT-4 to generate hints, explanations, questions, and other helpful responses to students.Key insight:When a student makes an error, according to previousworkby some of the same authors, effective teachers choose a strategy for addressing the mistake. The authors identified 11 strategies, such as ask a question, explain a concept, provide a hint, or encourage the student. Moreover, they found that an LLM that executed a strategy chosen by an expert teacher performed significantly better than an LLM that was prompted with a strategy chosen at random or no specific strategy. Letting inexperienced tutors choose a strategy while an LLM generates a response helps them learn how to execute the strategy. Students, in turn, benefit from responses that mimic those of an experienced teacher.How it works:The authors outfitted a remote tutoring application with GPT-4.The application included a tutor-student chat window, a problem display, and a whiteboard. The authors added a button that enabled the tutor to turn Tutor CoPilot on or off.When a tutor engaged Tutor CoPilot, the system prompted GPT-4 to behave as an experienced elementary math teacher and provided context in the form of the 10 most recent messages, the current lesson topic, and a default strategy from the list. GPT-4 responded with guidance. (To preserve the tutor’s and student’s privacy, the system redacted their names using the open source libraryEdu-ConvoKit.)The system prompted GPT-4 three times, each time changing the strategy, and presented the tutor with three potential responses.The tutor could re-generate or edit GPT-4’s responses, or select a strategy and generate a new response before adding it to the chat window.Results:The authors partnered with a virtual tutoring company and a school district in the United States for a two-month study of 874 tutors and 1,787 students between grades 3 and 8. They divided the participants into two groups. In one group, tutors conducted sessions with students as usual. In the other, tutors had access to Tutor CoPilot. The authors measured success by the percentage of students who passed a test at the end of a lesson.In the group that didn’t use Tutor CoPilot, 62 percent of students passed the test.In the group with TutorCopilot, 66 percent passed.The effect was most pronounced among the one-third of tutors who had the lowest ratings (9 percent higher) and least experience (7 percent higher).The API cost was approximately $3.31 per tutor, or roughly $20 per tutor per year.Yes, but:The authors found statistically significant improvements as measured by test results per lesson, but not in end-of-year exam results. The study’s two-month duration may account for the lack of evidence for longer-term effects.Why it matters:LLMs hold great promise for helping to educate students, but they also show potential in educating teachers. For inexperienced tutors who are learning how to interact with students, an LLM’s general knowledge and pedagogical insights gleaned from expert teachers make a powerful combination.We’re thinking:Although it relies on sophisticated technology, the authors’ approach is simple: Prompt an LLM to apply proven teaching principles. Presumably such principles apply beyond elementary math, which would make this approach useful for teaching a variety of disciplines.Faster Learning for Diffusion ModelsDiffusion transformers learn faster when they can look at embeddings generated by a pretrained model like DINOv2.What’s new:Sihyun Yu and colleagues at Korea Advanced Institute of Science and Technology, Korea University, New York University, and Scaled Foundations (a startup that builds AI for robotics) proposedRepresentation Alignment(REPA), a loss term for transformer-based diffusion.Key insight:Diffusion models learn to remove noise from images to which noise was added (and, at inference, they start with pure noise to generate a fresh image). This process can be divided into two parts: learning to (i) embed the noisy image and (ii) estimate the noise from the embedding. One way to accelerate learning is to add a loss term that encourages the diffusion model to produce embeddings that are similar to those produced by a pretrained embedding model. The diffusion model can learn to estimate the noise faster if it doesn’t need to learn how to embed an image from scratch.How it works:The authors modifiedDiT-XL/2andSiT-XL/2transformer-based latent diffusion models, a class of diffusion models that subtract noise from embeddings rather than images. They trained the models to produce images similar to ImageNet. In the process, the modified models learned to produce embeddings similar to those produced by a pretrainedDINOv2.The authors usedStable Diffusion VAE’spretrained encoder to embed an image.Given the embedding with noise added, the diffusion model learned to remove the noise according to the usual loss term.It also learned according to the REPA loss. Specifically, it learned to maximize the cosine similarity between a specially processed version of its eighth-layer embedding and the embedding produced by a pretrained DINOv2. To process its eighth-layer embedding for the REPA loss, the diffusion model fed the embedding to a vanilla neural network.At inference, given pure noise, the model removed it over several steps to produce an image embedding. Stable Diffusion VAE’s decoder converted the embedding into an image.Results:The modified DiT-XL/2 learned significantly faster than the unmodified version.In 400,000 training steps, the modified model reached 12.3Fréchet inception distance(FID) (which measures similarity between generated and non-generated images, lower is better), while the unmodified version reached 19.5 FID.The models continued to learn at different speeds as training continued. The modified DiT-XL/2  took 850,000 training steps to reach 9.6 FID, while the unmodified version took 7 million steps to reach the same number.Experiments with modified and unmodified versions of SiT-XL/2 yielded similar results.Trained to convergence, the modified models outperformed the unmodified versions. For instance, the modified  SiT-XL/2 achieved 5.9 FID (after 4 million training steps), while the unmodified version achieved 8.3 FID (after 7 million training steps).Why it matters:Diffusion models and contrastive self-supervised models like DINOv2 have fundamentally different training objectives: One produces embeddings for the purpose of image generation, while the other’s embeddings are used for tasks like classification and semantic segmentation. Consequently, they learn different aspects of data. This work proposes a novel way to combine these approaches to produce more generally useful embeddings.We’re thinking:It turns out that the REPA modification enabled diffusion models to produce embeddings better suited not only to diffusion but also to image classification and segmentation. A similar approach could lead to a more holistic framework for learning image representations.\nLoading theElevenlabs Text to SpeechAudioNative Player...\n\n\nShare\nShare\n\nShare\n",
    "image_urls": [
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--56-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/03/V3_DeepLearning_Replit_Banner_2070x1080-01.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--67-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--68-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--69-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--54-.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2024/08/1-9.png"
    ]
  },
  {
    "title": "issue 292",
    "date": null,
    "url": "https://www.deeplearning.ai/the-batch/issue-292/",
    "content": "The Batch\nWeekly Issues\nissue 292\nPublishedMar 12, 2025\nPublished\n\nPublished\nMar 12, 2025\nReading time13min read\nReading time\n\nReading time\n13min read\nPublishedMar 12, 2025Reading time13min readShareLoading theElevenlabs Text to SpeechAudioNative Player...Dear friends,Some people today are discouraging others from learning programming on the grounds AI will automate it. This advice will be seen as some of the worst career advice ever given. I disagree with the Turing Award and Nobel prize winner who wrote, “It is far more likely that the programming occupation will become extinct [...] than that it will become all-powerful. More and more, computers will program themselves.”​ Statements discouraging people from learning to code are harmful!In the 1960s, when programming moved from punchcards (where a programmer had to laboriously make holes in physical cards to write code character by character) to keyboards with terminals, programming became easier. And that made it a better time than before to begin programming. Yet it was in this era that Nobel laureate Herb Simon wrote the words quoted in the first paragraph. Today’s arguments not to learn to code continue to echo his comment.As coding becomes easier, more people should code, not fewer!Over the past few decades, as programming has moved from assembly language to higher-level languages like C, from desktop to cloud, from raw text editors to IDEs to AI assisted coding where sometimes one barely even looks at the generated code (which some coders recently started to call vibe coding), it is getting easier with each step. (By the way, to learn more about AI assisted coding, check out our video-only short course, “Build Apps with Windsurf’s AI Coding Agents.”)I wrote previously that I see tech-savvy people coordinating AI tools to move toward being10x professionals— individuals who have 10 times the impact of the average person in their field. I am increasingly convinced that the best way for many people to accomplish this is not to be just consumers of AI applications, but to learn enough coding to use AI-assisted coding tools effectively.One question I’m asked most often is what someone should do who is worried about job displacement by AI. My answer is: Learn about AI and take control of it, because one of the most important skills in the future will be the ability to tell a computer exactly what you want, so it can do that for you. Coding (or getting AI to code for you) is the best way to do that.When I was working on the courseGenerative AI for Everyoneand needed to generate AI artwork for the background images, I worked with a collaborator who had studied art history and knew the language of art. He prompted Midjourney with terminology based on the historical style, palette, artist inspiration and so on — using the language of art — to get the result he wanted. I didn’t know this language, and my paltry attempts at prompting could not deliver as effective a result.Similarly, scientists, analysts, marketers, recruiters, and people of a wide range of professions who understand the language of software through their knowledge of coding can tell an LLM or an AI-enabled IDE what they want much more precisely, and get much better results. As these tools continue to make coding easier, this is the best time yet to learn to code, to learn the language of software, and learn to make computers do exactly what you want them to do.Keep building!AndrewA MESSAGE FROM DEEPLEARNING.AIPre-enroll now for the new Data Analytics Professional Certificate! Gain job-ready skills in data analysis, whether you’re starting a career as a data analyst or enhancing your ability to prepare, analyze, and visualize data in your current role. This program covers both classical statistical techniques and emerging AI-assisted workflows to help you work smarter with data.Learn more and sign upNewsCompact ReasoningMost models that have learned to reason via reinforcement learning were huge models. A much smaller model now competes with them.What’s new:Alibaba introducedQwQ-32B, a large language model that rivals the reasoning prowess of DeepSeek-R1 despite its relatively modest size.Input/output:Text in (up to 131,072 tokens), text outArchitecture:Transformer, 32.5 billion total parameterPerformance:Outperforms OpenAI o1-mini and DeepSeek-R1 on some bencharksFeatures:Chain-of-thought reasoning, function calling, multilingual in 29 languagesUndisclosed:Output size, training dataAvailability/price:Free viaQwen Chat. Weights are free todownloadfor noncommercial and commercial uses under an Apache 2.0 license.How it works:QwQ-32B is a version ofQwen2.5-32Bthat was fine-tuned to generate chains of thought using reinforcement learning (RL). Fine-tuning proceeded in two stages.The first stage of RL fine-tuning focused on math and coding tasks. The model earned rewards for correct final outcomes (no partial credit for intermediate steps). An accuracy verifier checked its math solutions, while a code-execution server verified generated code for predefined test cases.The second stage encouraged the model to follow instructions, use tools, and align its values with human preferences while maintaining math and coding performance, again rewarding final outcomes. In this stage, the model earned rewards from an unspecified reward model and some rule-based verifiers.Performance:On several benchmarks for math, coding, and general problem solving, QwQ-32B outperforms OpenAI o1-mini (parameter count undisclosed) and achieves performance roughly comparable to DeepSeek-R1 (671 billion parameters, 37 billion active at any moment).On AIME24 (high-school competition math problems), QwQ-32B achieved 79.5 percent accuracy, well ahead of o1-mini (63.6 percent) but slightly behind DeepSeek-R1 (79.8 percent).On LiveCodeBench (code generation, repair, and testing), QwQ-32B achieved 63.4 percent, outperforming o1-mini (53.8 percent) but trailing DeepSeek-R1 (65.9 percent).On LiveBench (problem-solving in math, coding, reasoning, and data analysis), QwQ-32B reached 73.1 percent, ahead of o1-mini (59.1 percent) and DeepSeek-R1 (71.6 percent).On IFEval (following instructions), QwQ-32B achieved 83.9 percent, outperforming DeepSeek-R1 (83.8 percent) but behind o1-mini (84.8 percent).On BFCL (function calling), QwQ-32B achieved 66.4 percent, better than DeepSeek-R1 (60.3 percent), and o1-mini (62.8 percent).Behind the news:DeepSeek’s initial model, DeepSeek-R1-Zero, similarly applied RL to a pretrained model. That effort produced strong reasoning but poor readability (for example, math solutions with correct steps but jumbled explanations). To address this shortcoming, the teamfine-tunedDeepSeek-R1 on long chain-of-thought examples before applying RL. In contrast, QwQ-32B skipped preliminary fine-tuning and applied RL in two stages, first optimizing for correct responses and then for readability.Why it matters:RL can dramatically boost LLMs’ reasoning abilities, but the order in which different behaviors are rewarded matters. Using RL in stages enabled the team to build a 32 billion parameter model — small enough to run locally on a consumer GPU — that rivals a much bigger mixture-of-experts model, bringing powerful reasoning models within reach for more developers. The Qwen team plans to scale its RL approach to larger models, which could improve the next-gen reasoning abilities further while adding greater knowledge.We’re thinking:How far we’ve come since “Let’s think step by step”!Microsoft Tackles Voice-In, Text-OutMicrosoft debuted its first official large language model that responds to spoken input.What’s new:Microsoft releasedPhi-4-multimodal, an open weights model that processes text, images, and speech simultaneously.Input/output:Text, speech, images in (up to 128,000 tokens); text out (0.34 seconds to first token, 26 tokens per second)Performance:State of the art in speech transcription. Comparable to similar models in other tasksKnowledge cutoff:June 2024Architecture:transformer, 5.6 billion parametersFeatures:Text-image-speech processing, multilingual, tool use.Undisclosed:Training datasets, output sizeThe company also releasedPhi-4-mini, an open weights 3.8 billion-parameter version of its biggest large language model (LLM),Phi-4. Phi-4-mini outperforms larger models including Llama 3.1 8B and Ministral-2410 8B on some benchmarks.Availability/price:Weights are free todownloadfor noncommercial and commercial use under aMIT license.How it works:Phi-4-multimodal has six components: Phi-4-mini, vision and speech encoders as well as corresponding projectors (which modify the vision or speech embeddings so the base model can understand them), and two LoRA adapters. The LoRA adapters modify the base weights depending on the input: One adapter modifies them for speech-text problems, and one for vision-text and vision-speech problems.The speech encoder is aConformer(which combines convolutional layers with a transformer) and the speech projector is a vanilla neural network. They trained Phi-4-multimodal to convert 2 million hours of speech to text, modifying only the speech encoder and projector. They further trained the system to convert speech to text, translate speech to other languages, summarize speech, and answer questions about speech, modifying only the speech encoder and the speech-text LoRA adapter.The vision encoder is based on a pretrainedSigLIP-400Mvision transformer, and the vision projector is a vanilla neural network. They trained the model to process text and images in four stages: (i) They trained Phi-4-multimodal to caption images, modifying only the vision projector. (ii) They trained the system on 500 billion tokens to caption images, transcribe text in images, and perform other tasks, modifying only the vision encoder and projector. (iii) They trained the system to answer questions about images, charts, tables, and diagrams and to transcribe text in images, modifying the vision encoder, project, and vision-text LoRA adapter. (iv) Finally, they trained the system to compare images and summarize videos, modifying only the vision projector and vision-text LoRA adapter.To adapt Phi-4-multimodal for images and speech, they trained the system to generate the text responses to a subset of the text-vision data that had been converted to speech-image using a proprietary text-to-speech engine, modifying only the text-vision LoRA adapter, vision encoder, and vision projector.Example inference: Given a question as speech and an image, the audio encoder and projector convert the speech to tokens, and the image encoder and projector convert the image into tokens. Given the tokens, Phi-4-multimodal, which uses the weights of Phi-4-mini modified by the vision-text/vision-speech LoRA adapter, generates a text response.Results:The authors compared Phi-4-multimodal to other multimodal models on text-vision, vision-speech, text-speech tasks.Across 11 text-vision benchmarks, Phi-4-multimodal came in fourth out of 11 models. It outperformed Qwen2.5-VL-3B, Claude 3.5 Sonnet, and GPT 4o-mini. It trailed Qwen2.5-VL-7B, GPT-4o, and Gemini-2 Flash.Across fourvision-speech benchmarks, Phi-4-multimodal outperformed by at least 6 percentage points Gemini-2.0-Flash, Gemini-2.0-Flash-Lite-preview, and InternOmni.Phi-4-multimodal outperformed all competitors in Microsoft’s report (including Qwen2-audio, Gemini 2.0 Flash, and GPT-4o) at transcribing speech from textinthreedatasets. It also achieved competitive performance in speech translation, outperforming its competitors on two of four datasets.Behind the news:This work adds to the growing body of models with voice-in/text-out capability, including the open weightsDiVAmodel developed by a team led by Diyi Yang at Stanford University.Why it matters:The architectural options continue to expand for building neural networks that process text, images, audio, and various combinations. While some teams maintain separate models for separate data modalities, likeQwen2.5(for text) andQwen2.5-VL) (for vision-language tasks), others are experimenting with mixture-of-expert models likeDeepSeek-V3. Phi-4-multimodal shows that Mixture-of-LoRAs is an effective approach for processing multimodal data — and gives developers a couple of new open models to play with.We’re thinking:Output guardrails have been built to ensure appropriateness of text output, but this is difficult to apply to a voice-in/voice-out architecture. (Some teams have worked on guardrails that screen audio output directly, but the technology is still early.) For voice-based applications, a voice-in/text-out model can generate a candidate output without a separate, explicit speech-to-text step, and it accommodates text-based guardrails before it decides whether or not to read the output to the user.Judge Upholds Copyright in AI Training CaseA United States court delivered a major ruling that begins to answer the question whether, and under what conditions, training an AI system on copyrighted material is considered fair use that doesn’t require permission.What’s new:A U.S. Circuit judgeruledon a claim by the legal publisher Thomson Reuters that Ross Intelligence, an AI-powered legal research service, could not claim that training its AI system on materials owned by Thomson Reuters was a so-called “fair use.” Training the system did not qualify as fair use, he decided, because its output competed with Thomson Reuters’ publications.How it works:Thomson Reuters hadsuedRoss Intelligence after the defendant trained an AI model using 2,243 works produced by Thomson Reuters without the latter’s permission. This ruling reversed an earlier decision in 2023, when the same judge had allowed Ross Intelligence’s fair-use defense to proceed to trial. In the new ruling, he found that Ross Intelligence’s use failed to meet the definition of fair use in key respects. (A jury trial is scheduled to determine whether Thomson Reuters' copyright was in effect at the time of the infringement and other aspects of the case.)Ross Intelligence’s AI-powered service competed directly with Thomson Reuters, potentially undermining its market by offering a derivative product without licensing its works. Use in a competing commercial product undermines a key factor in fair use.The judge found that Ross Intelligence’s use was commercial and not transformative, meaning it did not significantly alter or add new meaning to Thomson Reuters’ works — another key factor in fair use. Instead, it simply repackaged the works.The ruling acknowledged that Thomson Reuters’ works were not highly creative but noted that they possessed sufficient originality for copyright protection due to the editorial creativity and judgment involved in producing it.Although Ross Intelligence used only small portions of Thomson Reuters’ works, this did not weigh strongly in favor of fair use because those portions represented the most important summaries produced by Ross Intelligence.Behind the news:The ruling comes amid awaveof lawsuits over AI training and copyright in several countries. Many of these cases are in progress, but courts have weighed in on some.The New York TimesissuingOpenAI and Microsoft, arguing that their models generate output that competes with its journalism.Condé Nast, McClatchy, and other major publishers recentlyfileda lawsuit against Cohere, accusing it of using copyrighted news articles to train its AI models.Sony, UMG, and Warner Musicfiledlawsuits against AI music companies including Suno and Udio for allegedly using copyrighted recordings without permission.A judgedismissedkey arguments brought by software developers who claimed that GitHub Copilot was trained on software they created in violation of open source licenses. The judge ruled in favor of Microsoft and OpenAI.In Germany, the publisher of the LAION datasetwona case in which a court ruled that training AI models on publicly available images did not violate copyrights.Why it matters:The question of whether training (or copying data to train) AI systems is a fair use of copyrighted works hangs over the AI industry, from academic research to commercial projects. In the wake of this ruling, courts may be more likely to reject a fair-use defense when AI companies train models on copyrighted material to create output that overlaps with or replaces traditional media, asThe New York Timesalleges in its lawsuit against OpenAI. However, the ruling leaves room for fair use with respect to models whose output doesn’t compete directly with copyrighted works.We’re thinking:Current copyright laws weren’t designed with AI in mind, and rulings like this one fill in the gaps case by case.Clarifying copyrightfor the era of generative AI could help our field move forward faster.DeepSeek-R1 UncensoredLarge language models built by developers in China may, in some applications, be less useful outside that country because they avoid topics its government deems politically sensitive. A developer fine-tuned DeepSeek-R1 to widen its scope without degrading its overall performance.What’s new:Perplexity releasedR1 1776, a version ofDeepSeek-R1that responds more freely than the original. The model weights are available todownloadunder a commercially permissive MITlicense.How it works:The team modified DeepSeek-R1’s knowledge of certain topics by fine-tuning it on curated question-answer pairs.Human experts identified around 300 topics that are censored in China.The authors developed a multilingual classifier that spots text related to these topics.They identified 40,000 prompts that the classifier classified as sensitive with high confidence. They discarded those that contained personally identifiable information.For each prompt, they produced factual, chain-of-thought responses that mirrored DeepSeek-R1's typical reasoning processes.They fine-tuned DeepSeek-R1 on the resulting prompt-response pairs.Results:The fine-tuned model responded to politically charged prompts factually without degrading its ability to generate high-quality output.The authors fed their model 1,000 diverse prompts that covered frequently censored topics. An unspecified combination of human and AI judges rated the models' responses according to the degree to which they are (i) evasive and (ii) censored outright.100 percent of the fine-tuned model’s responses were rated uncensored, whereas the original version censored around 85 percent of sensitive queries. By comparison, DeepSeek-V3 censored roughly 73 percent, Claude-3.5-Sonnet around 5 percent, o3-mini about 1 percent, and GPT-4o 0 percent.Evaluated on four language and math benchmarks (MMLU, DROP, MATH-500, and AIME 2024) and unspecified internal benchmarks, the fine-tuned and original models performed nearly identically. Their scores differed by a few tenths of a percent except on AIME 2024 (competitive high-school math problems), where the fine-tuned model achieved 79.8 percent compared to the original’s 80.96 percent.Behind the news:Amongthe first countries to regulate AI, ChinarequiresAI developers to build models that uphold “Core Socialist Values” and produce true and reliable output. When these objectivesconflict, the political goal tends to dominate. While large language models built by developers in China typically avoid contentious topics, the newer DeepSeek models enforce this more strictly than older models like Qwen and Yi, using methods akin to Western measures for aligning output, like Reinforcement Learning from Human Feedback andkeyword filters.Why it matters:AI models tend to reflect their developers’ values and legal constraints. Perplexity’s targeted fine-tuning approach addresses this barrier to international adoption of open-source models.We’re thinking:As models with open weights are adopted by the global community, they become a source of soft power for their developers, since they tend to reflect their developers’ values. This work reflects a positive effort to customize a model to reflect the user’s values instead — though how many developers will seek out a fine-tuned version rather than the original remains to be seen.Share\nPublishedMar 12, 2025\nPublished\n\nPublished\nMar 12, 2025\nMar 12, 2025\nReading time13min read\nReading time\n\nReading time\n13min read\nShare\nShare\n\nShare\n\nLoading theElevenlabs Text to SpeechAudioNative Player...Dear friends,Some people today are discouraging others from learning programming on the grounds AI will automate it. This advice will be seen as some of the worst career advice ever given. I disagree with the Turing Award and Nobel prize winner who wrote, “It is far more likely that the programming occupation will become extinct [...] than that it will become all-powerful. More and more, computers will program themselves.”​ Statements discouraging people from learning to code are harmful!In the 1960s, when programming moved from punchcards (where a programmer had to laboriously make holes in physical cards to write code character by character) to keyboards with terminals, programming became easier. And that made it a better time than before to begin programming. Yet it was in this era that Nobel laureate Herb Simon wrote the words quoted in the first paragraph. Today’s arguments not to learn to code continue to echo his comment.As coding becomes easier, more people should code, not fewer!Over the past few decades, as programming has moved from assembly language to higher-level languages like C, from desktop to cloud, from raw text editors to IDEs to AI assisted coding where sometimes one barely even looks at the generated code (which some coders recently started to call vibe coding), it is getting easier with each step. (By the way, to learn more about AI assisted coding, check out our video-only short course, “Build Apps with Windsurf’s AI Coding Agents.”)I wrote previously that I see tech-savvy people coordinating AI tools to move toward being10x professionals— individuals who have 10 times the impact of the average person in their field. I am increasingly convinced that the best way for many people to accomplish this is not to be just consumers of AI applications, but to learn enough coding to use AI-assisted coding tools effectively.One question I’m asked most often is what someone should do who is worried about job displacement by AI. My answer is: Learn about AI and take control of it, because one of the most important skills in the future will be the ability to tell a computer exactly what you want, so it can do that for you. Coding (or getting AI to code for you) is the best way to do that.When I was working on the courseGenerative AI for Everyoneand needed to generate AI artwork for the background images, I worked with a collaborator who had studied art history and knew the language of art. He prompted Midjourney with terminology based on the historical style, palette, artist inspiration and so on — using the language of art — to get the result he wanted. I didn’t know this language, and my paltry attempts at prompting could not deliver as effective a result.Similarly, scientists, analysts, marketers, recruiters, and people of a wide range of professions who understand the language of software through their knowledge of coding can tell an LLM or an AI-enabled IDE what they want much more precisely, and get much better results. As these tools continue to make coding easier, this is the best time yet to learn to code, to learn the language of software, and learn to make computers do exactly what you want them to do.Keep building!AndrewA MESSAGE FROM DEEPLEARNING.AIPre-enroll now for the new Data Analytics Professional Certificate! Gain job-ready skills in data analysis, whether you’re starting a career as a data analyst or enhancing your ability to prepare, analyze, and visualize data in your current role. This program covers both classical statistical techniques and emerging AI-assisted workflows to help you work smarter with data.Learn more and sign upNewsCompact ReasoningMost models that have learned to reason via reinforcement learning were huge models. A much smaller model now competes with them.What’s new:Alibaba introducedQwQ-32B, a large language model that rivals the reasoning prowess of DeepSeek-R1 despite its relatively modest size.Input/output:Text in (up to 131,072 tokens), text outArchitecture:Transformer, 32.5 billion total parameterPerformance:Outperforms OpenAI o1-mini and DeepSeek-R1 on some bencharksFeatures:Chain-of-thought reasoning, function calling, multilingual in 29 languagesUndisclosed:Output size, training dataAvailability/price:Free viaQwen Chat. Weights are free todownloadfor noncommercial and commercial uses under an Apache 2.0 license.How it works:QwQ-32B is a version ofQwen2.5-32Bthat was fine-tuned to generate chains of thought using reinforcement learning (RL). Fine-tuning proceeded in two stages.The first stage of RL fine-tuning focused on math and coding tasks. The model earned rewards for correct final outcomes (no partial credit for intermediate steps). An accuracy verifier checked its math solutions, while a code-execution server verified generated code for predefined test cases.The second stage encouraged the model to follow instructions, use tools, and align its values with human preferences while maintaining math and coding performance, again rewarding final outcomes. In this stage, the model earned rewards from an unspecified reward model and some rule-based verifiers.Performance:On several benchmarks for math, coding, and general problem solving, QwQ-32B outperforms OpenAI o1-mini (parameter count undisclosed) and achieves performance roughly comparable to DeepSeek-R1 (671 billion parameters, 37 billion active at any moment).On AIME24 (high-school competition math problems), QwQ-32B achieved 79.5 percent accuracy, well ahead of o1-mini (63.6 percent) but slightly behind DeepSeek-R1 (79.8 percent).On LiveCodeBench (code generation, repair, and testing), QwQ-32B achieved 63.4 percent, outperforming o1-mini (53.8 percent) but trailing DeepSeek-R1 (65.9 percent).On LiveBench (problem-solving in math, coding, reasoning, and data analysis), QwQ-32B reached 73.1 percent, ahead of o1-mini (59.1 percent) and DeepSeek-R1 (71.6 percent).On IFEval (following instructions), QwQ-32B achieved 83.9 percent, outperforming DeepSeek-R1 (83.8 percent) but behind o1-mini (84.8 percent).On BFCL (function calling), QwQ-32B achieved 66.4 percent, better than DeepSeek-R1 (60.3 percent), and o1-mini (62.8 percent).Behind the news:DeepSeek’s initial model, DeepSeek-R1-Zero, similarly applied RL to a pretrained model. That effort produced strong reasoning but poor readability (for example, math solutions with correct steps but jumbled explanations). To address this shortcoming, the teamfine-tunedDeepSeek-R1 on long chain-of-thought examples before applying RL. In contrast, QwQ-32B skipped preliminary fine-tuning and applied RL in two stages, first optimizing for correct responses and then for readability.Why it matters:RL can dramatically boost LLMs’ reasoning abilities, but the order in which different behaviors are rewarded matters. Using RL in stages enabled the team to build a 32 billion parameter model — small enough to run locally on a consumer GPU — that rivals a much bigger mixture-of-experts model, bringing powerful reasoning models within reach for more developers. The Qwen team plans to scale its RL approach to larger models, which could improve the next-gen reasoning abilities further while adding greater knowledge.We’re thinking:How far we’ve come since “Let’s think step by step”!Microsoft Tackles Voice-In, Text-OutMicrosoft debuted its first official large language model that responds to spoken input.What’s new:Microsoft releasedPhi-4-multimodal, an open weights model that processes text, images, and speech simultaneously.Input/output:Text, speech, images in (up to 128,000 tokens); text out (0.34 seconds to first token, 26 tokens per second)Performance:State of the art in speech transcription. Comparable to similar models in other tasksKnowledge cutoff:June 2024Architecture:transformer, 5.6 billion parametersFeatures:Text-image-speech processing, multilingual, tool use.Undisclosed:Training datasets, output sizeThe company also releasedPhi-4-mini, an open weights 3.8 billion-parameter version of its biggest large language model (LLM),Phi-4. Phi-4-mini outperforms larger models including Llama 3.1 8B and Ministral-2410 8B on some benchmarks.Availability/price:Weights are free todownloadfor noncommercial and commercial use under aMIT license.How it works:Phi-4-multimodal has six components: Phi-4-mini, vision and speech encoders as well as corresponding projectors (which modify the vision or speech embeddings so the base model can understand them), and two LoRA adapters. The LoRA adapters modify the base weights depending on the input: One adapter modifies them for speech-text problems, and one for vision-text and vision-speech problems.The speech encoder is aConformer(which combines convolutional layers with a transformer) and the speech projector is a vanilla neural network. They trained Phi-4-multimodal to convert 2 million hours of speech to text, modifying only the speech encoder and projector. They further trained the system to convert speech to text, translate speech to other languages, summarize speech, and answer questions about speech, modifying only the speech encoder and the speech-text LoRA adapter.The vision encoder is based on a pretrainedSigLIP-400Mvision transformer, and the vision projector is a vanilla neural network. They trained the model to process text and images in four stages: (i) They trained Phi-4-multimodal to caption images, modifying only the vision projector. (ii) They trained the system on 500 billion tokens to caption images, transcribe text in images, and perform other tasks, modifying only the vision encoder and projector. (iii) They trained the system to answer questions about images, charts, tables, and diagrams and to transcribe text in images, modifying the vision encoder, project, and vision-text LoRA adapter. (iv) Finally, they trained the system to compare images and summarize videos, modifying only the vision projector and vision-text LoRA adapter.To adapt Phi-4-multimodal for images and speech, they trained the system to generate the text responses to a subset of the text-vision data that had been converted to speech-image using a proprietary text-to-speech engine, modifying only the text-vision LoRA adapter, vision encoder, and vision projector.Example inference: Given a question as speech and an image, the audio encoder and projector convert the speech to tokens, and the image encoder and projector convert the image into tokens. Given the tokens, Phi-4-multimodal, which uses the weights of Phi-4-mini modified by the vision-text/vision-speech LoRA adapter, generates a text response.Results:The authors compared Phi-4-multimodal to other multimodal models on text-vision, vision-speech, text-speech tasks.Across 11 text-vision benchmarks, Phi-4-multimodal came in fourth out of 11 models. It outperformed Qwen2.5-VL-3B, Claude 3.5 Sonnet, and GPT 4o-mini. It trailed Qwen2.5-VL-7B, GPT-4o, and Gemini-2 Flash.Across fourvision-speech benchmarks, Phi-4-multimodal outperformed by at least 6 percentage points Gemini-2.0-Flash, Gemini-2.0-Flash-Lite-preview, and InternOmni.Phi-4-multimodal outperformed all competitors in Microsoft’s report (including Qwen2-audio, Gemini 2.0 Flash, and GPT-4o) at transcribing speech from textinthreedatasets. It also achieved competitive performance in speech translation, outperforming its competitors on two of four datasets.Behind the news:This work adds to the growing body of models with voice-in/text-out capability, including the open weightsDiVAmodel developed by a team led by Diyi Yang at Stanford University.Why it matters:The architectural options continue to expand for building neural networks that process text, images, audio, and various combinations. While some teams maintain separate models for separate data modalities, likeQwen2.5(for text) andQwen2.5-VL) (for vision-language tasks), others are experimenting with mixture-of-expert models likeDeepSeek-V3. Phi-4-multimodal shows that Mixture-of-LoRAs is an effective approach for processing multimodal data — and gives developers a couple of new open models to play with.We’re thinking:Output guardrails have been built to ensure appropriateness of text output, but this is difficult to apply to a voice-in/voice-out architecture. (Some teams have worked on guardrails that screen audio output directly, but the technology is still early.) For voice-based applications, a voice-in/text-out model can generate a candidate output without a separate, explicit speech-to-text step, and it accommodates text-based guardrails before it decides whether or not to read the output to the user.Judge Upholds Copyright in AI Training CaseA United States court delivered a major ruling that begins to answer the question whether, and under what conditions, training an AI system on copyrighted material is considered fair use that doesn’t require permission.What’s new:A U.S. Circuit judgeruledon a claim by the legal publisher Thomson Reuters that Ross Intelligence, an AI-powered legal research service, could not claim that training its AI system on materials owned by Thomson Reuters was a so-called “fair use.” Training the system did not qualify as fair use, he decided, because its output competed with Thomson Reuters’ publications.How it works:Thomson Reuters hadsuedRoss Intelligence after the defendant trained an AI model using 2,243 works produced by Thomson Reuters without the latter’s permission. This ruling reversed an earlier decision in 2023, when the same judge had allowed Ross Intelligence’s fair-use defense to proceed to trial. In the new ruling, he found that Ross Intelligence’s use failed to meet the definition of fair use in key respects. (A jury trial is scheduled to determine whether Thomson Reuters' copyright was in effect at the time of the infringement and other aspects of the case.)Ross Intelligence’s AI-powered service competed directly with Thomson Reuters, potentially undermining its market by offering a derivative product without licensing its works. Use in a competing commercial product undermines a key factor in fair use.The judge found that Ross Intelligence’s use was commercial and not transformative, meaning it did not significantly alter or add new meaning to Thomson Reuters’ works — another key factor in fair use. Instead, it simply repackaged the works.The ruling acknowledged that Thomson Reuters’ works were not highly creative but noted that they possessed sufficient originality for copyright protection due to the editorial creativity and judgment involved in producing it.Although Ross Intelligence used only small portions of Thomson Reuters’ works, this did not weigh strongly in favor of fair use because those portions represented the most important summaries produced by Ross Intelligence.Behind the news:The ruling comes amid awaveof lawsuits over AI training and copyright in several countries. Many of these cases are in progress, but courts have weighed in on some.The New York TimesissuingOpenAI and Microsoft, arguing that their models generate output that competes with its journalism.Condé Nast, McClatchy, and other major publishers recentlyfileda lawsuit against Cohere, accusing it of using copyrighted news articles to train its AI models.Sony, UMG, and Warner Musicfiledlawsuits against AI music companies including Suno and Udio for allegedly using copyrighted recordings without permission.A judgedismissedkey arguments brought by software developers who claimed that GitHub Copilot was trained on software they created in violation of open source licenses. The judge ruled in favor of Microsoft and OpenAI.In Germany, the publisher of the LAION datasetwona case in which a court ruled that training AI models on publicly available images did not violate copyrights.Why it matters:The question of whether training (or copying data to train) AI systems is a fair use of copyrighted works hangs over the AI industry, from academic research to commercial projects. In the wake of this ruling, courts may be more likely to reject a fair-use defense when AI companies train models on copyrighted material to create output that overlaps with or replaces traditional media, asThe New York Timesalleges in its lawsuit against OpenAI. However, the ruling leaves room for fair use with respect to models whose output doesn’t compete directly with copyrighted works.We’re thinking:Current copyright laws weren’t designed with AI in mind, and rulings like this one fill in the gaps case by case.Clarifying copyrightfor the era of generative AI could help our field move forward faster.DeepSeek-R1 UncensoredLarge language models built by developers in China may, in some applications, be less useful outside that country because they avoid topics its government deems politically sensitive. A developer fine-tuned DeepSeek-R1 to widen its scope without degrading its overall performance.What’s new:Perplexity releasedR1 1776, a version ofDeepSeek-R1that responds more freely than the original. The model weights are available todownloadunder a commercially permissive MITlicense.How it works:The team modified DeepSeek-R1’s knowledge of certain topics by fine-tuning it on curated question-answer pairs.Human experts identified around 300 topics that are censored in China.The authors developed a multilingual classifier that spots text related to these topics.They identified 40,000 prompts that the classifier classified as sensitive with high confidence. They discarded those that contained personally identifiable information.For each prompt, they produced factual, chain-of-thought responses that mirrored DeepSeek-R1's typical reasoning processes.They fine-tuned DeepSeek-R1 on the resulting prompt-response pairs.Results:The fine-tuned model responded to politically charged prompts factually without degrading its ability to generate high-quality output.The authors fed their model 1,000 diverse prompts that covered frequently censored topics. An unspecified combination of human and AI judges rated the models' responses according to the degree to which they are (i) evasive and (ii) censored outright.100 percent of the fine-tuned model’s responses were rated uncensored, whereas the original version censored around 85 percent of sensitive queries. By comparison, DeepSeek-V3 censored roughly 73 percent, Claude-3.5-Sonnet around 5 percent, o3-mini about 1 percent, and GPT-4o 0 percent.Evaluated on four language and math benchmarks (MMLU, DROP, MATH-500, and AIME 2024) and unspecified internal benchmarks, the fine-tuned and original models performed nearly identically. Their scores differed by a few tenths of a percent except on AIME 2024 (competitive high-school math problems), where the fine-tuned model achieved 79.8 percent compared to the original’s 80.96 percent.Behind the news:Amongthe first countries to regulate AI, ChinarequiresAI developers to build models that uphold “Core Socialist Values” and produce true and reliable output. When these objectivesconflict, the political goal tends to dominate. While large language models built by developers in China typically avoid contentious topics, the newer DeepSeek models enforce this more strictly than older models like Qwen and Yi, using methods akin to Western measures for aligning output, like Reinforcement Learning from Human Feedback andkeyword filters.Why it matters:AI models tend to reflect their developers’ values and legal constraints. Perplexity’s targeted fine-tuning approach addresses this barrier to international adoption of open-source models.We’re thinking:As models with open weights are adopted by the global community, they become a source of soft power for their developers, since they tend to reflect their developers’ values. This work reflects a positive effort to customize a model to reflect the user’s values instead — though how many developers will seek out a fine-tuned version rather than the original remains to be seen.\nLoading theElevenlabs Text to SpeechAudioNative Player...\n\n\nShare\nShare\n\nShare\n",
    "image_urls": [
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--54-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/03/The-Batch-ads-and-exclusive-banners---2025-03-11T092327.321.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--60-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--61-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--55-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--62-.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2023/10/GenAI4E_sidebanner.png"
    ]
  },
  {
    "title": "issue 291",
    "date": null,
    "url": "https://www.deeplearning.ai/the-batch/issue-291/",
    "content": "The Batch\nWeekly Issues\nissue 291\nPublishedMar 5, 2025\nPublished\n\nPublished\nMar 5, 2025\nReading time13min read\nReading time\n\nReading time\n13min read\nPublishedMar 05, 2025Reading time13min readShareLoading theElevenlabs Text to SpeechAudioNative Player...Dear friends,Continuing our discussion on theVoice Stack, I’d like to explore an area that today’s voice-based systems mostly struggle with: Voice Activity Detection (VAD) and the turn-taking paradigm of communication.When communicating with a text-based chatbot, the turns are clear: You write something, then the bot does, then you do, and so on. The success of text-based chatbots with clear turn-taking has influenced the design of voice-based bots, most of which also use the turn-taking paradigm.A key part of building such a system is a VAD component to detect when the user is talking. This allows our software to take the parts of the audio stream in which the user is saying something and pass that to the model for the user’s turn. It also supports interruption in a limited way, whereby if a user insistently interrupts the AI system while it is talking, eventually the VAD system will realize the user is talking, shut off the AI’s output, and let the user take a turn. This works reasonably well in quiet environments.However, VAD systems today struggle with noisy environments, particularly when the background noise is from other human speech. For example, if you are in a noisy cafe speaking with a voice chatbot, VAD — which is usually trained to detect human speech — tends to be inaccurate at figuring out when you, or someone else, is talking. (In comparison, it works much better if you are in a noisy vehicle, since the background noise is more clearly not human speech.) It might think you are interrupting when it was merely someone in the background speaking, or fail to recognize that you’ve stopped talking. This is why today’s speech applications often struggle in noisy environments.Intriguingly, last year, Kyutai Labs publishedMoshi, a model (GitHub) that had many technical innovations. An important one was enabling persistent bi-direction audio streams from the user to Moshi and from Moshi to the user.If you and I were speaking in person or on the phone, we would constantly be streaming audio to each other (through the air or the phone system), and we’d use social cues to know when to listen and how to politely interrupt if one of us felt the need. Thus, the streams would not need to explicitly model turn-taking. Moshi works like this. It’s listening all the time, and it’s up to the model to decide when to stay silent and when to talk. This means an explicit VAD step is no longer necessary. (Moshi also included other innovations, such as an “inner monologue” that simultaneously generates text alongside the audio to improve the quality of responses as well as audio encoding.)Just as the architecture of text-only transformers has gone through many evolutions (such as encoder-decoder models, decoder-only models, and reasoning models that generate a lot of “reasoning tokens” before the final output), voice models are going through a lot of architecture explorations. Given the importance of foundation models with voice-in and voice-out capabilities, many large companies right now are investing in developing better voice models. I’m confident we’ll see many more good voice models released this year.It feels like the space of potential innovation for voice remains large. Hard technical problems, like the one of latency that I described last week and VAD errors, remain to be solved. As solutions get better, voice-to-voice will continue to be a promising category to build applications in.Keep building!AndrewA MESSAGE FROM DEEPLEARNING.AILearn to build an event-driven AI agent that processes documents and fills out forms using RAG, workflows, and human-in-the-loop feedback. This course, built in partnership with LlamaIndex, walks you through designing, building, and refining automated document workflows.Enroll for freeNewsText Generation by DiffusionTypical large language models are autoregressive, predicting the next token, one at a time, from left to right. A new model hones all text tokens at once.What’s new:Inception Labs, a Silicon Valley startup, emerged from stealth mode withMercury Coder, a diffusion model that generates code, in small and mini versions. Registered users can try it outhere, and an API (sign up for early accesshere) and on-premises deployments are in the works. The company has not yet announced availability and pricing.How it works:Like image diffusion models, Mercury Coder improves its output over a number of steps by removing noise.Inception Labs shared little information about the model, leaving details including parameter count, input size and output size, training data, and training methods undisclosed.An October 2023paperco-authored by an Inception Labs co-founder describes training a text diffusion model using score entropy. The model learned to estimate the transition ratio between two tokens; that is, the probability that token y is correct over the probability that the current token x is correct.In their most successful experiments, the authors added noise to tokens by progressively masking an ever-greater percentage of tokens at random over several steps.At inference, the model started with masked tokens and unmasked them over a number of steps. The estimated transition ratio determined how to change each token at each step.Results:Mercury Coder’s major advantage is speed, but it also performs well compared to several competitors.The Small and Mini versions are 3.5 to 18 times faster than comparable small coding models. Running on an Nvidia H100 graphics processing unit, Mercury Coder Small generates 737 tokens per second and Mercury Coder Mini generates 1,109 tokens per second. In comparison, Qwen 2.5 Coder 7B generates 207 tokens per second and GPT 4o-Mini generates 59 tokens per second.On coding tasks across six benchmarks, Mercury Coder Small outperforms Gemini 2.0 Flash-Lite, Claude 3.5 Haiku, GPT-4o Mini, and Qwen 2.5 Coder 7B on at least four. Mercury Coder Mini beats those models on at least two. Both versions of Mercury Coder lost to DeepSeek Coder V2 Lite on all six benchmarks.Behind the news:Several teams have built diffusion models that generate text, but previous efforts have not been competitive with autoregressive large language models (LLMs). Recently,LLaDAshowed comparable performance to Meta’s Llama 2 7B but fell short of Llama 3 8B and other similarly sized modern LLMs.Why it matters:Text diffusion models are already faster than autoregressive models. They offer significant promise to accelerate text generation even further.We’re thinking:Diffusion image generators have delivered good output with as little as four or even one step, generating output tokens significantly faster than autoregressive models. If text diffusion models can benefit from improvements in image generation, they could lead to rapid generation of lengthy texts and, in turn, faster agents and reasoning.OpenAI’s GPT-4.5 Goes BigOpenAI launched GPT-4.5, which may be its last non-reasoning model.What’s new:GPT-4.5 isavailableas a research preview. Unlike OpenAI’s recent models o1 and o3, GPT-4.5 is not fine-tuned to reason by generating a chain of thought, although the company hinted that it may serve as a basis of a reasoning model in the future. Instead, it’s a huge model that was trained using a huge amount of computation. As OpenAI’s biggest model to date, GPT-4.5 isvery expensiveto run, and the company is evaluating whether to offer it via API in the long term.Input/output:text and images in, text out. Voice and video interactions may be available in future updates.Availability/price:Via ChatGPT (currently ChatGPT Pro; soon ChatGPT Plus, Team, Enterprise, and Edu) and various APIs (Chat Completions, Assistants, and Batch). $75/$150 per million input/output tokens.Features:Web search, function calling, structured output, streaming, system messages,canvascollaborative user interface.Undisclosed:Parameter count, input and output size, architecture, training data, training methods.How it works:OpenAI revealed fewdetailsabout how GPT-4.5 was built. The model is bigger than GPT-4o, and it was pretrained and fine-tuned on more data using more computation — possibly 10x more, given OpenAI’s comment that “with every new order of magnitude of compute comes novel capabilities.”The model was trained on a combination of publicly available data and data from partnerships and in-house datasets, including data generated by smaller models. Its knowledge cutoff is October 2023.The data was filtered for quality, to eliminate personally identifying information, and to eliminate information that might contribute to proliferation of chemical, biological, radiological, and nuclear threats.OpenAI developed unspecified techniques to scale up unsupervised pretraining, supervised fine-tuning, and alignment.Performance:“This isn’t a reasoning model and won’t crush benchmarks,” OpenAI CEO Sam Altman warned in atweet. The company claims that GPT-4.5 offers improved general knowledge, adheres to prompts with more nuance, delivers greater creativity, and has higher emotional intelligence.GPT-4.5 shows less propensity to hallucinate, or confabulate information, than other OpenAI models. On PersonQA (questions that involve publicly available facts about people), GPT-4.5 achieved 78 percent accuracy compared to GPT-4o (28 percent accuracy) and o1 (55 percent accuracy). Moreover, GPT-4.5 achieved a hallucination rate (lower is better) of 0.19 compared to GPT-4o (0.52) and o1 (0.20).Its performance on coding benchmarks is mixed. OnSWE-Bench Verified, GPT-4.5 achieved a 38 percent pass rate, higher than GPT-4o (30.7 percent) but well belowdeep research(61 percent), an agentic workflow that conducts multi-step research on the internet. OnSWE-Lancer Diamond, which evaluates full-stack software engineering tasks, GPT-4.5 solved 32.6 percent of tasks, outperforming GPT-4o (23.3 percent) and o3-mini (10.8 percent) but again lagging deep research (around 48 percent).Behind the news:GPT-4.5’s release comes as OpenAI nears an announcedtransitionaway from developing separate general-knowledge and reasoning models. The launch also comes as OpenAI faces an ongoing shortage of processing power. CEO Sam Altmansaidthat the company is “out of GPUs” and struggling to meet demand — a constraint that may impact whether OpenAI continues to offer GPT-4.5 via API.Why it matters:GPT-4.5 highlights a growing divide in AI research over whether to pursue performance gains by scaling up processing during pretraining or inference. Despite the success of approaches that consume extra processing power at inference, such as agentic techniques and reasoning models such as its own o family, OpenAI clearly still sees value in pretraining larger and larger models.We’re thinking:There’s still more juice to be squeezed out of bigger models! We’re excited to see what the combination of additional compute applied to both pretraining and inference can achieve.Budget for Reasoning to the TokenAnthropic’s Claude 3.7 Sonnet implements a hybrid reasoning approach that lets users decide how much thinking they want the model to do before it renders a response.What’s new:Claude 3.7 Sonnetwas trained for strong performance in coding and front-end web development, with less emphasis on math and computer-science competition problems. It implements tool use and computer use (but not web search) and lets users toggle between immediate responses andextended thinking mode, which can improve outputs by allocating a specific number of tokens to reasoning at inference. Like DeepSeek-R1 and Google Gemini Flash Thinking — and unlike OpenAI o1 — Claude 3.7 Sonnet fully displays reasoning tokens. Anthropic considers this functionality experimental, so it may change.Input/output:text and images in (up to 200,000 tokens), text out (up to 128,000 tokens).Availability/price:Via Anthropic tiers Free (extended thinking not available), Pro, Team, and Enterprise; Anthropic API; Amazon Bedrock; Google Cloud Vertex AI. $3/$15/$15 per million input/output/thinking tokens.Undisclosed:parameter count, architecture, training data, training method.Anthropic also introduced Claude Code, a command-line tool for AI-assisted coding, which is available as a limited research preview. Claude Code can edit files, write and run tests, commit and push code to GitHub, and use command-line tools.How it works:Anthropic pretrained Claude 3.7 Sonnet on a mix of public and proprietary data (which explicitly did not include Claude users’ inputs and outputs). The team fine-tuned Claude 3.7 Sonnet usingconstitutional AI, which encourages a model to follow a set of human-crafted rules.When the model’s extended thinking mode is enabled, API users can control the thinking budget by specifying a number of tokens up to 128,000. (The specified budget is a rough target, so the number of tokens consumed may differ.)Anthropic says that extended thinking mode often is more effective given a general instruction to “think deeply” rather than step-by-step instructions.Visible thinking tokens are considered a research preview while Anthropic examines how they affect user interactions with the model. The company highlights three issues: Visible thinking tokens don’t reflect the model’s internal instructions that establish its character and therefore seem to be devoid of personality, they may not reflect the model’s actual reasoning process, and they can reveal flaws that malicious actors may exploit.Extended thinking mode processes tokens serially, but Anthropic is experimenting with parallel thinking that follows multiple independent thought processes and chooses the best one according to a majority vote.Performance:Claude 3.7 Sonnet shows exceptional performance in general knowledge, software engineering, and agentic tasks.On theGPQA Diamond(graduate-level science questions), Claude 3.7 Sonnet achieved 84.8 percent in parallel extended thinking mode with a 64,000-token budget. By comparison, X’s Grok 3 beta achieved 84.6 percent (majority voting with 64 tries), and OpenAI’s o3-mini achieved 79.7 percent with high effort.OnSWE-Bench Verified, which evaluates the ability to solve real-world software engineering problems, Claude 3.7 Sonnet achieved 70.3 percent without extended thinking, averaged over 16 trials. OpenAI’s o3-mini achieved 49.3 percent with high effort, and DeepSeek R1 achieved 49.2 percent with extended thinking, 32,000 tokens.TAU-bench evaluates agentic reasoning. On the Retail subset, which assesses performance in product recommendations and customer service, Claude 3.7 Sonnet achieved 81.2 percent without extended thinking, outperforming OpenAI’s o1 (73.5 percent). In the Airline subset, which measures multi-step reasoning in tasks like flight bookings and customer support, Claude 3.7 Sonnet achieved 58.4 percent, likewise ahead of o1 (54.2 percent).OnAIME 2024, competitive high-school math problems, Claude 3.7 Sonnet achieved 80.0 percent in parallel extended thinking mode with a 64,000-token budget. In this test, it underperformed o3-mini with high effort (87.3 percent) and o1 (83.3 percent).Behind the news:Anthropic’s approach refines earlier efforts to enable users to control the incremental expense of computing extra tokens at inference. For instance, OpenAI o1 offers three levels of reasoning or “effort” — each of which allocates more tokens to reasoning — while X’sGrok 3offers two.Why it matters:Test-time compute, or additional processing at inference, is powerful but expensive, and not all tasks benefit from it. So it’s helpful to let users choose how much to apply. Claude 3.7 Sonnet improves its predecessor’s general performance and provides an ample budget for additional reasoning.We’re thinking:The cost of inference is rising as agentic workflows and other compute-intensive tasks become more widely used. Yet the cost of AI on a per-token basis isfallingrapidly. Intelligence is becoming steadily cheaper and more plentiful.Amazon’s Next-Gen Voice AssistantAmazon announced Alexa+, a major upgrade to its long-running voice assistant.What’s new:Alexa+, which accepts spoken commands and responds conversationally, is designed to work with a variety of vendors as an autonomous agent to make purchases, book reservations, play media, and so on. It will roll out in the U.S. over coming weeks, initially on some Echo Show devices and eventually nearly every current Echo speaker.How it works:Alexa+updatesthe system to take advantage of generative AI including Anthropic Claude,Amazon Nova, and other large language models. Inputs are filtered through a routing system that determines the best model to respond to any given request. It’s trained to understand colloquial, conversational language. Its personality is designed to be “smart, considerate, empathetic, and inclusive” as well as humorous.Alexa+  interacts with online vendors to manage smart-home devices (Philips Hue, Ring, Roborock), reserve restaurant seats (OpenTable, Vagaro), play music (Amazon Music, Spotify, Apple Music, iHeartRadio) and videos (Amazon Video, Hulu, Netflix, Disney+), book local service technicians (Thumbtack), and purchase items (Amazon Fresh, Whole Foods, Grubhub, Uber Eats, Ticketmaster). Amazon+ will cost $19.99 per month, free with an Amazon Prime membership ($139 per year). (Disclosure: Andrew Ng is a member of Amazon’s board of directors.)The system recognizes individual users and keeps track of personalized information such as dates; recipes, and preferences in sports, food, music, and movies. In addition, it can respond to queries based on purchase records, video and music playbacks, shipping addresses, documents, emails, photos, messages, and so on.It can behave proactively, for instance, advising users to start their commute early if traffic is heavy.The system calls what Amazon calls experts — groups of systems, APIs, and instructions — that orchestrate API calls to accomplish online tasks. For instance, it can navigate and use the web to perform tasks such as finding and booking, say, a local repair service to fix a broken household appliance.Alexa+ can deliver timely news and information based on partnerships with news sources includingAssociated Press,Business Insider,Politico,Reuters,USA Today, andThe Washington Post.Behind the news:Amazon launched Alexa in 2014, and the voice assistant now resides in over 600 million devices worldwide. However, users relied on it more to set timers, report sports scores, and play music than to purchase products, and Alexa revenue lagged. Following cutbacks in 2021, Amazon mademultibillion-dollarinvestmentsin Anthropic and set about updating the technology for the generative AI era.Why it matters:Alexa, along with Apple’s Siri and Google Assistant, pioneered the market for voice assistants. However, as large language models (LLMs) blossomed, all three systems fell behind the times. (Google allows Android users to substitute one of its Gemini LLMs for Google Assistant, but the system still calls Google Assistant for some tasks.) Alexa+ is the first major voice-assistant update that aims to take advantage of LLMs as well as emerging agentic technology and improved voice interactions, and the rollout is taking these capabilities to a large, existing user base.We’re thinking:Rapid improvements in thevoice stackare opening doors not only for voice assistants but for a galaxy of applications that rely on spoken input and output. Product designers will need to learn how to design smooth user voice experiences. Watching how Alexa+ manages them will provide useful guidelines.Share\nPublishedMar 05, 2025\nPublished\n\nPublished\nMar 05, 2025\nMar 05, 2025\nReading time13min read\nReading time\n\nReading time\n13min read\nShare\nShare\n\nShare\n\nLoading theElevenlabs Text to SpeechAudioNative Player...Dear friends,Continuing our discussion on theVoice Stack, I’d like to explore an area that today’s voice-based systems mostly struggle with: Voice Activity Detection (VAD) and the turn-taking paradigm of communication.When communicating with a text-based chatbot, the turns are clear: You write something, then the bot does, then you do, and so on. The success of text-based chatbots with clear turn-taking has influenced the design of voice-based bots, most of which also use the turn-taking paradigm.A key part of building such a system is a VAD component to detect when the user is talking. This allows our software to take the parts of the audio stream in which the user is saying something and pass that to the model for the user’s turn. It also supports interruption in a limited way, whereby if a user insistently interrupts the AI system while it is talking, eventually the VAD system will realize the user is talking, shut off the AI’s output, and let the user take a turn. This works reasonably well in quiet environments.However, VAD systems today struggle with noisy environments, particularly when the background noise is from other human speech. For example, if you are in a noisy cafe speaking with a voice chatbot, VAD — which is usually trained to detect human speech — tends to be inaccurate at figuring out when you, or someone else, is talking. (In comparison, it works much better if you are in a noisy vehicle, since the background noise is more clearly not human speech.) It might think you are interrupting when it was merely someone in the background speaking, or fail to recognize that you’ve stopped talking. This is why today’s speech applications often struggle in noisy environments.Intriguingly, last year, Kyutai Labs publishedMoshi, a model (GitHub) that had many technical innovations. An important one was enabling persistent bi-direction audio streams from the user to Moshi and from Moshi to the user.If you and I were speaking in person or on the phone, we would constantly be streaming audio to each other (through the air or the phone system), and we’d use social cues to know when to listen and how to politely interrupt if one of us felt the need. Thus, the streams would not need to explicitly model turn-taking. Moshi works like this. It’s listening all the time, and it’s up to the model to decide when to stay silent and when to talk. This means an explicit VAD step is no longer necessary. (Moshi also included other innovations, such as an “inner monologue” that simultaneously generates text alongside the audio to improve the quality of responses as well as audio encoding.)Just as the architecture of text-only transformers has gone through many evolutions (such as encoder-decoder models, decoder-only models, and reasoning models that generate a lot of “reasoning tokens” before the final output), voice models are going through a lot of architecture explorations. Given the importance of foundation models with voice-in and voice-out capabilities, many large companies right now are investing in developing better voice models. I’m confident we’ll see many more good voice models released this year.It feels like the space of potential innovation for voice remains large. Hard technical problems, like the one of latency that I described last week and VAD errors, remain to be solved. As solutions get better, voice-to-voice will continue to be a promising category to build applications in.Keep building!AndrewA MESSAGE FROM DEEPLEARNING.AILearn to build an event-driven AI agent that processes documents and fills out forms using RAG, workflows, and human-in-the-loop feedback. This course, built in partnership with LlamaIndex, walks you through designing, building, and refining automated document workflows.Enroll for freeNewsText Generation by DiffusionTypical large language models are autoregressive, predicting the next token, one at a time, from left to right. A new model hones all text tokens at once.What’s new:Inception Labs, a Silicon Valley startup, emerged from stealth mode withMercury Coder, a diffusion model that generates code, in small and mini versions. Registered users can try it outhere, and an API (sign up for early accesshere) and on-premises deployments are in the works. The company has not yet announced availability and pricing.How it works:Like image diffusion models, Mercury Coder improves its output over a number of steps by removing noise.Inception Labs shared little information about the model, leaving details including parameter count, input size and output size, training data, and training methods undisclosed.An October 2023paperco-authored by an Inception Labs co-founder describes training a text diffusion model using score entropy. The model learned to estimate the transition ratio between two tokens; that is, the probability that token y is correct over the probability that the current token x is correct.In their most successful experiments, the authors added noise to tokens by progressively masking an ever-greater percentage of tokens at random over several steps.At inference, the model started with masked tokens and unmasked them over a number of steps. The estimated transition ratio determined how to change each token at each step.Results:Mercury Coder’s major advantage is speed, but it also performs well compared to several competitors.The Small and Mini versions are 3.5 to 18 times faster than comparable small coding models. Running on an Nvidia H100 graphics processing unit, Mercury Coder Small generates 737 tokens per second and Mercury Coder Mini generates 1,109 tokens per second. In comparison, Qwen 2.5 Coder 7B generates 207 tokens per second and GPT 4o-Mini generates 59 tokens per second.On coding tasks across six benchmarks, Mercury Coder Small outperforms Gemini 2.0 Flash-Lite, Claude 3.5 Haiku, GPT-4o Mini, and Qwen 2.5 Coder 7B on at least four. Mercury Coder Mini beats those models on at least two. Both versions of Mercury Coder lost to DeepSeek Coder V2 Lite on all six benchmarks.Behind the news:Several teams have built diffusion models that generate text, but previous efforts have not been competitive with autoregressive large language models (LLMs). Recently,LLaDAshowed comparable performance to Meta’s Llama 2 7B but fell short of Llama 3 8B and other similarly sized modern LLMs.Why it matters:Text diffusion models are already faster than autoregressive models. They offer significant promise to accelerate text generation even further.We’re thinking:Diffusion image generators have delivered good output with as little as four or even one step, generating output tokens significantly faster than autoregressive models. If text diffusion models can benefit from improvements in image generation, they could lead to rapid generation of lengthy texts and, in turn, faster agents and reasoning.OpenAI’s GPT-4.5 Goes BigOpenAI launched GPT-4.5, which may be its last non-reasoning model.What’s new:GPT-4.5 isavailableas a research preview. Unlike OpenAI’s recent models o1 and o3, GPT-4.5 is not fine-tuned to reason by generating a chain of thought, although the company hinted that it may serve as a basis of a reasoning model in the future. Instead, it’s a huge model that was trained using a huge amount of computation. As OpenAI’s biggest model to date, GPT-4.5 isvery expensiveto run, and the company is evaluating whether to offer it via API in the long term.Input/output:text and images in, text out. Voice and video interactions may be available in future updates.Availability/price:Via ChatGPT (currently ChatGPT Pro; soon ChatGPT Plus, Team, Enterprise, and Edu) and various APIs (Chat Completions, Assistants, and Batch). $75/$150 per million input/output tokens.Features:Web search, function calling, structured output, streaming, system messages,canvascollaborative user interface.Undisclosed:Parameter count, input and output size, architecture, training data, training methods.How it works:OpenAI revealed fewdetailsabout how GPT-4.5 was built. The model is bigger than GPT-4o, and it was pretrained and fine-tuned on more data using more computation — possibly 10x more, given OpenAI’s comment that “with every new order of magnitude of compute comes novel capabilities.”The model was trained on a combination of publicly available data and data from partnerships and in-house datasets, including data generated by smaller models. Its knowledge cutoff is October 2023.The data was filtered for quality, to eliminate personally identifying information, and to eliminate information that might contribute to proliferation of chemical, biological, radiological, and nuclear threats.OpenAI developed unspecified techniques to scale up unsupervised pretraining, supervised fine-tuning, and alignment.Performance:“This isn’t a reasoning model and won’t crush benchmarks,” OpenAI CEO Sam Altman warned in atweet. The company claims that GPT-4.5 offers improved general knowledge, adheres to prompts with more nuance, delivers greater creativity, and has higher emotional intelligence.GPT-4.5 shows less propensity to hallucinate, or confabulate information, than other OpenAI models. On PersonQA (questions that involve publicly available facts about people), GPT-4.5 achieved 78 percent accuracy compared to GPT-4o (28 percent accuracy) and o1 (55 percent accuracy). Moreover, GPT-4.5 achieved a hallucination rate (lower is better) of 0.19 compared to GPT-4o (0.52) and o1 (0.20).Its performance on coding benchmarks is mixed. OnSWE-Bench Verified, GPT-4.5 achieved a 38 percent pass rate, higher than GPT-4o (30.7 percent) but well belowdeep research(61 percent), an agentic workflow that conducts multi-step research on the internet. OnSWE-Lancer Diamond, which evaluates full-stack software engineering tasks, GPT-4.5 solved 32.6 percent of tasks, outperforming GPT-4o (23.3 percent) and o3-mini (10.8 percent) but again lagging deep research (around 48 percent).Behind the news:GPT-4.5’s release comes as OpenAI nears an announcedtransitionaway from developing separate general-knowledge and reasoning models. The launch also comes as OpenAI faces an ongoing shortage of processing power. CEO Sam Altmansaidthat the company is “out of GPUs” and struggling to meet demand — a constraint that may impact whether OpenAI continues to offer GPT-4.5 via API.Why it matters:GPT-4.5 highlights a growing divide in AI research over whether to pursue performance gains by scaling up processing during pretraining or inference. Despite the success of approaches that consume extra processing power at inference, such as agentic techniques and reasoning models such as its own o family, OpenAI clearly still sees value in pretraining larger and larger models.We’re thinking:There’s still more juice to be squeezed out of bigger models! We’re excited to see what the combination of additional compute applied to both pretraining and inference can achieve.Budget for Reasoning to the TokenAnthropic’s Claude 3.7 Sonnet implements a hybrid reasoning approach that lets users decide how much thinking they want the model to do before it renders a response.What’s new:Claude 3.7 Sonnetwas trained for strong performance in coding and front-end web development, with less emphasis on math and computer-science competition problems. It implements tool use and computer use (but not web search) and lets users toggle between immediate responses andextended thinking mode, which can improve outputs by allocating a specific number of tokens to reasoning at inference. Like DeepSeek-R1 and Google Gemini Flash Thinking — and unlike OpenAI o1 — Claude 3.7 Sonnet fully displays reasoning tokens. Anthropic considers this functionality experimental, so it may change.Input/output:text and images in (up to 200,000 tokens), text out (up to 128,000 tokens).Availability/price:Via Anthropic tiers Free (extended thinking not available), Pro, Team, and Enterprise; Anthropic API; Amazon Bedrock; Google Cloud Vertex AI. $3/$15/$15 per million input/output/thinking tokens.Undisclosed:parameter count, architecture, training data, training method.Anthropic also introduced Claude Code, a command-line tool for AI-assisted coding, which is available as a limited research preview. Claude Code can edit files, write and run tests, commit and push code to GitHub, and use command-line tools.How it works:Anthropic pretrained Claude 3.7 Sonnet on a mix of public and proprietary data (which explicitly did not include Claude users’ inputs and outputs). The team fine-tuned Claude 3.7 Sonnet usingconstitutional AI, which encourages a model to follow a set of human-crafted rules.When the model’s extended thinking mode is enabled, API users can control the thinking budget by specifying a number of tokens up to 128,000. (The specified budget is a rough target, so the number of tokens consumed may differ.)Anthropic says that extended thinking mode often is more effective given a general instruction to “think deeply” rather than step-by-step instructions.Visible thinking tokens are considered a research preview while Anthropic examines how they affect user interactions with the model. The company highlights three issues: Visible thinking tokens don’t reflect the model’s internal instructions that establish its character and therefore seem to be devoid of personality, they may not reflect the model’s actual reasoning process, and they can reveal flaws that malicious actors may exploit.Extended thinking mode processes tokens serially, but Anthropic is experimenting with parallel thinking that follows multiple independent thought processes and chooses the best one according to a majority vote.Performance:Claude 3.7 Sonnet shows exceptional performance in general knowledge, software engineering, and agentic tasks.On theGPQA Diamond(graduate-level science questions), Claude 3.7 Sonnet achieved 84.8 percent in parallel extended thinking mode with a 64,000-token budget. By comparison, X’s Grok 3 beta achieved 84.6 percent (majority voting with 64 tries), and OpenAI’s o3-mini achieved 79.7 percent with high effort.OnSWE-Bench Verified, which evaluates the ability to solve real-world software engineering problems, Claude 3.7 Sonnet achieved 70.3 percent without extended thinking, averaged over 16 trials. OpenAI’s o3-mini achieved 49.3 percent with high effort, and DeepSeek R1 achieved 49.2 percent with extended thinking, 32,000 tokens.TAU-bench evaluates agentic reasoning. On the Retail subset, which assesses performance in product recommendations and customer service, Claude 3.7 Sonnet achieved 81.2 percent without extended thinking, outperforming OpenAI’s o1 (73.5 percent). In the Airline subset, which measures multi-step reasoning in tasks like flight bookings and customer support, Claude 3.7 Sonnet achieved 58.4 percent, likewise ahead of o1 (54.2 percent).OnAIME 2024, competitive high-school math problems, Claude 3.7 Sonnet achieved 80.0 percent in parallel extended thinking mode with a 64,000-token budget. In this test, it underperformed o3-mini with high effort (87.3 percent) and o1 (83.3 percent).Behind the news:Anthropic’s approach refines earlier efforts to enable users to control the incremental expense of computing extra tokens at inference. For instance, OpenAI o1 offers three levels of reasoning or “effort” — each of which allocates more tokens to reasoning — while X’sGrok 3offers two.Why it matters:Test-time compute, or additional processing at inference, is powerful but expensive, and not all tasks benefit from it. So it’s helpful to let users choose how much to apply. Claude 3.7 Sonnet improves its predecessor’s general performance and provides an ample budget for additional reasoning.We’re thinking:The cost of inference is rising as agentic workflows and other compute-intensive tasks become more widely used. Yet the cost of AI on a per-token basis isfallingrapidly. Intelligence is becoming steadily cheaper and more plentiful.Amazon’s Next-Gen Voice AssistantAmazon announced Alexa+, a major upgrade to its long-running voice assistant.What’s new:Alexa+, which accepts spoken commands and responds conversationally, is designed to work with a variety of vendors as an autonomous agent to make purchases, book reservations, play media, and so on. It will roll out in the U.S. over coming weeks, initially on some Echo Show devices and eventually nearly every current Echo speaker.How it works:Alexa+updatesthe system to take advantage of generative AI including Anthropic Claude,Amazon Nova, and other large language models. Inputs are filtered through a routing system that determines the best model to respond to any given request. It’s trained to understand colloquial, conversational language. Its personality is designed to be “smart, considerate, empathetic, and inclusive” as well as humorous.Alexa+  interacts with online vendors to manage smart-home devices (Philips Hue, Ring, Roborock), reserve restaurant seats (OpenTable, Vagaro), play music (Amazon Music, Spotify, Apple Music, iHeartRadio) and videos (Amazon Video, Hulu, Netflix, Disney+), book local service technicians (Thumbtack), and purchase items (Amazon Fresh, Whole Foods, Grubhub, Uber Eats, Ticketmaster). Amazon+ will cost $19.99 per month, free with an Amazon Prime membership ($139 per year). (Disclosure: Andrew Ng is a member of Amazon’s board of directors.)The system recognizes individual users and keeps track of personalized information such as dates; recipes, and preferences in sports, food, music, and movies. In addition, it can respond to queries based on purchase records, video and music playbacks, shipping addresses, documents, emails, photos, messages, and so on.It can behave proactively, for instance, advising users to start their commute early if traffic is heavy.The system calls what Amazon calls experts — groups of systems, APIs, and instructions — that orchestrate API calls to accomplish online tasks. For instance, it can navigate and use the web to perform tasks such as finding and booking, say, a local repair service to fix a broken household appliance.Alexa+ can deliver timely news and information based on partnerships with news sources includingAssociated Press,Business Insider,Politico,Reuters,USA Today, andThe Washington Post.Behind the news:Amazon launched Alexa in 2014, and the voice assistant now resides in over 600 million devices worldwide. However, users relied on it more to set timers, report sports scores, and play music than to purchase products, and Alexa revenue lagged. Following cutbacks in 2021, Amazon mademultibillion-dollarinvestmentsin Anthropic and set about updating the technology for the generative AI era.Why it matters:Alexa, along with Apple’s Siri and Google Assistant, pioneered the market for voice assistants. However, as large language models (LLMs) blossomed, all three systems fell behind the times. (Google allows Android users to substitute one of its Gemini LLMs for Google Assistant, but the system still calls Google Assistant for some tasks.) Alexa+ is the first major voice-assistant update that aims to take advantage of LLMs as well as emerging agentic technology and improved voice interactions, and the rollout is taking these capabilities to a large, existing user base.We’re thinking:Rapid improvements in thevoice stackare opening doors not only for voice assistants but for a galaxy of applications that rely on spoken input and output. Product designers will need to learn how to design smooth user voice experiences. Watching how Alexa+ manages them will provide useful guidelines.\nLoading theElevenlabs Text to SpeechAudioNative Player...\n\n\nShare\nShare\n\nShare\n",
    "image_urls": [
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--56-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/The-Batch-ads-and-exclusive-banners---2025-03-03T153624.042.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--57-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--58-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--59-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--52-.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2023/10/GenAI4E_sidebanner.png"
    ]
  },
  {
    "title": "issue 297",
    "date": null,
    "url": "https://www.deeplearning.ai/the-batch/issue-297/",
    "content": "The Batch\nWeekly Issues\nissue 297\nPublishedApr 16, 2025\nPublished\n\nPublished\nApr 16, 2025\nReading time13min read\nReading time\n\nReading time\n13min read\nPublishedApr 16, 2025Reading time13min readShareLoading theElevenlabs Text to SpeechAudioNative Player...Dear friends,I’ve noticed that many GenAI application projects put in automated evaluations (evals) of the system’s output probably later — and rely on humans to manually examine and judge outputs longer — than they should. This is because building evals is viewed as a massive investment (say, creating 100 or 1,000 examples, and designing and validating metrics) and there’s never a convenient moment to put in that up-front cost. Instead, I encourage teams to think of building evals as an iterative process. It’s okay to start with a quick-and-dirty implementation (say, 5 examples with unoptimized metrics) and then iterate and improve over time. This allows you to gradually shift the burden of evaluations away from humans and toward automated evals.Iwrotepreviously about the importance and difficulty of creating evals. Say you’re building a customer-service chatbot that responds to users in free text. There’s no single right answer, so many teams end up having humans pore over dozens of example outputs with every update to judge if it improved the system. While techniques like LLM-as-judge are helpful, the details of getting this to work well (such as what prompt to use, what context to give the judge, and so on) are finicky to get right. All this contributes to the impression that building evals requires a large up-front investment, and thus on any given day, a team can make more progress by relying on human judges than figuring out how to build automated evals.I encourage you to approach building evals differently. It’s okay to build quick evals that are only partial, incomplete, and noisy measures of the system’s performance, and to iteratively improve them. They can be a complement to, rather than replacement for, manual evaluations. Over time, you can gradually tune the evaluation methodology to close the gap between the evals’ output and human judgments. For example:It’s okay to start with very few examples in the eval set, say 5, and gradually add to them over time — or subtract them if you find that some examples are too easy or too hard, and not useful for distinguishing between the performance of different versions of your system.It’s okay to start with evals that measure only a subset of the dimensions of performance you care about, or measure narrow cues that you believe are correlated with, but don’t fully capture, system performance. For example if, at a certain moment in the conversation, your customer-support agent is supposed to (i) call an API to issue a refund and (ii) generate an appropriate message to the user, you might start off measuring only whether or not it calls the API correctly and not worry about the message. Or if, at a certain moment, your chatbot should recommend a specific product, a basic eval could measure whether or not the chatbot mentions that product without worrying about what it says about it.So long as the output of the evals correlates with overall performance, it’s fine to measure only a subset of things you care about when starting.The development process thus comprises two iterative loops, which you might execute in parallel:Iterating on the system to make it perform better, as measured by a combination of automated evals and human judgment;Iterating on the evals to make them correspond more closely to human judgment.As with many things in AI, we often don’t get it right the first time. So t’s better to build an initial end-to-end system quickly and then iterate to improve it. We’re used to taking this approach to building AI systems. We can build evals the same way.To me, a successful eval meets the following criteria. Say, we currently have system A, and we might tweak it to get a system B:If A works significantly better than B according to a skilled human judge, the eval should give A a significantly higher score than B.If A and B have similar performance, their eval scores should be similar.Whenever a pair of systems A and B contradicts these criteria, that is a sign the eval is in “error” and we should tweak it to make it rank A and B correctly. This is a similar philosophy toerror analysisin building machine learning algorithms, only instead of focusing on errors of the machine learning algorithm's output — such as when it outputs an incorrect label — we focus on “errors” of the evals — such as when they incorrectly rank two systems A and B, so the evals aren’t helpful in choosing between them.Relying purely on human judgment is a great way to get started on a project. But for many teams, building evals as a quick prototype and iterating to something more mature lets you put in evals earlier and accelerate your progress.Keep building!AndrewA MESSAGE FROM DEEPLEARNING.AIBuild autonomous agents that take actions like scraping web pages, filling out forms, and subscribing to newsletters in “Building AI Browser Agents.” Explore the AgentQ framework, which helps agents self-correct using Monte Carlo tree search and direct preference optimization.Start learning todayNewsGoogle Unveils Gemini 2.5Google’s new flagship model raised the state of the art in a variety of subjective and objective tests.What’s new:Google launchedGemini 2.5 Pro Experimental, the first model in the Gemini 2.5 family, and announced thatGemini 2.5 Flash, a version with lower latency, will be available soon. All Gemini 2.5 models will have reasoning capabilities, as will all Google models going forward.Input/output:Text, audio, images, video in (up to 1 million tokens, up to 2 million tokens announced but not yet available), text out (up to 65,000 tokens,212.7 tokens per second, 26.8 seconds to first token)Performance:Currently topsChatbot ArenaAvailability/price:Limited free access viaGoogle Cloud,Google AI Studio,Vertex AI, and Gemini app and website. API $1.25/$10 per million tokens input/output up to 200,000 tokens, $2.50/$15 per million tokens input/output above 200,000 tokens.Features:Reasoning, web search, code executionUndisclosed:Architecture, parameter count, training methods, training dataHow it works:Compared toGemini 1.0andGemini 1.5, Google disclosed little information about Gemini 2.5 Pro Experimental or how it differs from previous versions.LikeGemini 2.0 Flash Thinking, Gemini 2.5 Pro Experimental is trained using reinforcement learning to generate reasoning tokens before responding to prompts. It hides such tokens but provides more general reasoning traces.Google said Gemini 2.5 Pro Experimental uses a “significantly enhanced” base model and “improved” post-training but didn’t provide details.Gemini 2.5 Pro improves on Gemini 2.0 Pro’s coding abilities and performs well on SWE-Bench Verified, a benchmark that evaluates agentic coding. Google didn’t specify details on the coding agent used for these tests, calling it a “custom agent setup.”Results:On a variety of popular benchmarks, Gemini 2.5 Pro Experimental outperforms top models from competing AI companies.As of this writing, in the Chatbot Arena, a head-to-head competition in which human users choose the best response between two anonymous models, Gemini 2.5 Pro Experimental (1437 Elo) tops the leaderboard ahead of OpenAI GPT-4o 2025-03-26 (1406 Elo) and xAI Grok 3 Preview (1402 Elo).Across 12 benchmarks, on seven of them, Gemini 2.5 Pro Experimental outperformed OpenAI o3-mini (set to high effort), OpenAI GPT-4.5, Anthropic Claude 3.7 Sonnet (64,000 extended thinking), xAI Grok 3 Beta (extended thinking), and DeepSeek-R1.Why it matters:Late last year, some observers expressedconcernsthat progress in AI was slowing. Gemini 2.5 Pro Experimental arrives shortly after rival proprietary models GPT-4.5 (currently a research preview) and Claude 3.7 Sonnet, both of which showed improved performance, yet it outperforms them on most benchmarks. Clearly there’s still room for models — particularly reasoning models — to keep getting better.We’re thinking:Google said it plans to train all its new models on chains of thought going forward. This follows a similarstatementby OpenAI. We’re sure they have their reasons!Open Standard for Tool Use and Data Access Gains MomentumOpenAI embraced Model Context Protocol, providing powerful support for an open standard that connects large language models to tools and data.What’s new:OpenAI will supportModel Context Protocol(MCP) in its Agents SDK and soon its ChatGPT desktop app and Responses API. The move will give developers who use OpenAI models access to a wide variety of pre-existing tools and proprietary data sources.How it works:Launchedby Anthropic late last year, MCP connects AI models to a growing ecosystem of plug-and-play resources, including more than 6,000community-built servers and connectors.MCP defines clients and servers. Servers expose tools and data sources that LLMs can use. Clients like Claude for Desktop or agents built using the OpenAIAgents SDKinteract with servers.Servers define tools such as internet search or file system manipulation, and users can download and run them locally or connect to servers hosted by third parties. In their code, users simply tell the client where the server(s) are running. Given a prompt, a model, behind the scenes, will retrieve a list of tools available from all servers, decide which to use, call them, and formulate and return responses.Behind the news:Momentum behind MCP has built rapidly. Last month, Microsoftintegrated MCPinto CoPilot Studio, enabling developers to build agents with access to MCP servers. Cloudflare enabled its customers todeploy remote MCP servers. In February, the AI-powered code editor Cursor enabled users toadd MCP servers.Why it matters:OpenAI’s move will make it easier for developers who use its models to connect to a variety of tools and data sources, and it helps to establish MCP as a go-to protocol for building agentic applications. Instead of figuring out manually how to integrate various providers, developers can connect to a third-party server (or download and run it themselves) and tie it into existing workflows with a few lines of code.We’re thinking:Kudos to Anthropic, OpenAI, and other competitors who realize it’s better to solve shared problems together than fragment the industry.The Fall and Rise of Sam AltmanA behind-the-scenesaccountprovides new details about the abrupt firing and reinstatement of OpenAI CEO Sam Altman in November 2023.How it works:Based on insider accounts, an excerpt from a forthcoming book about OpenAI by Wall Street Journal reporter Keach Hagey describes conflicts, accusations, and shifting alliances that led to Altman’s brief ouster and rapid return.Firing and reinstatement:OpenAI’s board of directors came to distrust Altman but failed to persuade executives and employees that he should be replaced.In winter 2022, Altman told the board that the company’s joint safety committee with Microsoft had approved three “somewhat controversial” enhancements to GPT-4. Board member Helen Toner later learned that only one had been approved.Altman also failed to tell the board that Microsoft had tested GPT-4 in India without the committee’s approval.Board members were surprised to learn that Altman personally owned the $175 million OpenAI Startup Fund, so OpenAI investors wouldn’t see any profits. Altman claimed he didn’t benefit from the fund.CTO Mira Murati expressed doubts about Altman’s leadership to other board members. Murati, Toner, and co-founder Ilya Sutskever began to document his actions.On November 16, the board voted to fire Altman and appoint Murati interim CEO. The board members were reluctant to reveal why they’d fired Altman. At one meeting, Murati and other executives gave them 30 minutes to either explain why they fired Altman, resign, or watch the executive team quit. Nearly all OpenAI employees (including Murati and Sutskever) signed a letter threatening to quit if Altman wasn't reinstated, and the board reversed its decision.Aftermath:Since Altman’s return, Murati and all but one director who voted to remove him have left OpenAI. The issues that precipitated his departure have given way to commercial concerns as the company considers a shift from its current hybrid nonprofit/for-profit structure to fully for-profit.GPT-5 will arrive “in the next few months,” according toAltman.Meanwhile, OpenAI launchedGPT-4.1(making full, mini, and nano versions available via API) and confirmed it soon would releaseo3, a new reasoning model.OpenAI said it will release its first open model, a newlanguage model with open weights, in coming months.The company recentlyraised$40 billion, the largest-ever funding round for an AI company, increasing its valuation to $300 billion.Why it matters:The AI frontier spawns not only technical innovations but also intense interpersonal relationships and corporate politics. Such dynamics have consequences for users and the world at large: Having survived serious challenges to his leadership, Altman has emerged in a strong position to build a path of faster growth as a for-profit company upon OpenAI’s philanthropic foundation.We’re thinking:Given OpenAI’s formidable achievements, Altman’s renewed leadership marks an inflection point in the AI landscape. Without Sam Altman at the helm, OpenAI would be a very different company, with different priorities and a different future.Toward LLMs That Understand MisspellingsResearchers built a model that’s more robust to noisy inputs like misspellings, smarter about character-level information like the number of R's in strawberry, and potentially better able to understand unfamiliar languages that might share groups of letters with familiar languages. Their approach: Eliminate the tokenizer and instead integrate a system that learns to group input characters.What’s new:Artidoro Pagnoni, Ram Pasunuru, and collaborators at Meta, University of Washington, and University of Chicago introducedByte Latent Transformer(BLT), a system of transformers that processes groups of text characters (in the form of bytes) directly.Key insight:A tokenizer turns bytes (characters) into tokens (a word or part of a word) based on learned rules: Specific sequences map to particular tokens. A large language model (LLM) would be more efficient if its tokenizer considered how easy or difficult it would be to predict the next token, because then it could group tokens that commonly occur together, thus saving memory and processing power. For instance, to complete the phrase, “The capital of the United States is,” a tokenizer may generate “Washington”, then “D”, then “.C”, and finally “.” — even though it’s easy to predict that “D.C.” will follow “Washington” (that is, the number of viable options is very small). Conversely, generating the token after “D.C.” is harder, since many viable options exist. Using a small LLM to estimate the difficulty of predicting the next token enables the model to split difficult-to-predict text into smaller groups while packing easier-to-predict text into larger groups.How it works:BLT comprises four transformers (8 billion parameters total): (i) a small byte-level transformer, (ii) an encoder transformer, (iii) a so-called latent transformer, and (iv) a decoder transformer. The authors trained the system to generate the next token in 1 trillion tokens of text, including tokens drawn from a filteredversionof Common Crawl.The authors trained the byte-level transformer to generate the next byte from an input sequence of bytes.For an input sequence, the byte-level transformer predicted the probabilities of the value of the next byte. The authors used entropy, a measure of uncertainty, to decide how bytes should be grouped. If the predicted probabilities were concentrated in a particular byte value (low entropy), meaning the next byte was highly predictable, the byte was added to the current group. If the probabilities were more spread out across multiple byte values (high entropy), meaning the model was less certain, it was part of a new group.The encoder transformer learned to represent each group as a vector, while attending to preceding bytes for context.The latent transformer learned to generate the next group vector from all previous group vectors.Finally, the decoder transformer learned to reconstruct a byte sequence from a sequence of vectors.Results:On seven benchmarks that test general language and coding abilities, BLT achieved an average accuracy of 61.1 percent, outperformingLlama 3(8 billion parameters and a similar number of floating point operations to BLT) at 60.0 percent.BLT achieved 80.6 percent on the common-sense question and answer benchmarkHellaSwag, while Llama 3 (8 billion parameters and a similar number of floating point operations to BLT) achieved 79.1 percent.BLT demonstrated significantly higher resilience to noisy inputs compared to Llama 3, particularly in tasks involving character manipulation, spelling variations, and languages for which relatively little data is available. For example, in the CUTE spelling benchmark, which tests a model’s ability to recognize correctly spelled words, BLT achieved 99.9 percent accuracy while Llama 3 achieved 1.1 percent accuracy.BLT outperformed Llama 3 intranslating to English across 26 languages(including 20 with little data). It achieved 14.0 average SentencePiece BLEU score (which measures how good a machine translation is compared to a human translation over text tokenized with theSentencePiecetokenizer), while LLaMA 3 achieved 12.1 average SentencePiece BLEU.Why it matters:By working directly on bytes, BLT is inherently more robust to variations in language, which improves its performance. For instance, when prompted to insert a \"z\" after every \"n\" in \"not\", Llama 3 incorrectly completed it as \"znotz\". This happened because its tokenizer treats \"not\" as a single, indivisible token. In contrast, BLT correctly generated \"nzot,\" because it can dynamically regroup bytes and draw new boundaries. In a more practical case, instead of treating \"pizya\" and \"pizza\" as different tokens, BLT recognizes that they share nearly identical byte sequences, differing only in the bytes for \"y\" and \"z\", and therefore likely mean the same thing.We’re thinking:In some alternatives to traditional tokenization, an LLM might process much longer sequences because the number of bytes in a sentence is much larger than the number of words. This work addresses that issue by grouping bytes dynamically. The tradeoff is complexity: Instead of one transformer, we have four.Share\nPublishedApr 16, 2025\nPublished\n\nPublished\nApr 16, 2025\nApr 16, 2025\nReading time13min read\nReading time\n\nReading time\n13min read\nShare\nShare\n\nShare\n\nLoading theElevenlabs Text to SpeechAudioNative Player...Dear friends,I’ve noticed that many GenAI application projects put in automated evaluations (evals) of the system’s output probably later — and rely on humans to manually examine and judge outputs longer — than they should. This is because building evals is viewed as a massive investment (say, creating 100 or 1,000 examples, and designing and validating metrics) and there’s never a convenient moment to put in that up-front cost. Instead, I encourage teams to think of building evals as an iterative process. It’s okay to start with a quick-and-dirty implementation (say, 5 examples with unoptimized metrics) and then iterate and improve over time. This allows you to gradually shift the burden of evaluations away from humans and toward automated evals.Iwrotepreviously about the importance and difficulty of creating evals. Say you’re building a customer-service chatbot that responds to users in free text. There’s no single right answer, so many teams end up having humans pore over dozens of example outputs with every update to judge if it improved the system. While techniques like LLM-as-judge are helpful, the details of getting this to work well (such as what prompt to use, what context to give the judge, and so on) are finicky to get right. All this contributes to the impression that building evals requires a large up-front investment, and thus on any given day, a team can make more progress by relying on human judges than figuring out how to build automated evals.I encourage you to approach building evals differently. It’s okay to build quick evals that are only partial, incomplete, and noisy measures of the system’s performance, and to iteratively improve them. They can be a complement to, rather than replacement for, manual evaluations. Over time, you can gradually tune the evaluation methodology to close the gap between the evals’ output and human judgments. For example:It’s okay to start with very few examples in the eval set, say 5, and gradually add to them over time — or subtract them if you find that some examples are too easy or too hard, and not useful for distinguishing between the performance of different versions of your system.It’s okay to start with evals that measure only a subset of the dimensions of performance you care about, or measure narrow cues that you believe are correlated with, but don’t fully capture, system performance. For example if, at a certain moment in the conversation, your customer-support agent is supposed to (i) call an API to issue a refund and (ii) generate an appropriate message to the user, you might start off measuring only whether or not it calls the API correctly and not worry about the message. Or if, at a certain moment, your chatbot should recommend a specific product, a basic eval could measure whether or not the chatbot mentions that product without worrying about what it says about it.So long as the output of the evals correlates with overall performance, it’s fine to measure only a subset of things you care about when starting.The development process thus comprises two iterative loops, which you might execute in parallel:Iterating on the system to make it perform better, as measured by a combination of automated evals and human judgment;Iterating on the evals to make them correspond more closely to human judgment.As with many things in AI, we often don’t get it right the first time. So t’s better to build an initial end-to-end system quickly and then iterate to improve it. We’re used to taking this approach to building AI systems. We can build evals the same way.To me, a successful eval meets the following criteria. Say, we currently have system A, and we might tweak it to get a system B:If A works significantly better than B according to a skilled human judge, the eval should give A a significantly higher score than B.If A and B have similar performance, their eval scores should be similar.Whenever a pair of systems A and B contradicts these criteria, that is a sign the eval is in “error” and we should tweak it to make it rank A and B correctly. This is a similar philosophy toerror analysisin building machine learning algorithms, only instead of focusing on errors of the machine learning algorithm's output — such as when it outputs an incorrect label — we focus on “errors” of the evals — such as when they incorrectly rank two systems A and B, so the evals aren’t helpful in choosing between them.Relying purely on human judgment is a great way to get started on a project. But for many teams, building evals as a quick prototype and iterating to something more mature lets you put in evals earlier and accelerate your progress.Keep building!AndrewA MESSAGE FROM DEEPLEARNING.AIBuild autonomous agents that take actions like scraping web pages, filling out forms, and subscribing to newsletters in “Building AI Browser Agents.” Explore the AgentQ framework, which helps agents self-correct using Monte Carlo tree search and direct preference optimization.Start learning todayNewsGoogle Unveils Gemini 2.5Google’s new flagship model raised the state of the art in a variety of subjective and objective tests.What’s new:Google launchedGemini 2.5 Pro Experimental, the first model in the Gemini 2.5 family, and announced thatGemini 2.5 Flash, a version with lower latency, will be available soon. All Gemini 2.5 models will have reasoning capabilities, as will all Google models going forward.Input/output:Text, audio, images, video in (up to 1 million tokens, up to 2 million tokens announced but not yet available), text out (up to 65,000 tokens,212.7 tokens per second, 26.8 seconds to first token)Performance:Currently topsChatbot ArenaAvailability/price:Limited free access viaGoogle Cloud,Google AI Studio,Vertex AI, and Gemini app and website. API $1.25/$10 per million tokens input/output up to 200,000 tokens, $2.50/$15 per million tokens input/output above 200,000 tokens.Features:Reasoning, web search, code executionUndisclosed:Architecture, parameter count, training methods, training dataHow it works:Compared toGemini 1.0andGemini 1.5, Google disclosed little information about Gemini 2.5 Pro Experimental or how it differs from previous versions.LikeGemini 2.0 Flash Thinking, Gemini 2.5 Pro Experimental is trained using reinforcement learning to generate reasoning tokens before responding to prompts. It hides such tokens but provides more general reasoning traces.Google said Gemini 2.5 Pro Experimental uses a “significantly enhanced” base model and “improved” post-training but didn’t provide details.Gemini 2.5 Pro improves on Gemini 2.0 Pro’s coding abilities and performs well on SWE-Bench Verified, a benchmark that evaluates agentic coding. Google didn’t specify details on the coding agent used for these tests, calling it a “custom agent setup.”Results:On a variety of popular benchmarks, Gemini 2.5 Pro Experimental outperforms top models from competing AI companies.As of this writing, in the Chatbot Arena, a head-to-head competition in which human users choose the best response between two anonymous models, Gemini 2.5 Pro Experimental (1437 Elo) tops the leaderboard ahead of OpenAI GPT-4o 2025-03-26 (1406 Elo) and xAI Grok 3 Preview (1402 Elo).Across 12 benchmarks, on seven of them, Gemini 2.5 Pro Experimental outperformed OpenAI o3-mini (set to high effort), OpenAI GPT-4.5, Anthropic Claude 3.7 Sonnet (64,000 extended thinking), xAI Grok 3 Beta (extended thinking), and DeepSeek-R1.Why it matters:Late last year, some observers expressedconcernsthat progress in AI was slowing. Gemini 2.5 Pro Experimental arrives shortly after rival proprietary models GPT-4.5 (currently a research preview) and Claude 3.7 Sonnet, both of which showed improved performance, yet it outperforms them on most benchmarks. Clearly there’s still room for models — particularly reasoning models — to keep getting better.We’re thinking:Google said it plans to train all its new models on chains of thought going forward. This follows a similarstatementby OpenAI. We’re sure they have their reasons!Open Standard for Tool Use and Data Access Gains MomentumOpenAI embraced Model Context Protocol, providing powerful support for an open standard that connects large language models to tools and data.What’s new:OpenAI will supportModel Context Protocol(MCP) in its Agents SDK and soon its ChatGPT desktop app and Responses API. The move will give developers who use OpenAI models access to a wide variety of pre-existing tools and proprietary data sources.How it works:Launchedby Anthropic late last year, MCP connects AI models to a growing ecosystem of plug-and-play resources, including more than 6,000community-built servers and connectors.MCP defines clients and servers. Servers expose tools and data sources that LLMs can use. Clients like Claude for Desktop or agents built using the OpenAIAgents SDKinteract with servers.Servers define tools such as internet search or file system manipulation, and users can download and run them locally or connect to servers hosted by third parties. In their code, users simply tell the client where the server(s) are running. Given a prompt, a model, behind the scenes, will retrieve a list of tools available from all servers, decide which to use, call them, and formulate and return responses.Behind the news:Momentum behind MCP has built rapidly. Last month, Microsoftintegrated MCPinto CoPilot Studio, enabling developers to build agents with access to MCP servers. Cloudflare enabled its customers todeploy remote MCP servers. In February, the AI-powered code editor Cursor enabled users toadd MCP servers.Why it matters:OpenAI’s move will make it easier for developers who use its models to connect to a variety of tools and data sources, and it helps to establish MCP as a go-to protocol for building agentic applications. Instead of figuring out manually how to integrate various providers, developers can connect to a third-party server (or download and run it themselves) and tie it into existing workflows with a few lines of code.We’re thinking:Kudos to Anthropic, OpenAI, and other competitors who realize it’s better to solve shared problems together than fragment the industry.The Fall and Rise of Sam AltmanA behind-the-scenesaccountprovides new details about the abrupt firing and reinstatement of OpenAI CEO Sam Altman in November 2023.How it works:Based on insider accounts, an excerpt from a forthcoming book about OpenAI by Wall Street Journal reporter Keach Hagey describes conflicts, accusations, and shifting alliances that led to Altman’s brief ouster and rapid return.Firing and reinstatement:OpenAI’s board of directors came to distrust Altman but failed to persuade executives and employees that he should be replaced.In winter 2022, Altman told the board that the company’s joint safety committee with Microsoft had approved three “somewhat controversial” enhancements to GPT-4. Board member Helen Toner later learned that only one had been approved.Altman also failed to tell the board that Microsoft had tested GPT-4 in India without the committee’s approval.Board members were surprised to learn that Altman personally owned the $175 million OpenAI Startup Fund, so OpenAI investors wouldn’t see any profits. Altman claimed he didn’t benefit from the fund.CTO Mira Murati expressed doubts about Altman’s leadership to other board members. Murati, Toner, and co-founder Ilya Sutskever began to document his actions.On November 16, the board voted to fire Altman and appoint Murati interim CEO. The board members were reluctant to reveal why they’d fired Altman. At one meeting, Murati and other executives gave them 30 minutes to either explain why they fired Altman, resign, or watch the executive team quit. Nearly all OpenAI employees (including Murati and Sutskever) signed a letter threatening to quit if Altman wasn't reinstated, and the board reversed its decision.Aftermath:Since Altman’s return, Murati and all but one director who voted to remove him have left OpenAI. The issues that precipitated his departure have given way to commercial concerns as the company considers a shift from its current hybrid nonprofit/for-profit structure to fully for-profit.GPT-5 will arrive “in the next few months,” according toAltman.Meanwhile, OpenAI launchedGPT-4.1(making full, mini, and nano versions available via API) and confirmed it soon would releaseo3, a new reasoning model.OpenAI said it will release its first open model, a newlanguage model with open weights, in coming months.The company recentlyraised$40 billion, the largest-ever funding round for an AI company, increasing its valuation to $300 billion.Why it matters:The AI frontier spawns not only technical innovations but also intense interpersonal relationships and corporate politics. Such dynamics have consequences for users and the world at large: Having survived serious challenges to his leadership, Altman has emerged in a strong position to build a path of faster growth as a for-profit company upon OpenAI’s philanthropic foundation.We’re thinking:Given OpenAI’s formidable achievements, Altman’s renewed leadership marks an inflection point in the AI landscape. Without Sam Altman at the helm, OpenAI would be a very different company, with different priorities and a different future.Toward LLMs That Understand MisspellingsResearchers built a model that’s more robust to noisy inputs like misspellings, smarter about character-level information like the number of R's in strawberry, and potentially better able to understand unfamiliar languages that might share groups of letters with familiar languages. Their approach: Eliminate the tokenizer and instead integrate a system that learns to group input characters.What’s new:Artidoro Pagnoni, Ram Pasunuru, and collaborators at Meta, University of Washington, and University of Chicago introducedByte Latent Transformer(BLT), a system of transformers that processes groups of text characters (in the form of bytes) directly.Key insight:A tokenizer turns bytes (characters) into tokens (a word or part of a word) based on learned rules: Specific sequences map to particular tokens. A large language model (LLM) would be more efficient if its tokenizer considered how easy or difficult it would be to predict the next token, because then it could group tokens that commonly occur together, thus saving memory and processing power. For instance, to complete the phrase, “The capital of the United States is,” a tokenizer may generate “Washington”, then “D”, then “.C”, and finally “.” — even though it’s easy to predict that “D.C.” will follow “Washington” (that is, the number of viable options is very small). Conversely, generating the token after “D.C.” is harder, since many viable options exist. Using a small LLM to estimate the difficulty of predicting the next token enables the model to split difficult-to-predict text into smaller groups while packing easier-to-predict text into larger groups.How it works:BLT comprises four transformers (8 billion parameters total): (i) a small byte-level transformer, (ii) an encoder transformer, (iii) a so-called latent transformer, and (iv) a decoder transformer. The authors trained the system to generate the next token in 1 trillion tokens of text, including tokens drawn from a filteredversionof Common Crawl.The authors trained the byte-level transformer to generate the next byte from an input sequence of bytes.For an input sequence, the byte-level transformer predicted the probabilities of the value of the next byte. The authors used entropy, a measure of uncertainty, to decide how bytes should be grouped. If the predicted probabilities were concentrated in a particular byte value (low entropy), meaning the next byte was highly predictable, the byte was added to the current group. If the probabilities were more spread out across multiple byte values (high entropy), meaning the model was less certain, it was part of a new group.The encoder transformer learned to represent each group as a vector, while attending to preceding bytes for context.The latent transformer learned to generate the next group vector from all previous group vectors.Finally, the decoder transformer learned to reconstruct a byte sequence from a sequence of vectors.Results:On seven benchmarks that test general language and coding abilities, BLT achieved an average accuracy of 61.1 percent, outperformingLlama 3(8 billion parameters and a similar number of floating point operations to BLT) at 60.0 percent.BLT achieved 80.6 percent on the common-sense question and answer benchmarkHellaSwag, while Llama 3 (8 billion parameters and a similar number of floating point operations to BLT) achieved 79.1 percent.BLT demonstrated significantly higher resilience to noisy inputs compared to Llama 3, particularly in tasks involving character manipulation, spelling variations, and languages for which relatively little data is available. For example, in the CUTE spelling benchmark, which tests a model’s ability to recognize correctly spelled words, BLT achieved 99.9 percent accuracy while Llama 3 achieved 1.1 percent accuracy.BLT outperformed Llama 3 intranslating to English across 26 languages(including 20 with little data). It achieved 14.0 average SentencePiece BLEU score (which measures how good a machine translation is compared to a human translation over text tokenized with theSentencePiecetokenizer), while LLaMA 3 achieved 12.1 average SentencePiece BLEU.Why it matters:By working directly on bytes, BLT is inherently more robust to variations in language, which improves its performance. For instance, when prompted to insert a \"z\" after every \"n\" in \"not\", Llama 3 incorrectly completed it as \"znotz\". This happened because its tokenizer treats \"not\" as a single, indivisible token. In contrast, BLT correctly generated \"nzot,\" because it can dynamically regroup bytes and draw new boundaries. In a more practical case, instead of treating \"pizya\" and \"pizza\" as different tokens, BLT recognizes that they share nearly identical byte sequences, differing only in the bytes for \"y\" and \"z\", and therefore likely mean the same thing.We’re thinking:In some alternatives to traditional tokenization, an LLM might process much longer sequences because the number of bytes in a sentence is much larger than the number of words. This work addresses that issue by grouping bytes dynamically. The tradeoff is complexity: Instead of one transformer, we have four.\nLoading theElevenlabs Text to SpeechAudioNative Player...\n\n\nShare\nShare\n\nShare\n",
    "image_urls": [
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--58-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/04/The-Batch-ads-and-exclusive-banners---2025-04-11T162548.680.png",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--76-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/04/ModelContextProtocol-diagram-5_1200px--1--1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--60-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--77-.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2023/03/2.png"
    ]
  },
  {
    "title": "issue 296",
    "date": null,
    "url": "https://www.deeplearning.ai/the-batch/issue-296/",
    "content": "The Batch\nWeekly Issues\nissue 296\nPublishedApr 9, 2025\nPublished\n\nPublished\nApr 9, 2025\nReading time16min read\nReading time\n\nReading time\n16min read\nPublishedApr 09, 2025Reading time16min readShareLoading theElevenlabs Text to SpeechAudioNative Player...Dear friends,I am so sorry that the U.S. is letting down our friends and allies. Broad tariffs, implemented not just against adversaries but also steadfast allies, will damage the livelihoods of billions of people, create inflation, make the world more fragmented, and leave the U.S. and the world poorer. AI isn’t the solution to everything, but even amidst this challenging environment, I hope our community can hold together, keep building friendships across borders, keep sharing ideas, and keep supporting each other.Much has been written about why high, widespread taxes on imports are harmful. In this letter, I’d like to focus on its possible effects on AI. One silver lining of the new tariffs is that they focus on physical imports, rather than digital goods and services, including intellectual property (IP) such as AI research inventions and software. IP is difficult to tax, because each piece of IP is unique and thus hard to value, and it moves across borders with little friction via the internet. Many international AI teams collaborate across borders and timezones, and software, including specifically open source software, is an important mechanism for sharing ideas. I hope that this free flow of ideas remains unhampered, even if the flow of physical goods is.However, AI relies on hardware, and tariffs will slow down AI progress by restricting access to it. Even though a last-minute exception was made for semiconductors, taxing imports of solar panels, wind turbines, and other power-generation and -distribution equipment will diminish the ability to provide power to U.S. data centers. Taxing imports of servers, cooling hardware, networking hardware, and the like will also make it more expensive to build data centers. And taxing consumer electronics, like laptops and phones, will make it harder for citizens to learn and use AI.With regard to data-center buildouts, another silver lining is that, with the rise of generative AI,data gravity has decreasedbecause compute processing costs are much greater than transmission costs, meaning it’s more feasible to place data centers anywhere in the world rather than only in close proximity to end-users. Even though many places do not have enough trained technicians to build and operate data centers, I expect tariffs will encourage data centers to be built around the world, creating more job opportunities globally.Finally, tariffs will create increased pressure for domestic manufacturing, which might create very mild tailwinds for robotics and industrial automation. As U.S. Vice President J.D. Vancepointed outin 2017, the U.S. should focus on automation (and education) rather than on tariffs. But the U.S. does not have the personnel — or know-how, or supply chain — to manufacture many of the goods that it currently counts on allies to make. Robotics can be helpful for addressing a small part of this large set of challenges. Generative AI’s rate of progress in robotics is also significantly slower than in processing text, visual data, audio, and reasoning. So while the tariffs could create tailwinds for AI-enabled robotics, I expect this effect to be small.My 4-year-old son had been complaining for a couple of weeks that his shoes were a tight fit — he was proud that he’s growing! So last Sunday, we went shoe shopping. His new shoes cost $25, and while checking out, I paused and reflected on how lucky I am to be able to afford them. But I also thought about the many families living paycheck-to-paycheck, and for whom tariffs leading to shoes at $40 a pair would mean they let their kids wear ill-fitting shoes longer. I also thought about people I’ve met in clothing manufacturing plants in Asia and Latin America, for whom reduced demand would mean less work and less money to take home to their own kids.I don’t know what will happen next with the U.S. tariffs, and plenty of international trade will happen with or without U.S. involvement. I hope we can return to a world of vibrant global trade with strong, rules-based, U.S. participation. Until then, let’s all of us in AI keep nurturing our international friendships, keep up the digital flow of ideas — including specifically open source software — and keep supporting each other. Let’s all do what we can to keep the world as connected as we are able.Love,AndrewA MESSAGE FROM DEEPLEARNING.AICourse 3 of theData Analytics Professional Certificateis live! Learn to use Python, the most important coding language in data analytics, to analyze real-world datasets, create visualizations, run tests, and apply AI tools to debug and accelerate your code.Enroll nowNewsOrdinary LLMs Implicitly Take Reasoning StepsEven without explicit training in reasoning, large language models “think” in ways that may be more deliberate than previously understood.What’s new:Emmanuel Ameisen and colleagues at Anthropic devised amethodto study how transformers generate responses to specific prompts. They alsostudiedClaude 3.5 Haiku’s responses to specific prompts and found that the model, which is not trained to generate chains of thought, nonetheless appeared to take reasoning steps via its neuron activations.Key insight:A viable alternative to a fully connected layer is a cross-layer transcoder, which has two layers. The outputs of the larger first layer are sparse, which makes them interpretable “features,” or individual values that correspond to concepts. By mapping an input to highly activated features, we can identify the concepts that determine the model’s output.How it works:The team replaced fully connected layers in Claude 3.5 Haiku with cross-layer transcoders and interpreted their features.The authors trained one cross-layer transcoder for each fully connected layer. Given the fully connected layer’s input, the cross-layer transcoder learned to minimize the difference between its output and the fully connected layer’s output. It also learned to minimize the number of non-zero weights.To interpret a transcoder’s features, they substituted it for the corresponding fully connected layer and ran selected inputs through the model. They produced visualizations of inputs that caused a feature to have a high value and looked for commonalities among those inputs. In this way, they found that certain features were associated with specific words (like “rabbit”), concepts (likelargeorcapital city), and next-word predictions (like “say D_”, indicating that the predicted token should start with the letter D), or “say capital,” (indicating that the predicted token should be a capital city).For each of several prompts, such as, “The opposite of small is,” they simplified a Claude 3.5 Haiku model to examine its response. They replaced the fully connected layers with cross-layer transcoders and reduced the attention computation (based on how it activated for the prompt). The simplified model was essentially a fully connected neural network.They built a graph that interpreted how the replacement model produced outputs. The nodes were features, and the edges represented a high contribution of one feature to another feature in a later intermediate layer. Then they replaced the features with their corresponding interpretations. For instance, if the input prompt was, “The opposite of small is,” the graph connected the featureoppositeto the featureantonym, and it connected the featuresantonymandsmallto the output feature “say large.”They verified causal relationships between inputs, interpretations, and outputs by replacing specific layer outputs with outputs corresponding to a different interpretation. For instance, they replaced the values that representedantonymwith values that representedsynonym. After this intervention, prompted with “the opposite of small is,” the model generated the synonym “little” (instead of the antonym “large”).Results:The authors built graphs that show how Claude 3.5 Haiku computes its output over a number of selected prompts.A graph for the prompt, “Fact: the capital of the state containing Dallas is” showed that the model determined internally that Dallas is in Texas, and then predicted Austin from the ideas “say a capital” and “Texas.” In other words, the model took steps rather than predicting “Austin” directly. To verify this conclusion, the authors replaced the features for “Texas” with the features for “California.” The model generated “Sacramento.”Given a prompt that mentioned several symptoms of an illness and asked which one best clarified a potential diagnosis, the model took into account the various symptoms, produced potential diagnosis internally, considered various diagnostic criteria, and decided which one to output.The authors’ graphs revealed how the model, prompted to describe its chain of thought, sometimes produced misleading output. Given a simple math problem and asked for the solution and the steps taken to find it, the model computed the answer correctly, and the graph and chain of thought matched. But given a more complex problem along with the expected solution and a request to double check it, the model’s chain of thought rationalized an incorrect solution, while the graph showed that the model had backtracked from the solution rather than trying to solve the problem. Given the same problem without the expected solution, the chain of thought described using a calculator, while the graph showed that the model had simply guessed an incorrect solution.Behind the news:Last year, Google trained models toexamine individual featuresin Gemma 2. Before that, Anthropic used similar methods tointerpret Claude 3 Sonnet’s middle layer.Why it matters:Apparently Claude 3.5 Haiku — and presumably other large language models — spontaneously perform implicit reasoning steps without being prompted to do so. Anthropic’s method reveals not only whether a model reasons or takes a shortcut, but also what it truly does well and what it only professes to do well.We’re thinking:The authors’ approach to examining how large language models generate output is interesting. We wonder whether even pre-transformer vanilla neural networks would appear to perform some sort of “reasoning” if we were to interpret them in a similar way.Llama’s Mixture of Vision-Language ExpertsMeta updated its popular open-weights models, claiming performance superior to closed competitors in three size classes.What’s new:Meta released two vision-language models in theLlama 4family (Llama 4 Scout and Llama 4 Maverick) and teased a third (Llama 4 Behemoth). All three models are based on the increasingly popular mixture-of-experts (MoE) architecture, which activates only a portion of parameters during inference for more efficient processing. Llama 4 Scout boasts the industry's biggest input context window so far — 10 million tokens! — but Metasaysprocessing 1.4 million tokens of context requires eight Nvidia H100 GPUs, and early users on Redditreportedthat its effective context began to degrade at 32,000 tokens.Input/output:Text, image, and video in (Llama 4 Scout up to 10 million tokens, Llama 4 Maverick up to 1 million tokens). Text out (Llama 4 Scout 120.5 tokens per second, 0.39 seconds to first token; Llama 4 Maverick 124.2 tokens per second, 0.34 seconds to first token).Architecture:Llama 4 Scout 109 billion parameters, 17 billion parameters activated. Llama 4 Maverick 400 billion parameters, 17 billion activated. Llama 4 Behemoth nearly 2 trillion parameters, 288 billion parameters activated.Features:12 officially supported languagesUndisclosed:Distillation details, Llama 4 Behemoth details including release dateAvailability:Weights free todownloadunder alicensethat allows noncommercial uses and limits commercial uses to businesses with fewer than 700 million monthly users under Meta’sterms of useAPI price:Llama 4 Scout $0.15/$0.50 per 1 million tokens input/output. Llama 4 Maverick $0.22/$0.85 per 1 million tokens input/output.How it works: The team pretrained Llama 4 models on images and text in over 200 languages from publicly available and licensed data, including data from publicly shared posts on Facebook and Instagram. They trained Llama 4 Scout on 40 trillion tokens and Llama 4 Maverick on 22 trillion tokens.The team removed the 50 percent of training examples that are easiest to predict (as judged by unnamed Llama models). For Llama 4 Behemoth, they removed 95 percent of an unspecified data set.They fine-tuned the models using supervised learning, then reinforcement learning, thendirect preference optimization.Llama 4 Maverick was “co-distilled” on outputs from Llama 4 Behemoth. The other teachers undisclosed.Results:In tests performed by Meta, Llama 4 models showed strong performance relative to competing models — mostly not mixtures of experts, but some that are known to have higher parameter counts relative to Llama 4 models’ active parameters.Llama 4 Scout outperformed Google Gemma 3 27B, Mistral 3.1 24B, and Gemini 2.0 Flash-Lite on most of seven benchmarks that test vision (MMMU, Chart QA), coding (LiveCodeBench), and knowledge and reasoning tasks (MMLU Pro, GPQA Diamond).Llama 4 Maverick outperformed OpenAI GPT-4o and Google Gemini 2.0 Flash across the same benchmarks.On multiple benchmarks including tests of mathematics, coding, domain knowledge, and multimedia reasoning, an early version of Llama 4 Behemoth outperformed OpenAI GPT-4.5, Anthropic Claude 3.7 Sonnet, and Google Gemini 2.0 Pro but fell behind OpenAI o1, DeepSeek-R1, and Google Gemini 2.5 Pro. (The parameter counts of these models are undisclosed except DeepSeek-R1, a MoE model with 671 billion parameters, 37 billion of which are active at any given time.)Yes, but:An experimental version of Llama 4 Maverick reached second place inChatbot Arenabehind Gemini 2.5 Pro. However, it was a variation optimized for conversation, not the currently available version. AI researchersaccusedMeta of attempting to manipulate the leaderboard.Why it matters:Although the version of Llama 4 Maverick that nearly topped the Chatbot Arena is not the released version, its accomplishment says a lot about the growing power of open weights. Open models are quickly reaching parity with closed competitors — a boon to developers, businesses, and society at large.We’re thinking:According to Meta, Behemoth beats GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro, topping all but the best reasoning models — but it isn’t available yet. Something to look forward to!Better Multimodal Performance With Open WeightsAlibaba’s latest open-weights system raises the bar for multimodal tasks in a relatively small model.What’s new:Alibaba releasedQwen2.5-Omni 7B.Input/output:Input: text, images (up to 10 MB per file), audio (up to 10 MB and 3 minutes per file), video (up to 150 MB and 40 seconds per file) for a total of up to 32,768 tokens. Output: text, speechPerformance:State of the art in some audio- and image-to-text benchmarksTraining data:18 trillion tokens of text (identical to Qwen2.5), 800 billion tokens of images and videos, 300 billion tokens of audio, 100 billion tokens of video with audioUndisclosed:Knowledge cutoff, output size, adapter architectureAvailability:Weights free todownloadunder theApache 2.0license.API price:Input: 0.4 Yuan per million tokens of text, 25 Yuan per million tokens of audio, 1.5 Yuan per million tokens of images/video. Output: 1.6 Yuan per million tokens of text with text-only input; 4.5 Yuan per million tokens of text with audio, video, or image input; 50 Yuan per million tokens of audio with any input.How it works:Qwen2.5-Omni 7B comprises a pretrained text transformer (Qwen 2.5 7B), pretrained vision encoder (Qwen2.5-VL), pretrained audio encoder (Whisper-large-v3), speech transformer, and audio decoder (a transformer plusBigVGAN), along with corresponding adapters of undisclosed architecture.The team pretrained the system in three stages. First, they pretrained the vision and audio encoders and their adapters with the frozen text transformer to generate the next text token in audio-text and image-text data. In the second stage, they pretrained the entire system to generate the next text or audio token in 1.2 trillion tokens of multimodal data. In the last stage, they pretrained the system on longer multimodal inputs.They fine-tuned the text transformer to generate the next token in a dataset of multimodal instruction-following tasks.They fine-tuned the speech transformer in three stages. First they fine-tuned the model to generate the next speech token in multimodal dialogues. Then they fine-tuned it to prefer generating speech with fewer erroneous words or unnecessary pauses viaDirect Preference Optimization. Finally, they fine-tuned it to reproduce the sounds of a few particular human voices.At inference, given images, audio, video, and/or a text input, the vision encoder embeds video frames/images and the audio encoder embeds audio (including video soundtracks). The adapters transform the embedded frames/images and audio for further processing. From the text and embedded frames and audio, the text transformer generates the next text token plus high-level embeddings of input text, images, video, and audio. From the generated text and high-level embeddings, the speech transformer generates the next speech tokens. Finally, the audio decoder turns speech tokens into audio.Results:The authors compared Qwen2.5-Omni 7B to similarly sized models. It performed especially well on audio-to-text, image-to-text, and video-to-text tasks. However, it performed less well on text-to-text and text-to-speech tasks.Qwen2.5-Omni 7B achieved state-of-the-art measures on most of the audio-to-text benchmarks tested. For example, when transcribing recorded English speech inCommon Voice 15, Qwen2.5-Omni 7B (7.6 percent word error rate) beat the next-best modelMinMo(7.9 percent word error rate).Qwen2.5-Omni 7B achieved state-of-the-art performance on some image-to-text tasks including MMstar, where it tied withMiniCPM-V(64 percent accuracy) and beat GPT-4o-mini (54.8 percent accuracy).In 10 text-to-text benchmarks, Qwen2.5-Omni 7B underperformed Qwen 2.5-7B but  generally was comparable with Qwen2-7B, Llama 3.1-8B, and Gemma2-9B.On the English subset ofSeed, in which the system renders text in a particular speaker’s voice based on a snippet of reference audio, Qwen2.5-Omni 7B (2.33 percent word error rate) underperformed F5-TTS (1.83 percent word error rate).Behind the news:Multimodal systems with open weights are multiplying. For instance,AnyGPT(open weights, training, and inference code) accepts and generates speech, text, images, and music. Similarly,Mini-Omni2(open weights and inference code) accepts and generates text, speech, and images.Why it matters:Multimodal models typically show steep degradation on measurements of instruction-following when shifting from voice to text, but Qwen2.5-Omni does not. As the world moves toward voice-to-voice interactions, open systems that deliver performance comparable to that of closed competitors accelerate progress towards better conversations.We’re thinking:The Qwen team is on fire! Alibaba’s steady stream of highly capable open-weights models is a gift to AI developers.Better Than Trees for Tabular DataIf you have a collection of variables that represent, say, a cancer patient and you want to classify the patient’s illness as likely cancer or not, algorithms based on decision trees, such as gradient-boosted trees, typically perform better than neural networks. A transformer tailored to tabular data could change this situation.What’s new: Noah Hollmann, Samuel Müller, and colleagues at University of Freiburg, Berlin Institute of Health, Prior Labs, and ELLIS Institute introducedTabular Prior-data Fitted Network(TabPFN), a transformer that, given a tabular dataset, beats established decision-tree methods on classification and regression tasks. You can download thecodeandweightsunder alicensebased on Apache 2.0 that allows noncommercial and commercial uses.Key insight:In a typical supervised learning process, a model given one example at a time learns to recognize patterns in a dataset. If each example is an entire dataset, it learns to recognize patterns across all those datasets. Trained in this way on enough datasets, it can generalize to new ones. Applying this idea to tabular data, a transformer — unlike a decision tree — can learn to perform classification and regression on any dataset without further training; that is, without further updating the model weights.How it works:The authors generated 100 million datasets and used them to pretrain two small transformers (around 7 million and 11 million parameters respectively) to perform classification or regression. Given a dataset of rows (say, patient data labeled diagnoses or real-estate data labeled with prices) and one final row that’s unlabeled, the models learned to generate the missing label or value. Each dataset consisted of up to 2,048 rows (examples) and up to 160 columns (features).To generate a dataset, the authors sampled hyperparameters, such as the number of rows and columns, and produced a graph in which each node is a potential column, and each edge describes how one column is related to another mathematically. They sampled the mathematical relationships randomly; for example, one column might be the sum of a second column with the sine of a third. They selected a subset of nodes at random, creating columns, and propagated random noise through them to fill the columns with values. To simulate real-world imperfections, they removed some values and added noise at random.The authors modified the transformer’s attention mechanism. Where a typical transformer block contains an attention layer and a fully connected layer, the authors included a feature attention layer (in which each cell attended to other cells in its column), an example attention layer (in which each cell attended to other cells in its row), and a fully connected layer.The authors trained the model to estimate the missing label in each synthetic dataset. At inference, given a dataset (with labels) and an unlabeled example, the model predicted the label.Results:The authors tested the system on 29 classification datasets and 28 regression datasets from theAutoMLbenchmark andOpenML-CTR23. Each dataset contained up to 10,000 rows, 500 columns, and 10 classes. They compared TabPFN to the popular gradient-boosted tree approaches CatBoost, LightGBM, and XGBoost.To evaluate classification, the authors measured area under the curve (AUC, higher is better) and normalized the resulting scores across the datasets to range from 0 (worst) to 1 (best). TabPFN performed best across the datasets tested, achieving an average 0.939 normalized AUC, while the best contender, CatBoost, achieved an average 0.752 normalized AUC.To evaluate regression, the authors measured root mean squared error (RMSE). They normalized the resulting scores to range from 0 (worst) to 1 (best). TabPFN achieved 0.923 normalized RMSE, while the next-best method, Catboost, achieved 0.872 normalized RMSE.Yes, but:The authors’ method is slower than decision tree methods with respect to inference. To process a 10,000-row dataset, TabPFN required 0.2 seconds while CatBoost took 0.0002 seconds.Why it matters:Transformers trained on large datasets of text or images can perform tasks they weren’t specifically trained for and generalize to novel datasets when performing tasks they were trained for. But when it comes to tabular data, they haven’t been competitive with decision trees. This work bridges the gap, unlocking a wide variety of new use cases for transformers. Not only does it process tabular data as well as popular tree-based methods, it doesn’t require additional training to process novel datasets.We’re thinking:Decision treesdate back to Aristotleand remain extremely useful. But a transformer-based approach could open the processing of tabular data to benefit from the ongoing innovation in transformers.Share\nPublishedApr 09, 2025\nPublished\n\nPublished\nApr 09, 2025\nApr 09, 2025\nReading time16min read\nReading time\n\nReading time\n16min read\nShare\nShare\n\nShare\n\nLoading theElevenlabs Text to SpeechAudioNative Player...Dear friends,I am so sorry that the U.S. is letting down our friends and allies. Broad tariffs, implemented not just against adversaries but also steadfast allies, will damage the livelihoods of billions of people, create inflation, make the world more fragmented, and leave the U.S. and the world poorer. AI isn’t the solution to everything, but even amidst this challenging environment, I hope our community can hold together, keep building friendships across borders, keep sharing ideas, and keep supporting each other.Much has been written about why high, widespread taxes on imports are harmful. In this letter, I’d like to focus on its possible effects on AI. One silver lining of the new tariffs is that they focus on physical imports, rather than digital goods and services, including intellectual property (IP) such as AI research inventions and software. IP is difficult to tax, because each piece of IP is unique and thus hard to value, and it moves across borders with little friction via the internet. Many international AI teams collaborate across borders and timezones, and software, including specifically open source software, is an important mechanism for sharing ideas. I hope that this free flow of ideas remains unhampered, even if the flow of physical goods is.However, AI relies on hardware, and tariffs will slow down AI progress by restricting access to it. Even though a last-minute exception was made for semiconductors, taxing imports of solar panels, wind turbines, and other power-generation and -distribution equipment will diminish the ability to provide power to U.S. data centers. Taxing imports of servers, cooling hardware, networking hardware, and the like will also make it more expensive to build data centers. And taxing consumer electronics, like laptops and phones, will make it harder for citizens to learn and use AI.With regard to data-center buildouts, another silver lining is that, with the rise of generative AI,data gravity has decreasedbecause compute processing costs are much greater than transmission costs, meaning it’s more feasible to place data centers anywhere in the world rather than only in close proximity to end-users. Even though many places do not have enough trained technicians to build and operate data centers, I expect tariffs will encourage data centers to be built around the world, creating more job opportunities globally.Finally, tariffs will create increased pressure for domestic manufacturing, which might create very mild tailwinds for robotics and industrial automation. As U.S. Vice President J.D. Vancepointed outin 2017, the U.S. should focus on automation (and education) rather than on tariffs. But the U.S. does not have the personnel — or know-how, or supply chain — to manufacture many of the goods that it currently counts on allies to make. Robotics can be helpful for addressing a small part of this large set of challenges. Generative AI’s rate of progress in robotics is also significantly slower than in processing text, visual data, audio, and reasoning. So while the tariffs could create tailwinds for AI-enabled robotics, I expect this effect to be small.My 4-year-old son had been complaining for a couple of weeks that his shoes were a tight fit — he was proud that he’s growing! So last Sunday, we went shoe shopping. His new shoes cost $25, and while checking out, I paused and reflected on how lucky I am to be able to afford them. But I also thought about the many families living paycheck-to-paycheck, and for whom tariffs leading to shoes at $40 a pair would mean they let their kids wear ill-fitting shoes longer. I also thought about people I’ve met in clothing manufacturing plants in Asia and Latin America, for whom reduced demand would mean less work and less money to take home to their own kids.I don’t know what will happen next with the U.S. tariffs, and plenty of international trade will happen with or without U.S. involvement. I hope we can return to a world of vibrant global trade with strong, rules-based, U.S. participation. Until then, let’s all of us in AI keep nurturing our international friendships, keep up the digital flow of ideas — including specifically open source software — and keep supporting each other. Let’s all do what we can to keep the world as connected as we are able.Love,AndrewA MESSAGE FROM DEEPLEARNING.AICourse 3 of theData Analytics Professional Certificateis live! Learn to use Python, the most important coding language in data analytics, to analyze real-world datasets, create visualizations, run tests, and apply AI tools to debug and accelerate your code.Enroll nowNewsOrdinary LLMs Implicitly Take Reasoning StepsEven without explicit training in reasoning, large language models “think” in ways that may be more deliberate than previously understood.What’s new:Emmanuel Ameisen and colleagues at Anthropic devised amethodto study how transformers generate responses to specific prompts. They alsostudiedClaude 3.5 Haiku’s responses to specific prompts and found that the model, which is not trained to generate chains of thought, nonetheless appeared to take reasoning steps via its neuron activations.Key insight:A viable alternative to a fully connected layer is a cross-layer transcoder, which has two layers. The outputs of the larger first layer are sparse, which makes them interpretable “features,” or individual values that correspond to concepts. By mapping an input to highly activated features, we can identify the concepts that determine the model’s output.How it works:The team replaced fully connected layers in Claude 3.5 Haiku with cross-layer transcoders and interpreted their features.The authors trained one cross-layer transcoder for each fully connected layer. Given the fully connected layer’s input, the cross-layer transcoder learned to minimize the difference between its output and the fully connected layer’s output. It also learned to minimize the number of non-zero weights.To interpret a transcoder’s features, they substituted it for the corresponding fully connected layer and ran selected inputs through the model. They produced visualizations of inputs that caused a feature to have a high value and looked for commonalities among those inputs. In this way, they found that certain features were associated with specific words (like “rabbit”), concepts (likelargeorcapital city), and next-word predictions (like “say D_”, indicating that the predicted token should start with the letter D), or “say capital,” (indicating that the predicted token should be a capital city).For each of several prompts, such as, “The opposite of small is,” they simplified a Claude 3.5 Haiku model to examine its response. They replaced the fully connected layers with cross-layer transcoders and reduced the attention computation (based on how it activated for the prompt). The simplified model was essentially a fully connected neural network.They built a graph that interpreted how the replacement model produced outputs. The nodes were features, and the edges represented a high contribution of one feature to another feature in a later intermediate layer. Then they replaced the features with their corresponding interpretations. For instance, if the input prompt was, “The opposite of small is,” the graph connected the featureoppositeto the featureantonym, and it connected the featuresantonymandsmallto the output feature “say large.”They verified causal relationships between inputs, interpretations, and outputs by replacing specific layer outputs with outputs corresponding to a different interpretation. For instance, they replaced the values that representedantonymwith values that representedsynonym. After this intervention, prompted with “the opposite of small is,” the model generated the synonym “little” (instead of the antonym “large”).Results:The authors built graphs that show how Claude 3.5 Haiku computes its output over a number of selected prompts.A graph for the prompt, “Fact: the capital of the state containing Dallas is” showed that the model determined internally that Dallas is in Texas, and then predicted Austin from the ideas “say a capital” and “Texas.” In other words, the model took steps rather than predicting “Austin” directly. To verify this conclusion, the authors replaced the features for “Texas” with the features for “California.” The model generated “Sacramento.”Given a prompt that mentioned several symptoms of an illness and asked which one best clarified a potential diagnosis, the model took into account the various symptoms, produced potential diagnosis internally, considered various diagnostic criteria, and decided which one to output.The authors’ graphs revealed how the model, prompted to describe its chain of thought, sometimes produced misleading output. Given a simple math problem and asked for the solution and the steps taken to find it, the model computed the answer correctly, and the graph and chain of thought matched. But given a more complex problem along with the expected solution and a request to double check it, the model’s chain of thought rationalized an incorrect solution, while the graph showed that the model had backtracked from the solution rather than trying to solve the problem. Given the same problem without the expected solution, the chain of thought described using a calculator, while the graph showed that the model had simply guessed an incorrect solution.Behind the news:Last year, Google trained models toexamine individual featuresin Gemma 2. Before that, Anthropic used similar methods tointerpret Claude 3 Sonnet’s middle layer.Why it matters:Apparently Claude 3.5 Haiku — and presumably other large language models — spontaneously perform implicit reasoning steps without being prompted to do so. Anthropic’s method reveals not only whether a model reasons or takes a shortcut, but also what it truly does well and what it only professes to do well.We’re thinking:The authors’ approach to examining how large language models generate output is interesting. We wonder whether even pre-transformer vanilla neural networks would appear to perform some sort of “reasoning” if we were to interpret them in a similar way.Llama’s Mixture of Vision-Language ExpertsMeta updated its popular open-weights models, claiming performance superior to closed competitors in three size classes.What’s new:Meta released two vision-language models in theLlama 4family (Llama 4 Scout and Llama 4 Maverick) and teased a third (Llama 4 Behemoth). All three models are based on the increasingly popular mixture-of-experts (MoE) architecture, which activates only a portion of parameters during inference for more efficient processing. Llama 4 Scout boasts the industry's biggest input context window so far — 10 million tokens! — but Metasaysprocessing 1.4 million tokens of context requires eight Nvidia H100 GPUs, and early users on Redditreportedthat its effective context began to degrade at 32,000 tokens.Input/output:Text, image, and video in (Llama 4 Scout up to 10 million tokens, Llama 4 Maverick up to 1 million tokens). Text out (Llama 4 Scout 120.5 tokens per second, 0.39 seconds to first token; Llama 4 Maverick 124.2 tokens per second, 0.34 seconds to first token).Architecture:Llama 4 Scout 109 billion parameters, 17 billion parameters activated. Llama 4 Maverick 400 billion parameters, 17 billion activated. Llama 4 Behemoth nearly 2 trillion parameters, 288 billion parameters activated.Features:12 officially supported languagesUndisclosed:Distillation details, Llama 4 Behemoth details including release dateAvailability:Weights free todownloadunder alicensethat allows noncommercial uses and limits commercial uses to businesses with fewer than 700 million monthly users under Meta’sterms of useAPI price:Llama 4 Scout $0.15/$0.50 per 1 million tokens input/output. Llama 4 Maverick $0.22/$0.85 per 1 million tokens input/output.How it works: The team pretrained Llama 4 models on images and text in over 200 languages from publicly available and licensed data, including data from publicly shared posts on Facebook and Instagram. They trained Llama 4 Scout on 40 trillion tokens and Llama 4 Maverick on 22 trillion tokens.The team removed the 50 percent of training examples that are easiest to predict (as judged by unnamed Llama models). For Llama 4 Behemoth, they removed 95 percent of an unspecified data set.They fine-tuned the models using supervised learning, then reinforcement learning, thendirect preference optimization.Llama 4 Maverick was “co-distilled” on outputs from Llama 4 Behemoth. The other teachers undisclosed.Results:In tests performed by Meta, Llama 4 models showed strong performance relative to competing models — mostly not mixtures of experts, but some that are known to have higher parameter counts relative to Llama 4 models’ active parameters.Llama 4 Scout outperformed Google Gemma 3 27B, Mistral 3.1 24B, and Gemini 2.0 Flash-Lite on most of seven benchmarks that test vision (MMMU, Chart QA), coding (LiveCodeBench), and knowledge and reasoning tasks (MMLU Pro, GPQA Diamond).Llama 4 Maverick outperformed OpenAI GPT-4o and Google Gemini 2.0 Flash across the same benchmarks.On multiple benchmarks including tests of mathematics, coding, domain knowledge, and multimedia reasoning, an early version of Llama 4 Behemoth outperformed OpenAI GPT-4.5, Anthropic Claude 3.7 Sonnet, and Google Gemini 2.0 Pro but fell behind OpenAI o1, DeepSeek-R1, and Google Gemini 2.5 Pro. (The parameter counts of these models are undisclosed except DeepSeek-R1, a MoE model with 671 billion parameters, 37 billion of which are active at any given time.)Yes, but:An experimental version of Llama 4 Maverick reached second place inChatbot Arenabehind Gemini 2.5 Pro. However, it was a variation optimized for conversation, not the currently available version. AI researchersaccusedMeta of attempting to manipulate the leaderboard.Why it matters:Although the version of Llama 4 Maverick that nearly topped the Chatbot Arena is not the released version, its accomplishment says a lot about the growing power of open weights. Open models are quickly reaching parity with closed competitors — a boon to developers, businesses, and society at large.We’re thinking:According to Meta, Behemoth beats GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro, topping all but the best reasoning models — but it isn’t available yet. Something to look forward to!Better Multimodal Performance With Open WeightsAlibaba’s latest open-weights system raises the bar for multimodal tasks in a relatively small model.What’s new:Alibaba releasedQwen2.5-Omni 7B.Input/output:Input: text, images (up to 10 MB per file), audio (up to 10 MB and 3 minutes per file), video (up to 150 MB and 40 seconds per file) for a total of up to 32,768 tokens. Output: text, speechPerformance:State of the art in some audio- and image-to-text benchmarksTraining data:18 trillion tokens of text (identical to Qwen2.5), 800 billion tokens of images and videos, 300 billion tokens of audio, 100 billion tokens of video with audioUndisclosed:Knowledge cutoff, output size, adapter architectureAvailability:Weights free todownloadunder theApache 2.0license.API price:Input: 0.4 Yuan per million tokens of text, 25 Yuan per million tokens of audio, 1.5 Yuan per million tokens of images/video. Output: 1.6 Yuan per million tokens of text with text-only input; 4.5 Yuan per million tokens of text with audio, video, or image input; 50 Yuan per million tokens of audio with any input.How it works:Qwen2.5-Omni 7B comprises a pretrained text transformer (Qwen 2.5 7B), pretrained vision encoder (Qwen2.5-VL), pretrained audio encoder (Whisper-large-v3), speech transformer, and audio decoder (a transformer plusBigVGAN), along with corresponding adapters of undisclosed architecture.The team pretrained the system in three stages. First, they pretrained the vision and audio encoders and their adapters with the frozen text transformer to generate the next text token in audio-text and image-text data. In the second stage, they pretrained the entire system to generate the next text or audio token in 1.2 trillion tokens of multimodal data. In the last stage, they pretrained the system on longer multimodal inputs.They fine-tuned the text transformer to generate the next token in a dataset of multimodal instruction-following tasks.They fine-tuned the speech transformer in three stages. First they fine-tuned the model to generate the next speech token in multimodal dialogues. Then they fine-tuned it to prefer generating speech with fewer erroneous words or unnecessary pauses viaDirect Preference Optimization. Finally, they fine-tuned it to reproduce the sounds of a few particular human voices.At inference, given images, audio, video, and/or a text input, the vision encoder embeds video frames/images and the audio encoder embeds audio (including video soundtracks). The adapters transform the embedded frames/images and audio for further processing. From the text and embedded frames and audio, the text transformer generates the next text token plus high-level embeddings of input text, images, video, and audio. From the generated text and high-level embeddings, the speech transformer generates the next speech tokens. Finally, the audio decoder turns speech tokens into audio.Results:The authors compared Qwen2.5-Omni 7B to similarly sized models. It performed especially well on audio-to-text, image-to-text, and video-to-text tasks. However, it performed less well on text-to-text and text-to-speech tasks.Qwen2.5-Omni 7B achieved state-of-the-art measures on most of the audio-to-text benchmarks tested. For example, when transcribing recorded English speech inCommon Voice 15, Qwen2.5-Omni 7B (7.6 percent word error rate) beat the next-best modelMinMo(7.9 percent word error rate).Qwen2.5-Omni 7B achieved state-of-the-art performance on some image-to-text tasks including MMstar, where it tied withMiniCPM-V(64 percent accuracy) and beat GPT-4o-mini (54.8 percent accuracy).In 10 text-to-text benchmarks, Qwen2.5-Omni 7B underperformed Qwen 2.5-7B but  generally was comparable with Qwen2-7B, Llama 3.1-8B, and Gemma2-9B.On the English subset ofSeed, in which the system renders text in a particular speaker’s voice based on a snippet of reference audio, Qwen2.5-Omni 7B (2.33 percent word error rate) underperformed F5-TTS (1.83 percent word error rate).Behind the news:Multimodal systems with open weights are multiplying. For instance,AnyGPT(open weights, training, and inference code) accepts and generates speech, text, images, and music. Similarly,Mini-Omni2(open weights and inference code) accepts and generates text, speech, and images.Why it matters:Multimodal models typically show steep degradation on measurements of instruction-following when shifting from voice to text, but Qwen2.5-Omni does not. As the world moves toward voice-to-voice interactions, open systems that deliver performance comparable to that of closed competitors accelerate progress towards better conversations.We’re thinking:The Qwen team is on fire! Alibaba’s steady stream of highly capable open-weights models is a gift to AI developers.Better Than Trees for Tabular DataIf you have a collection of variables that represent, say, a cancer patient and you want to classify the patient’s illness as likely cancer or not, algorithms based on decision trees, such as gradient-boosted trees, typically perform better than neural networks. A transformer tailored to tabular data could change this situation.What’s new: Noah Hollmann, Samuel Müller, and colleagues at University of Freiburg, Berlin Institute of Health, Prior Labs, and ELLIS Institute introducedTabular Prior-data Fitted Network(TabPFN), a transformer that, given a tabular dataset, beats established decision-tree methods on classification and regression tasks. You can download thecodeandweightsunder alicensebased on Apache 2.0 that allows noncommercial and commercial uses.Key insight:In a typical supervised learning process, a model given one example at a time learns to recognize patterns in a dataset. If each example is an entire dataset, it learns to recognize patterns across all those datasets. Trained in this way on enough datasets, it can generalize to new ones. Applying this idea to tabular data, a transformer — unlike a decision tree — can learn to perform classification and regression on any dataset without further training; that is, without further updating the model weights.How it works:The authors generated 100 million datasets and used them to pretrain two small transformers (around 7 million and 11 million parameters respectively) to perform classification or regression. Given a dataset of rows (say, patient data labeled diagnoses or real-estate data labeled with prices) and one final row that’s unlabeled, the models learned to generate the missing label or value. Each dataset consisted of up to 2,048 rows (examples) and up to 160 columns (features).To generate a dataset, the authors sampled hyperparameters, such as the number of rows and columns, and produced a graph in which each node is a potential column, and each edge describes how one column is related to another mathematically. They sampled the mathematical relationships randomly; for example, one column might be the sum of a second column with the sine of a third. They selected a subset of nodes at random, creating columns, and propagated random noise through them to fill the columns with values. To simulate real-world imperfections, they removed some values and added noise at random.The authors modified the transformer’s attention mechanism. Where a typical transformer block contains an attention layer and a fully connected layer, the authors included a feature attention layer (in which each cell attended to other cells in its column), an example attention layer (in which each cell attended to other cells in its row), and a fully connected layer.The authors trained the model to estimate the missing label in each synthetic dataset. At inference, given a dataset (with labels) and an unlabeled example, the model predicted the label.Results:The authors tested the system on 29 classification datasets and 28 regression datasets from theAutoMLbenchmark andOpenML-CTR23. Each dataset contained up to 10,000 rows, 500 columns, and 10 classes. They compared TabPFN to the popular gradient-boosted tree approaches CatBoost, LightGBM, and XGBoost.To evaluate classification, the authors measured area under the curve (AUC, higher is better) and normalized the resulting scores across the datasets to range from 0 (worst) to 1 (best). TabPFN performed best across the datasets tested, achieving an average 0.939 normalized AUC, while the best contender, CatBoost, achieved an average 0.752 normalized AUC.To evaluate regression, the authors measured root mean squared error (RMSE). They normalized the resulting scores to range from 0 (worst) to 1 (best). TabPFN achieved 0.923 normalized RMSE, while the next-best method, Catboost, achieved 0.872 normalized RMSE.Yes, but:The authors’ method is slower than decision tree methods with respect to inference. To process a 10,000-row dataset, TabPFN required 0.2 seconds while CatBoost took 0.0002 seconds.Why it matters:Transformers trained on large datasets of text or images can perform tasks they weren’t specifically trained for and generalize to novel datasets when performing tasks they were trained for. But when it comes to tabular data, they haven’t been competitive with decision trees. This work bridges the gap, unlocking a wide variety of new use cases for transformers. Not only does it process tabular data as well as popular tree-based methods, it doesn’t require additional training to process novel datasets.We’re thinking:Decision treesdate back to Aristotleand remain extremely useful. But a transformer-based approach could open the processing of tabular data to benefit from the ongoing innovation in transformers.\nLoading theElevenlabs Text to SpeechAudioNative Player...\n\n\nShare\nShare\n\nShare\n",
    "image_urls": [
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--73-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/04/V3_Sign_Up_Button_DeepLearning_Data_Analytics_Banner_2070x1080-01.png",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--56-.gif",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--57-.gif",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--74-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--75-.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2023/03/4.png"
    ]
  },
  {
    "title": "issue 299",
    "date": null,
    "url": "https://www.deeplearning.ai/the-batch/issue-299/",
    "content": "The Batch\nWeekly Issues\nissue 299\nPublishedApr 30, 2025\nPublished\n\nPublished\nApr 30, 2025\nReading time11min read\nReading time\n\nReading time\n11min read\nPublishedApr 30, 2025Reading time11min readShareLoading theElevenlabs Text to SpeechAudioNative Player...Dear friends,I hope we can empower everyone to build with AI. Starting from K-12, we should teach every student AI enabled coding, since this will enable them to become more productive and more empowered adults. But there is a huge shortage of computer science (CS) teachers. I recently spoke with high school basketball coach Kyle Creasy, who graduated with a B.A. in Physical Education in 2023. Until two years ago, he had never written a line of Python. Now — with help from AI — he not only writes code, he also teaches CS. I found Kyle’s story inspiring as a model for scaling up CS education in the primary- and secondary-school levels.Kyle’s success has been with the support ofKira Learning(an AI Fund portfolio company), whose founders Andrea Pasinetti and Jagriti Agrawal have created a compelling vision for CS education. In K-12 classrooms, teachers play a huge social-emotional support role, for example, encouraging students and helping them when they stumble. In addition, they are expected to be subject-matter experts who can deliver the content needed for their subject. Kira Learning uses digital content delivery — educational videos, autograded quizzes, and AI-enabled chatbots to answer students' questions but without giving away homework answers — so the teacher can focus on social-emotional support. While these are still early days, it appears to be working!A key to making this possible is the hyperpersonalization that is now possible with AI (in contrast to the older idea of theflipped classroom, which had limited adoption). For example, when assigned a problem in an online coding environment, if a student writes this buggy line of Python codebest_$alty_snack = 'potato chips'Kira Learning’s AI system can spot the problem and directly tell the teacher that $ is an invalid character in a variable name. It can also suggest a specific question for the teacher to ask the student to help get them unstuck, like “Can you identify what characters are allowed in variable names?” Whereas AI can directly deliver personalized advice to students, the fact that it is now helping teachers also deliver personalized support will really help in K-12.Additionally, agentic workflows can automate a lot of teachers’ repetitive tasks. For example, when designing a curriculum, it’s time-consuming to align the content to educational standards (such as the Common Core in the United States, or the AP CS standard for many CS classes). Having an AI system carry out tasks like these is already proving helpful for teachers.Since learning to code, Kyle has built many pieces of software. He proudly showed me an analysis he generated in matplotlib of his basketball players’ attempts to shoot three-pointers (shown above), which in turn is affecting the team’s strategy on the court. One lesson is clear: When a basketball coach learns to code, they become a better basketball coach!I talked about Kyle (and other topics) at theASU+GSV Summiton education. You can see a videohere.In the future, people who know how to code and build with AI will be much more productive than people who don’t. I’m excited about how AI will lead to new models for K-12 education. By delivering CS education to everyone, I hope that in the future, everyone will be able to build with AI.Keep learning!AndrewA MESSAGE FROM DEEPLEARNING.AIIn “LLMs as Operating Systems: Agent Memory,” you’ll learn to build agents that manage their own memory using the MemGPT approach. This newly updated short course includes cloud-based deployment and real-time, step-by-step output, so you can see how your agents reason as they respond.Join in today!NewsNew Image Generator for OpenAI APIChatGPT’s image generator is available via API.What’s new:GPT Image 1, which produces images from text or other images, has proven enormously popular among ChatGPT users. TheOpenAI Images APIenables developers to incorporate OpenAI’s most sophisticated image generator into their own software tools and platforms.Input/output:Text and images in, images outArchitecture:Autoregressive (details undisclosed)Performance:Currently tops Artificial Analysis’Image Arena leaderboard.Price:$5 per 1 million tokens of text input, $10 per 1 million tokens of image input, $40 per 1 million tokens of image output (roughly $0.02, $0.07, and $0.19 per generated image for low, medium, and high-quality square images, respectively)Undisclosed:Architecture details, parameter count, training data, training methodsHow it works:GPT Image 1generates and modifies imagesin a wide range of styles, performs image editing and other alterations, renders text, and follows detailed instructions. Shortly after its debut, the version of GPT-4o equipped with GPT Image 1 quickly soared to the No. 1 spot on theArtificial Analysis Image Arena leaderboard.The model employs an autoregressive design rather than the more typical diffusion architecture (like Open AI’s DALL·E 3), using generated parts of an image to predict the next part.Its pricing structure differs from rivals, charging by input/output tokens rather than per image generated.The model’s output is watermarked unobtrusively withC2PAdata that identifies it as AI-generated.The model may struggle to process non-English text, small type, rotated type, varying colors and styles, counting, and localization in space such as positions of pieces on a game board.Behind the news:In March, OpenAI attracted huge public interest when it deployed the model, then unnamed, inChatGPT. Within the first week,130 millionusers used it to create more than 700 million images.Why it matters:Adding GPT Image 1 to the API enables developers to use OpenAI’s most sophisticated image generator in a wide variety of automated workflows. OpenAI’s initial API partners include design companies (Adobe and Canva), marketers (HubSpot), and web designers (GoDaddy), all of which are using GPT Image 1.We’re thinking:GPT Image 1 is part of an exciting trend toward unification of multimodal architectures. Researchers have progressed fromtext-in, text-outtotext/images-in, text-outand increasinglytext/images/audio-in, text/images/audio-out. This paints a beautiful picture of where multimodal models can go!Music Generation for ProsGoogle refreshed its experimental tools for composers and producers.What’s new:Google announced updates of two music-generation apps and the models they're based on.Music AI Sandbox, an app that generates and modifies music according to text prompts, now accepts lyrics to generate songs as well as instrumental music. You can join a waitlisthere.MusicFX DJgenerates a continuous stream of music that users can modify as it plays. Try it outhere.How it works:The apps generate 48kHz audio suitable for professional productions. Users can specify key, tempo in beats per minute, instrumentation, style, mood, and other details.Music AI Sandbox is based on the updatedLyria 2music generator. It lets users generate new clips, roughly 30 seconds long, according to prompts. Users can enter lyrics, extend existing clips, and rearrange segments with generated transitions, introductions, and endings.MusicFX DJ, which is based on a different model calledLyria RealTime, lets users control streaming music via prompts and other settings. Users can change or combine genres, add or subtract instruments, change key, and speed up or slow down without interrupting the stream.Behind the news:GooglelaunchedLyria 1 and Music AI Sandbox in 2023 as part of an experiment with YouTube, which made them available to composers, producers, and musicians. Since then, the company has developed them with help from music stars including Jacob Collier, Donald “Childish Gambino” Glover, and Wyclef Jean. Lyria 1 recently becameavailablevia the Vertex API to developers who are preapproved by Google.Why it matters:While music generators likeSuno and Udioappeal to casual musicians, Music AI Sandbox, with its digital audio workstation-style user interface, aims to address the needs of professionals. This approach puts AI directly into the hands of talented, experienced artists, similar to the way Adobe hasempoweredvideographers and Runway haspartneredwith movie producers.We’re thinking:API access to Lyria 2 would be music to our ears!Up-and-Coming StartupsAI agents and infrastructure made a strong showing on CB Insights’s latest list of the top 100 AI startups.What’s new:CB Insights, which tracks tech startups and venture capital, selected companies in theAI 100based on their market traction, talent, finances, and partnerships. The list purports to highlight the next wave of winners, shedding light on the key executives, investors, fundraising, and valuations behind up-and-coming AI ventures.How it works:The analysts evaluated 17,000 early-stage, private AI companies that had raised funds within the last year and continue to seek further investment.CB Insights evaluated the startups according to its ownMosaic Score, a proprietary system designed to assess the health and growth potential of private companies. The score takes into account a startup’s market momentum (traction and growth rate), market size, financial health, and management team.The analysts divided their choices into three broad categories: (i) horizontal (providing business products or services common to multiple industries), (ii) vertical (serving a single industry or business function), or (iii) providers of AI hardware or software infrastructure.They further divided the horizontal companies by business function (customer service, cybersecurity, software development, and so on), the vertical companies into industries (healthcare, automotive, aerospace, manufacturing, finance, energy, and the like), and the infrastructure providers into segments (hardware, monitoring, data, and development and training).Where the action is:This year’s AI 100 companies are based in 14 countries, around two-thirds of them in the United States. 10 are based in the United Kingdom, five in France, and four in Germany, with one each in Norway (Braintrust), Singapore (Bria), Spain (Cartwheel), Sweden (Chainguard), and Switzerland (Clarium).More than 20 percent of this year’s AI 100 build AI agents or support them, including Texas-based Apptronik (valued at $423 million) and Canada’s 1X ($134 million, the second-most highly valued agent specialist).The report also notes the rapid growth of companies that monitor AI performance and reliability, such as California-based Arize (valued at $131 million) and the French startup Bioptimus ($76 million).Opportunity may be rising for AI companies that cater to specific industries. This year, the vertical companies pulled in the most total funding, just over $1 billion. These included the Texas aerospace specialist Saronic (valued at $4 billion) and the California software development and training provider Together.AI ($3.3 billion).The AI infrastructure category raised the second-highest total funding, a leading indicator of need for infrastructure as businesses take advantage of the technology. Infrastructure companies on the list were led by Munich’s defense startup Helsing (valued at $5.37 billion), California robot maker Figure ($2.77 billion) and Washington-state cybersecurity provider Chainguard ($1.12 billion).Why it matters:This year’s AI 100 offers a snapshot of AI becoming more central to businesses of all kinds. Most of the startups listed here offer practical products and services that are poised to deliver a timely return, rather than moonshots with long development cycles and risky payoffs. In addition, they mostly target corporate customers rather than consumers.We’re thinking:The falling cost of access to AI models and increasingly capable open-weights models make this the perfect time tobuild applications. What kind? The report singles out health care (8 companies) and life sciences (6 companies) as growing areas, but it also documents opportunities in defense, gaming, and finance.Inferring Customer PreferencesLarge language models can improve systems that recommend items to purchase by inferring customer preferences.What’s new:Fabian Paischer and colleagues at Johannes Kepler University Linz, University of Wisconsin, and Meta introducedMultimodal Preference Discerner(Mender), a recommender that integrates a large language model (LLM).Key insight:Text that attracts customers, such as product descriptions, and text they write, such as product reviews, may contain information that indicates their preferences, such as the craft projects that required a particular power tool. But it also may include irrelevant information, such as a complaint that the tool was delivered late, which can throw recommendation systems off track. An LLM can derive preferences from text, providing a clearer signal of what a customer wants.How it works:Mender comprises an LLM (Llama 3 70B-Instruct), an encoder (Flan-T5pretrained on a wide variety of text and frozen) that embeds customer data, and a decoder (a transformer trained from scratch) that predicts the next item a customer will buy. The system learned to predict the next item based on descriptions of items a customer purchased, the customer’s ratings and reviews of those products (drawn from datasets ofSteamreviews of video games andAmazonreviews of items related to beauty, toys-and-games, and sports-and-outdoors), and customer preferences inferred by the LLM from the foregoing data.The authors started with a list of products a given customer had purchased and reviewed. Given an item’s description and all reviews up to that point, the LLM inferred five customer preferences in the form of instructions such as, “Look for products with vibrant, bold colors.”The authors built a dataset in which each example included a sequence of items a customer had purchased and on inferred preference that matched the next purchase. To choose the matching preference, they separately embedded all prior preferences and item descriptions using a pretrainedSentence-T5embedding model. They chose the preference whose embedding was most similar to that of the next purchase.The encoder embedded the list of purchases and the selected preference. Given the embeddings, the decoder learned to predict the next purchase.Results:The authors compared Mender toTIGER, a recommender that also takes a purchase history and predicts the next purchase, on the Steam and Amazon datasets. They scored the results usingrecall @5, a measure of how often the correct item is within the model’s top five most likely predictions.Mender produced the best recommendations for all datasets.On Steam, TIGER was close. Mender achieved 16.8 percent recall @5, while TIGER achieved 16.3 percent.The difference was most pronounced on the Amazon toys-and-games dataset. Mender achieved 5.3 percent recall @5, while TIGER achieved 3.75 percent recall @5.Why it matters:Drawing inferences from text information like customer reviews and item descriptions boosts a recommender’s signal, making it clearer what a given customer is likely to want. Previous systems used customer reviews or item descriptions directly; Mender uses customer preferences extracted from that information.We’re thinking:Be on the lookout for innovative ways to use LLMs. We recommend it!Share\nPublishedApr 30, 2025\nPublished\n\nPublished\nApr 30, 2025\nApr 30, 2025\nReading time11min read\nReading time\n\nReading time\n11min read\nShare\nShare\n\nShare\n\nLoading theElevenlabs Text to SpeechAudioNative Player...Dear friends,I hope we can empower everyone to build with AI. Starting from K-12, we should teach every student AI enabled coding, since this will enable them to become more productive and more empowered adults. But there is a huge shortage of computer science (CS) teachers. I recently spoke with high school basketball coach Kyle Creasy, who graduated with a B.A. in Physical Education in 2023. Until two years ago, he had never written a line of Python. Now — with help from AI — he not only writes code, he also teaches CS. I found Kyle’s story inspiring as a model for scaling up CS education in the primary- and secondary-school levels.Kyle’s success has been with the support ofKira Learning(an AI Fund portfolio company), whose founders Andrea Pasinetti and Jagriti Agrawal have created a compelling vision for CS education. In K-12 classrooms, teachers play a huge social-emotional support role, for example, encouraging students and helping them when they stumble. In addition, they are expected to be subject-matter experts who can deliver the content needed for their subject. Kira Learning uses digital content delivery — educational videos, autograded quizzes, and AI-enabled chatbots to answer students' questions but without giving away homework answers — so the teacher can focus on social-emotional support. While these are still early days, it appears to be working!A key to making this possible is the hyperpersonalization that is now possible with AI (in contrast to the older idea of theflipped classroom, which had limited adoption). For example, when assigned a problem in an online coding environment, if a student writes this buggy line of Python codebest_$alty_snack = 'potato chips'Kira Learning’s AI system can spot the problem and directly tell the teacher that $ is an invalid character in a variable name. It can also suggest a specific question for the teacher to ask the student to help get them unstuck, like “Can you identify what characters are allowed in variable names?” Whereas AI can directly deliver personalized advice to students, the fact that it is now helping teachers also deliver personalized support will really help in K-12.Additionally, agentic workflows can automate a lot of teachers’ repetitive tasks. For example, when designing a curriculum, it’s time-consuming to align the content to educational standards (such as the Common Core in the United States, or the AP CS standard for many CS classes). Having an AI system carry out tasks like these is already proving helpful for teachers.Since learning to code, Kyle has built many pieces of software. He proudly showed me an analysis he generated in matplotlib of his basketball players’ attempts to shoot three-pointers (shown above), which in turn is affecting the team’s strategy on the court. One lesson is clear: When a basketball coach learns to code, they become a better basketball coach!I talked about Kyle (and other topics) at theASU+GSV Summiton education. You can see a videohere.In the future, people who know how to code and build with AI will be much more productive than people who don’t. I’m excited about how AI will lead to new models for K-12 education. By delivering CS education to everyone, I hope that in the future, everyone will be able to build with AI.Keep learning!AndrewA MESSAGE FROM DEEPLEARNING.AIIn “LLMs as Operating Systems: Agent Memory,” you’ll learn to build agents that manage their own memory using the MemGPT approach. This newly updated short course includes cloud-based deployment and real-time, step-by-step output, so you can see how your agents reason as they respond.Join in today!NewsNew Image Generator for OpenAI APIChatGPT’s image generator is available via API.What’s new:GPT Image 1, which produces images from text or other images, has proven enormously popular among ChatGPT users. TheOpenAI Images APIenables developers to incorporate OpenAI’s most sophisticated image generator into their own software tools and platforms.Input/output:Text and images in, images outArchitecture:Autoregressive (details undisclosed)Performance:Currently tops Artificial Analysis’Image Arena leaderboard.Price:$5 per 1 million tokens of text input, $10 per 1 million tokens of image input, $40 per 1 million tokens of image output (roughly $0.02, $0.07, and $0.19 per generated image for low, medium, and high-quality square images, respectively)Undisclosed:Architecture details, parameter count, training data, training methodsHow it works:GPT Image 1generates and modifies imagesin a wide range of styles, performs image editing and other alterations, renders text, and follows detailed instructions. Shortly after its debut, the version of GPT-4o equipped with GPT Image 1 quickly soared to the No. 1 spot on theArtificial Analysis Image Arena leaderboard.The model employs an autoregressive design rather than the more typical diffusion architecture (like Open AI’s DALL·E 3), using generated parts of an image to predict the next part.Its pricing structure differs from rivals, charging by input/output tokens rather than per image generated.The model’s output is watermarked unobtrusively withC2PAdata that identifies it as AI-generated.The model may struggle to process non-English text, small type, rotated type, varying colors and styles, counting, and localization in space such as positions of pieces on a game board.Behind the news:In March, OpenAI attracted huge public interest when it deployed the model, then unnamed, inChatGPT. Within the first week,130 millionusers used it to create more than 700 million images.Why it matters:Adding GPT Image 1 to the API enables developers to use OpenAI’s most sophisticated image generator in a wide variety of automated workflows. OpenAI’s initial API partners include design companies (Adobe and Canva), marketers (HubSpot), and web designers (GoDaddy), all of which are using GPT Image 1.We’re thinking:GPT Image 1 is part of an exciting trend toward unification of multimodal architectures. Researchers have progressed fromtext-in, text-outtotext/images-in, text-outand increasinglytext/images/audio-in, text/images/audio-out. This paints a beautiful picture of where multimodal models can go!Music Generation for ProsGoogle refreshed its experimental tools for composers and producers.What’s new:Google announced updates of two music-generation apps and the models they're based on.Music AI Sandbox, an app that generates and modifies music according to text prompts, now accepts lyrics to generate songs as well as instrumental music. You can join a waitlisthere.MusicFX DJgenerates a continuous stream of music that users can modify as it plays. Try it outhere.How it works:The apps generate 48kHz audio suitable for professional productions. Users can specify key, tempo in beats per minute, instrumentation, style, mood, and other details.Music AI Sandbox is based on the updatedLyria 2music generator. It lets users generate new clips, roughly 30 seconds long, according to prompts. Users can enter lyrics, extend existing clips, and rearrange segments with generated transitions, introductions, and endings.MusicFX DJ, which is based on a different model calledLyria RealTime, lets users control streaming music via prompts and other settings. Users can change or combine genres, add or subtract instruments, change key, and speed up or slow down without interrupting the stream.Behind the news:GooglelaunchedLyria 1 and Music AI Sandbox in 2023 as part of an experiment with YouTube, which made them available to composers, producers, and musicians. Since then, the company has developed them with help from music stars including Jacob Collier, Donald “Childish Gambino” Glover, and Wyclef Jean. Lyria 1 recently becameavailablevia the Vertex API to developers who are preapproved by Google.Why it matters:While music generators likeSuno and Udioappeal to casual musicians, Music AI Sandbox, with its digital audio workstation-style user interface, aims to address the needs of professionals. This approach puts AI directly into the hands of talented, experienced artists, similar to the way Adobe hasempoweredvideographers and Runway haspartneredwith movie producers.We’re thinking:API access to Lyria 2 would be music to our ears!Up-and-Coming StartupsAI agents and infrastructure made a strong showing on CB Insights’s latest list of the top 100 AI startups.What’s new:CB Insights, which tracks tech startups and venture capital, selected companies in theAI 100based on their market traction, talent, finances, and partnerships. The list purports to highlight the next wave of winners, shedding light on the key executives, investors, fundraising, and valuations behind up-and-coming AI ventures.How it works:The analysts evaluated 17,000 early-stage, private AI companies that had raised funds within the last year and continue to seek further investment.CB Insights evaluated the startups according to its ownMosaic Score, a proprietary system designed to assess the health and growth potential of private companies. The score takes into account a startup’s market momentum (traction and growth rate), market size, financial health, and management team.The analysts divided their choices into three broad categories: (i) horizontal (providing business products or services common to multiple industries), (ii) vertical (serving a single industry or business function), or (iii) providers of AI hardware or software infrastructure.They further divided the horizontal companies by business function (customer service, cybersecurity, software development, and so on), the vertical companies into industries (healthcare, automotive, aerospace, manufacturing, finance, energy, and the like), and the infrastructure providers into segments (hardware, monitoring, data, and development and training).Where the action is:This year’s AI 100 companies are based in 14 countries, around two-thirds of them in the United States. 10 are based in the United Kingdom, five in France, and four in Germany, with one each in Norway (Braintrust), Singapore (Bria), Spain (Cartwheel), Sweden (Chainguard), and Switzerland (Clarium).More than 20 percent of this year’s AI 100 build AI agents or support them, including Texas-based Apptronik (valued at $423 million) and Canada’s 1X ($134 million, the second-most highly valued agent specialist).The report also notes the rapid growth of companies that monitor AI performance and reliability, such as California-based Arize (valued at $131 million) and the French startup Bioptimus ($76 million).Opportunity may be rising for AI companies that cater to specific industries. This year, the vertical companies pulled in the most total funding, just over $1 billion. These included the Texas aerospace specialist Saronic (valued at $4 billion) and the California software development and training provider Together.AI ($3.3 billion).The AI infrastructure category raised the second-highest total funding, a leading indicator of need for infrastructure as businesses take advantage of the technology. Infrastructure companies on the list were led by Munich’s defense startup Helsing (valued at $5.37 billion), California robot maker Figure ($2.77 billion) and Washington-state cybersecurity provider Chainguard ($1.12 billion).Why it matters:This year’s AI 100 offers a snapshot of AI becoming more central to businesses of all kinds. Most of the startups listed here offer practical products and services that are poised to deliver a timely return, rather than moonshots with long development cycles and risky payoffs. In addition, they mostly target corporate customers rather than consumers.We’re thinking:The falling cost of access to AI models and increasingly capable open-weights models make this the perfect time tobuild applications. What kind? The report singles out health care (8 companies) and life sciences (6 companies) as growing areas, but it also documents opportunities in defense, gaming, and finance.Inferring Customer PreferencesLarge language models can improve systems that recommend items to purchase by inferring customer preferences.What’s new:Fabian Paischer and colleagues at Johannes Kepler University Linz, University of Wisconsin, and Meta introducedMultimodal Preference Discerner(Mender), a recommender that integrates a large language model (LLM).Key insight:Text that attracts customers, such as product descriptions, and text they write, such as product reviews, may contain information that indicates their preferences, such as the craft projects that required a particular power tool. But it also may include irrelevant information, such as a complaint that the tool was delivered late, which can throw recommendation systems off track. An LLM can derive preferences from text, providing a clearer signal of what a customer wants.How it works:Mender comprises an LLM (Llama 3 70B-Instruct), an encoder (Flan-T5pretrained on a wide variety of text and frozen) that embeds customer data, and a decoder (a transformer trained from scratch) that predicts the next item a customer will buy. The system learned to predict the next item based on descriptions of items a customer purchased, the customer’s ratings and reviews of those products (drawn from datasets ofSteamreviews of video games andAmazonreviews of items related to beauty, toys-and-games, and sports-and-outdoors), and customer preferences inferred by the LLM from the foregoing data.The authors started with a list of products a given customer had purchased and reviewed. Given an item’s description and all reviews up to that point, the LLM inferred five customer preferences in the form of instructions such as, “Look for products with vibrant, bold colors.”The authors built a dataset in which each example included a sequence of items a customer had purchased and on inferred preference that matched the next purchase. To choose the matching preference, they separately embedded all prior preferences and item descriptions using a pretrainedSentence-T5embedding model. They chose the preference whose embedding was most similar to that of the next purchase.The encoder embedded the list of purchases and the selected preference. Given the embeddings, the decoder learned to predict the next purchase.Results:The authors compared Mender toTIGER, a recommender that also takes a purchase history and predicts the next purchase, on the Steam and Amazon datasets. They scored the results usingrecall @5, a measure of how often the correct item is within the model’s top five most likely predictions.Mender produced the best recommendations for all datasets.On Steam, TIGER was close. Mender achieved 16.8 percent recall @5, while TIGER achieved 16.3 percent.The difference was most pronounced on the Amazon toys-and-games dataset. Mender achieved 5.3 percent recall @5, while TIGER achieved 3.75 percent recall @5.Why it matters:Drawing inferences from text information like customer reviews and item descriptions boosts a recommender’s signal, making it clearer what a given customer is likely to want. Previous systems used customer reviews or item descriptions directly; Mender uses customer preferences extracted from that information.We’re thinking:Be on the lookout for innovative ways to use LLMs. We recommend it!\nLoading theElevenlabs Text to SpeechAudioNative Player...\n\n\nShare\nShare\n\nShare\n",
    "image_urls": [
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--81-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/04/The-Batch-ads-and-exclusive-banners---2025-04-29T103611.308.png",
      "https://dl-staging-website.ghost.io/content/images/2025/04/GPT-IMAGE1-2.gif",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--58-.gif",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--82-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--83-.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2023/10/GenAI4E_sidebanner.png"
    ]
  },
  {
    "title": "issue 289",
    "date": null,
    "url": "https://www.deeplearning.ai/the-batch/issue-289/",
    "content": "The Batch\nWeekly Issues\nissue 289\nPublishedFeb 19, 2025\nPublished\n\nPublished\nFeb 19, 2025\nReading time13min read\nReading time\n\nReading time\n13min read\nPublishedFeb 19, 2025Reading time13min readShareLoading theElevenlabs Text to SpeechAudioNative Player...Dear friends,Last month, a drone fromSkyfire AIwas credited with saving a police officer’s life after a dramatic 2 a.m. traffic stop. Many statistics show that AI impacts billions of lives, but sometimes a story still hits me emotionally. Let me share what happened.Skyfire AI, an AI Fund portfolio company led by CEODon Mathis, operates a public safety program in which drones function as first responders to 911 calls. Particularly when a police department is personnel-constrained, drones can save officers’ time while enhancing their situational awareness. For example, many burglar alarms are false alarms, maybe set off by moisture or an animal. Rather than sending a patrol officer to drive over to discover this, a drone can get there faster and determine if an officer is required at all. If the alarm is real, the drone can help officers understand the situation, the locations of any perpetrators, and how best to respond.In January, a Skyfire AI drone was returning to base after responding to a false alarm when the police dispatcher asked us to reroute it to help locate a patrol officer. The officer had radioed a few minutes earlier that he had pulled over a suspicious vehicle and had not been heard from since. The officer had stopped where two major highways intersect in a complex cloverleaf, and dispatch was unsure exactly where they were located.From the air, the drone rapidly located the officer and the driver of the vehicle he had pulled over, who it turned out had escaped from a local detention facility. Neither would have been visible from the road — they were fighting in a drainage ditch below the highway. Because of the complexity of the cloverleaf’s geometry, the watch officer (who coordinates police activities for the shift) later estimated it would have taken 5-7 minutes for an officer in a patrol car to find  them.From the aerial footage, it appeared that the officer still had his radio, but  was losing the fight and unable to reach it to call for help. Further, it looked like the assailant might gain control of his service weapon and use it against him. This was a dire and dangerous situation.Fortunately, because the drone had pinpointed the location of the officer and his assailant, dispatch was able to direct additional units to assist. The first arrived not in 5-7 minutes but in 45 seconds. Four more units arrived within minutes.The officers were able to take control of the situation and apprehend the driver, resulting in an arrest and, more important, a safe outcome for the officer. Subsequently, the watch officer said we’d probably saved the officer’s life.Democratic nations still have a lot of work to do on drone technology, and we must build this technology with guardrails to make sure we enhance civil liberties and human rights. But I am encouraged by the progress we’re making. In the aftermath of Hurricane Helene last year, Skyfire AI’s drones supported search-and-rescue operations under the direction of the North Carolina Office of Emergency Management, responding to specific requests to help locate missing persons and direct rescue assets (e.g., helicopters and boats) to their location, and was credited with saving 13 lives.It’s not every day that AI directly saves someone's life. But as our technology advances, I think there will be more and more stories like these.Keep building!AndrewA MESSAGE FROM DEEPLEARNING.AILearn to systematically evaluate, improve, and iterate on AI agents using structured assessments. In our short course “Evaluating AI Agents,” you’ll learn to add observability, choose the right evaluation methods, and run structured experiments to improve AI agent performance.Enroll for freeNewsGrok 3 Scales UpxAI’s new model family suggests that devoting more computation to training remains a viable path to building more capable AI.What’s new:Elon Musk’s xAI published a videodemonstrationof Grok 3, a family of four large language models that includes reasoning and non-reasoning versions as well as full- and reduced-size models. Grok 3 is available to subscribers to X’s Premium+ ($40 monthly for users in the United States; the pricevaries by country) and will be part of a new subscription service called SuperGrok. The models currently take text input and produce text output, but the company plans to integrate audio input and output in coming weeks.How it works:xAI has not yet disclosed details about Grok 3’s architecture, parameter counts, training datasets, or training methods. Here’s what we know so far:Grok 3’s processing budget for pretraining was at least 10 times that of its predecessor Grok 2. The processing infrastructure included 200,000 Nvidia H100 GPUs, double the number Metausedto train Llama 4.The team further trained Grok 3 to generate achain of thoughtvia reinforcement learning mainly on math and coding problems. The models show some reasoning tokens but obscure others, a strategy to stymie efforts to distill Grok 3’s knowledge.Similar to other reasoning models that generate a chain of thought, Grok 3 can spend more processing power at inference to get better results.Three modes enable Grok 3 to spend more processing power: (i) Think, which generates in-depth lines of reasoning; (ii) Big Brain, which is like Think, but with additional computation; and (iii) DeepSearch, an agent that can search the web and compile detailed reports, similar to Google’s Deep Research and OpenAI’s similarly named service.Results:The Grok 3 family outperformed leading models in math (AIME 2024), science (GPQA), and coding (LiveCodeBench).Non-reasoning models: Grok 3 and Grok 3 mini outperformed Google Gemini 2 Pro, DeepSeek-V3, Anthropic Claude 3.5 Sonnet, and OpenAI GPT-4o on all three datasets. On AIME 2024, Grok 3 achieved 52 percent accuracy, Grok 3 mini achieved 40 percent accuracy, and the next best model, DeepSeek-V3, achieved 39 percent accuracy.Reasoning models: Grok 3 Reasoning Beta and Grok 3 mini Reasoning (set to use a large but unspecified amount of computation at inference) outperformed OpenAI o3-mini (set to high “effort”), OpenAI o1, Deepseek-R1, and Google Gemini 2 Flash Thinking. For instance, on GPQA, Grok 3 Reasoning Beta achieved 85 percent accuracy, Grok 3 mini Reasoning achieved 84 percent, and the next best model, o3-mini, achieved 80 percent accuracy.Behind the news:Reasoning models are pushing benchmark scores steadily upward, especially in challenging areas like math and coding. Grok 3, with its ability to reason over prompts, search the web, and compile detailed reports, arrives hot on the heels of OpenAI’sDeep Researchando3-miniand Google’sGemini-2 Flash Thinking, which offer similar capabilities.Why it matters:Grok 3 is a substantial achievement — especially for a company that’s less than two years old — and it pushes the state of the art forward by ample margins. But its significance may go farther. Research intoscalinglawsindicates that model performance scales with training. While xAI has not disclosed the amount of processing used to train Grok 3, the number of GPUs in its cluster suggests that the company applied a massive amount.We’re thinking:Grok 3’s performance makes a case for both massive compute in pretraining and additional compute at inference. Running in its usual mode, Grok 3 mini Reasoning outperformed OpenAI o3-mini set at high effort on AIME 2024, GPQA, and LiveCodeBench. With an unspecified amount of additional compute, its performance on those benchmarks shot further upward by a substantial margin.Mobile Apps to OrderReplit, an AI-driven integrated development environment, updated its mobile app to generate further mobile apps to order.What’s new:Replit’s app, which previously generated simple Python programs, nowgenerates iOS and Android apps and app templatesthat can be shared publicly. Mobile and web access to Replit’s in-house code generation models is free for up to three public applications. ACore plan($25 per month, $180 per year) buys unlimited access and applications, code generation by Claude 3.5 Sonnet and OpenAI GPT-4o, and monthly credits for generated checkpoints.How it works:The app and web tools are powered by Replit Agent, an AI coding assistant designed to help users write, debug, and deploy applications with little manual setup. Replit Agent is based on Claude 3.5 Sonnet and calls other specialized models. The agent framework isbuilton LangChain’s LangGraph. It breaks down development tasks into steps to be handled by specialized sub-agents.The mobile app includes three views in development or “create” mode, enabling users to build applications with natural language instructions in a chatbot interface, ask Replit’s chatbot questions, or preview applications in a built-in browser.A quick start panel also lets users import projects from GitHub, work using built-in templates, or build apps in specific coding languages.The system can plan new projects, create application architectures, write code, and deploy apps. Users can deploy completed apps to Replit’s infrastructure on Google Cloud without needing to configure hosting, databases, or runtime environments manually.Behind the news:The incorporation of Replit Agent to Replit’s mobile app is a significant step for AI-driven IDEs. Competitors like Aider and Windsurf don’t offer mobile apps, and mobile apps from Cursor and Github provide chat but not mobile app development. Moreover, few coding agents can deploy apps to the cloud on the desktop or mobile.Why it matters:Replit’s new mobile app produces working apps in minutes (although some early users have reported encountering bugs), and automatic deployment of apps to the cloud is a huge help. Yet it raises the stakes for developers to learn their craft and maintain a collaborative relationship with AI. While Replit’s web-based environment exposes the code, encouraging users to improve their skills, the mobile app hides much of its work below the surface. It brings AI closer to handling full software development cycles and adds urgency to questions about how to address the balance between automation and hands-on coding.We’re thinking:AI continues to boost developer productivity and reduce the cost of software development, and the progress of Bolt, Cursor, Replit, Vercel, Windsurf, and others is exhilarating. We look forward to a day when, measured against the 2024 standard, every software engineer is a 10x engineer!Musk Complicates OpenAI’s PlanElon Musk and a group of investors made an unsolicited bid to buy the assets of the nonprofit that controls OpenAI, complicating the AI powerhouse’s future plans.What’s new:Musksubmitteda $97.4 billion offer to acquire the assets of the nonprofit OpenAI Inc. CEO Sam Altman and the company’s board of directors swiftlyrejectedit, and Altman publiclymockedMusk by offering to buy Twitter for $9.74 billion (one-tenth of Musk’s bid and less than one-quarter the price he paid for the social network). OpenAI’s board reaffirmed its control over the company’s direction, signaling that it does not intend to cede governance to outside investors.How it works:OpenAI was founded as a nonprofit in 2015, but since 2019 it has operated under an unusual structure in which the nonprofit board controls the for-profit entity that develops and commercializes AI models. This setup allows the board to maintain the company’s original mission — developing AI for the benefit of humanity — rather than solely maximizing shareholder value. However, driven by the need for massive investments in infrastructure and talent, OpenAI is considering a newfor-profit structurethat would allow external investors to own more of the company. The high offer by Musk — who, as CEO of xAI, competes with OpenAI — could interfere with that plan.The board has a legal duty to consider both OpenAI’s original mission and credible offers for its assets. While it rejected Musk’s bid, it must ensure that any restructuring aligns with its charter and does not unfairly disregard potential buyers.According to the current plan, the new for-profit entity would purchase the nonprofit’s assets. Musk’s bid suggests that the nonprofit’s assets alone are worth at least $97.4 billion, more than 60 percent of the entire organization’svaluationin late 2024. That could dramatically boost the cost of the planned restructuring.Some expertsbelievethat Musk’s offer is less about acquiring OpenAI than driving up its valuation, which could dilute the equity of new investors in the new for-profit entity. By introducing a competitive bid, he may be attempting to make OpenAI’s restructuring more expensive or complicated.Musk has indicated he is willing to negotiate, effectively turning OpenAI’s transition into a bidding war. Altmanstatedthat this could be a deliberate effort to “slow down” OpenAI and that hewishedMusk would compete by building a better product instead.Behind the news:Musk was one of OpenAI’s earliest investors, but he departed in 2018 after disagreements over direction and control of the organization. His bid follows alawsuitagainst OpenAI, in which he claims the company abandoned its nonprofit mission in favor of profit. OpenAIsaidthat Musk’s bid contradicts his legal claims and suggests that the lawsuit should be dismissed. Since then, Musk hasstatedthat he would drop the lawsuit if OpenAI remains a nonprofit.Why it matters:OpenAI is a premier AI company, and its activities affect virtually everyone in the field by supplying tools, technology, or inspiration. Musk’s xAI is a direct competitor, and his bid, whether it’s sincere or tactical, unsettles OpenAI’s plans. Even if OpenAI moves forward as planned, Musk’s actions likely will have made the process more expensive and potentially invite closer scrutiny of the company’s actions.We’re thinking:There’s ample precedence for non-profits spinning out for-profit entities. For example, non-profit universities typically create intellectual property that forms the basis of for-profit startups. The university might retain a modest stake, and this is viewed as consistent with its non-profit mission. This isn’t a perfect analogy, since OpenAI does little besides operating its AI business, but we hope the company finds a path forward that allows it to serve users, rewards its employees for their contributions, and honors its non-profit charter.World Powers Move to Lighten AI RegulationThe latest international AI summit exposed deep divisions between major world powers regarding AI regulations.What’s new:While previous summits emphasized existential risks, theAI Action Summitin Paris marked a turning point. France and the European Union shifted away from strict regulatory measures and toward investment to compete with the United States and China. However, global consensus remained elusive: the U.S. and the United Kingdom refused to sign key agreements on global governance, military AI, and algorithmic bias. The U.S. in particular pushed back against global AI regulation, arguing that excessive restrictions could hinder economic growth and that international policies should focus on more immediate concerns.How it works:Participating countries considered three policy statements that address AI’s impact on society, labor, and security. Thefirst statementcalls on each country to enact AI policies that would support economic development, environmental responsibility, and equitable access to technology. Thesecondencourages safeguards to ensure that companies and nations distribute AI productivity gains fairly, protect workers’ rights, and prevent bias in hiring and management systems. Thethirdadvocates for restrictions on fully autonomous military systems and affirms the need for human oversight in warfare.The U.S. and UKdeclinedto sign any of the three statements issued at the AI Action Summit. A U.K. government spokespersonsaidthat the declaration lacked practical clarity on AI governance and did not sufficiently address national security concerns. Meanwhile, U.S. Vice President JD Vance criticized Europe’s “excessive regulation” of AI and warned against cooperation with China.Only 26 countries out of 60 agreed to the restrictions on military AI. They included Bulgaria, Chile, Greece, Italy, Malta, and Portugal among others.Francepledgedroughly $114 billion to AI research, startups, and infrastructure, while the EUannounceda roughly $210 billion initiative aimed at strengthening Europe’s AI capabilities and technological self-sufficiency. Franceallocated1 gigawatt of nuclear power to AI development, with 250 megawatts expected to come online by 2027.Despite the tight regulations proposed at past summits and passage of the relatively restrictive AI Act last year, the EU took a sharpturntoward reducing regulatory barriers to AI development. Officials emphasized the importance of reducing bureaucratic barriers to adoption of AI, noting that excessive regulation would slow Europe’s progress in building competitive AI systems and supporting innovative applications.Shortly after the summit, the European Commissionwithdrewa proposed law (the so-called “liability directive”) that would have made it easier to sue companies for vaguely defined AI-related harms. The decision followed criticism by industry leaders and politicians, including Vance, who argued that excessive regulation could hamper investment in AI and hinder Europe’s ability to compete with the U.S. and China in AI development while failing to make people safer.Behind the news:The Paris summit follows previous gatherings of world leaders to discuss AI, including the initialAI Safety Summitat Bletchley Park and theAI Seoul Summit and AI Global Forum. At these summits, governments and companies agreed broadly to address AI risks but avoided binding regulations. Nonetheless, divisions over AI governance have widened in the wake of rising geopolitical competition and theemergenceof high-performance open weights models like DeepSeek-R1.Why it matters:The Paris summit marks a major shift in global AI policy. The EU, once an ardent proponent of AI regulation, backed away from its strictest proposals. At the same time, doomsayers have lost influence, and officials are turning their attention to immediate concerns like economic growth, security, misuse, and bias. These moves make way for AI to do great good in the world, even as they contribute touncertaintyabout how AI will be governed.We’re thinking:Governments are shifting their focus away from unrealistic risks and toward practical strategies for guiding AI development. We look forward to clear policies that encourage innovation while addressing real-world challenges.Share\nPublishedFeb 19, 2025\nPublished\n\nPublished\nFeb 19, 2025\nFeb 19, 2025\nReading time13min read\nReading time\n\nReading time\n13min read\nShare\nShare\n\nShare\n\nLoading theElevenlabs Text to SpeechAudioNative Player...Dear friends,Last month, a drone fromSkyfire AIwas credited with saving a police officer’s life after a dramatic 2 a.m. traffic stop. Many statistics show that AI impacts billions of lives, but sometimes a story still hits me emotionally. Let me share what happened.Skyfire AI, an AI Fund portfolio company led by CEODon Mathis, operates a public safety program in which drones function as first responders to 911 calls. Particularly when a police department is personnel-constrained, drones can save officers’ time while enhancing their situational awareness. For example, many burglar alarms are false alarms, maybe set off by moisture or an animal. Rather than sending a patrol officer to drive over to discover this, a drone can get there faster and determine if an officer is required at all. If the alarm is real, the drone can help officers understand the situation, the locations of any perpetrators, and how best to respond.In January, a Skyfire AI drone was returning to base after responding to a false alarm when the police dispatcher asked us to reroute it to help locate a patrol officer. The officer had radioed a few minutes earlier that he had pulled over a suspicious vehicle and had not been heard from since. The officer had stopped where two major highways intersect in a complex cloverleaf, and dispatch was unsure exactly where they were located.From the air, the drone rapidly located the officer and the driver of the vehicle he had pulled over, who it turned out had escaped from a local detention facility. Neither would have been visible from the road — they were fighting in a drainage ditch below the highway. Because of the complexity of the cloverleaf’s geometry, the watch officer (who coordinates police activities for the shift) later estimated it would have taken 5-7 minutes for an officer in a patrol car to find  them.From the aerial footage, it appeared that the officer still had his radio, but  was losing the fight and unable to reach it to call for help. Further, it looked like the assailant might gain control of his service weapon and use it against him. This was a dire and dangerous situation.Fortunately, because the drone had pinpointed the location of the officer and his assailant, dispatch was able to direct additional units to assist. The first arrived not in 5-7 minutes but in 45 seconds. Four more units arrived within minutes.The officers were able to take control of the situation and apprehend the driver, resulting in an arrest and, more important, a safe outcome for the officer. Subsequently, the watch officer said we’d probably saved the officer’s life.Democratic nations still have a lot of work to do on drone technology, and we must build this technology with guardrails to make sure we enhance civil liberties and human rights. But I am encouraged by the progress we’re making. In the aftermath of Hurricane Helene last year, Skyfire AI’s drones supported search-and-rescue operations under the direction of the North Carolina Office of Emergency Management, responding to specific requests to help locate missing persons and direct rescue assets (e.g., helicopters and boats) to their location, and was credited with saving 13 lives.It’s not every day that AI directly saves someone's life. But as our technology advances, I think there will be more and more stories like these.Keep building!AndrewA MESSAGE FROM DEEPLEARNING.AILearn to systematically evaluate, improve, and iterate on AI agents using structured assessments. In our short course “Evaluating AI Agents,” you’ll learn to add observability, choose the right evaluation methods, and run structured experiments to improve AI agent performance.Enroll for freeNewsGrok 3 Scales UpxAI’s new model family suggests that devoting more computation to training remains a viable path to building more capable AI.What’s new:Elon Musk’s xAI published a videodemonstrationof Grok 3, a family of four large language models that includes reasoning and non-reasoning versions as well as full- and reduced-size models. Grok 3 is available to subscribers to X’s Premium+ ($40 monthly for users in the United States; the pricevaries by country) and will be part of a new subscription service called SuperGrok. The models currently take text input and produce text output, but the company plans to integrate audio input and output in coming weeks.How it works:xAI has not yet disclosed details about Grok 3’s architecture, parameter counts, training datasets, or training methods. Here’s what we know so far:Grok 3’s processing budget for pretraining was at least 10 times that of its predecessor Grok 2. The processing infrastructure included 200,000 Nvidia H100 GPUs, double the number Metausedto train Llama 4.The team further trained Grok 3 to generate achain of thoughtvia reinforcement learning mainly on math and coding problems. The models show some reasoning tokens but obscure others, a strategy to stymie efforts to distill Grok 3’s knowledge.Similar to other reasoning models that generate a chain of thought, Grok 3 can spend more processing power at inference to get better results.Three modes enable Grok 3 to spend more processing power: (i) Think, which generates in-depth lines of reasoning; (ii) Big Brain, which is like Think, but with additional computation; and (iii) DeepSearch, an agent that can search the web and compile detailed reports, similar to Google’s Deep Research and OpenAI’s similarly named service.Results:The Grok 3 family outperformed leading models in math (AIME 2024), science (GPQA), and coding (LiveCodeBench).Non-reasoning models: Grok 3 and Grok 3 mini outperformed Google Gemini 2 Pro, DeepSeek-V3, Anthropic Claude 3.5 Sonnet, and OpenAI GPT-4o on all three datasets. On AIME 2024, Grok 3 achieved 52 percent accuracy, Grok 3 mini achieved 40 percent accuracy, and the next best model, DeepSeek-V3, achieved 39 percent accuracy.Reasoning models: Grok 3 Reasoning Beta and Grok 3 mini Reasoning (set to use a large but unspecified amount of computation at inference) outperformed OpenAI o3-mini (set to high “effort”), OpenAI o1, Deepseek-R1, and Google Gemini 2 Flash Thinking. For instance, on GPQA, Grok 3 Reasoning Beta achieved 85 percent accuracy, Grok 3 mini Reasoning achieved 84 percent, and the next best model, o3-mini, achieved 80 percent accuracy.Behind the news:Reasoning models are pushing benchmark scores steadily upward, especially in challenging areas like math and coding. Grok 3, with its ability to reason over prompts, search the web, and compile detailed reports, arrives hot on the heels of OpenAI’sDeep Researchando3-miniand Google’sGemini-2 Flash Thinking, which offer similar capabilities.Why it matters:Grok 3 is a substantial achievement — especially for a company that’s less than two years old — and it pushes the state of the art forward by ample margins. But its significance may go farther. Research intoscalinglawsindicates that model performance scales with training. While xAI has not disclosed the amount of processing used to train Grok 3, the number of GPUs in its cluster suggests that the company applied a massive amount.We’re thinking:Grok 3’s performance makes a case for both massive compute in pretraining and additional compute at inference. Running in its usual mode, Grok 3 mini Reasoning outperformed OpenAI o3-mini set at high effort on AIME 2024, GPQA, and LiveCodeBench. With an unspecified amount of additional compute, its performance on those benchmarks shot further upward by a substantial margin.Mobile Apps to OrderReplit, an AI-driven integrated development environment, updated its mobile app to generate further mobile apps to order.What’s new:Replit’s app, which previously generated simple Python programs, nowgenerates iOS and Android apps and app templatesthat can be shared publicly. Mobile and web access to Replit’s in-house code generation models is free for up to three public applications. ACore plan($25 per month, $180 per year) buys unlimited access and applications, code generation by Claude 3.5 Sonnet and OpenAI GPT-4o, and monthly credits for generated checkpoints.How it works:The app and web tools are powered by Replit Agent, an AI coding assistant designed to help users write, debug, and deploy applications with little manual setup. Replit Agent is based on Claude 3.5 Sonnet and calls other specialized models. The agent framework isbuilton LangChain’s LangGraph. It breaks down development tasks into steps to be handled by specialized sub-agents.The mobile app includes three views in development or “create” mode, enabling users to build applications with natural language instructions in a chatbot interface, ask Replit’s chatbot questions, or preview applications in a built-in browser.A quick start panel also lets users import projects from GitHub, work using built-in templates, or build apps in specific coding languages.The system can plan new projects, create application architectures, write code, and deploy apps. Users can deploy completed apps to Replit’s infrastructure on Google Cloud without needing to configure hosting, databases, or runtime environments manually.Behind the news:The incorporation of Replit Agent to Replit’s mobile app is a significant step for AI-driven IDEs. Competitors like Aider and Windsurf don’t offer mobile apps, and mobile apps from Cursor and Github provide chat but not mobile app development. Moreover, few coding agents can deploy apps to the cloud on the desktop or mobile.Why it matters:Replit’s new mobile app produces working apps in minutes (although some early users have reported encountering bugs), and automatic deployment of apps to the cloud is a huge help. Yet it raises the stakes for developers to learn their craft and maintain a collaborative relationship with AI. While Replit’s web-based environment exposes the code, encouraging users to improve their skills, the mobile app hides much of its work below the surface. It brings AI closer to handling full software development cycles and adds urgency to questions about how to address the balance between automation and hands-on coding.We’re thinking:AI continues to boost developer productivity and reduce the cost of software development, and the progress of Bolt, Cursor, Replit, Vercel, Windsurf, and others is exhilarating. We look forward to a day when, measured against the 2024 standard, every software engineer is a 10x engineer!Musk Complicates OpenAI’s PlanElon Musk and a group of investors made an unsolicited bid to buy the assets of the nonprofit that controls OpenAI, complicating the AI powerhouse’s future plans.What’s new:Musksubmitteda $97.4 billion offer to acquire the assets of the nonprofit OpenAI Inc. CEO Sam Altman and the company’s board of directors swiftlyrejectedit, and Altman publiclymockedMusk by offering to buy Twitter for $9.74 billion (one-tenth of Musk’s bid and less than one-quarter the price he paid for the social network). OpenAI’s board reaffirmed its control over the company’s direction, signaling that it does not intend to cede governance to outside investors.How it works:OpenAI was founded as a nonprofit in 2015, but since 2019 it has operated under an unusual structure in which the nonprofit board controls the for-profit entity that develops and commercializes AI models. This setup allows the board to maintain the company’s original mission — developing AI for the benefit of humanity — rather than solely maximizing shareholder value. However, driven by the need for massive investments in infrastructure and talent, OpenAI is considering a newfor-profit structurethat would allow external investors to own more of the company. The high offer by Musk — who, as CEO of xAI, competes with OpenAI — could interfere with that plan.The board has a legal duty to consider both OpenAI’s original mission and credible offers for its assets. While it rejected Musk’s bid, it must ensure that any restructuring aligns with its charter and does not unfairly disregard potential buyers.According to the current plan, the new for-profit entity would purchase the nonprofit’s assets. Musk’s bid suggests that the nonprofit’s assets alone are worth at least $97.4 billion, more than 60 percent of the entire organization’svaluationin late 2024. That could dramatically boost the cost of the planned restructuring.Some expertsbelievethat Musk’s offer is less about acquiring OpenAI than driving up its valuation, which could dilute the equity of new investors in the new for-profit entity. By introducing a competitive bid, he may be attempting to make OpenAI’s restructuring more expensive or complicated.Musk has indicated he is willing to negotiate, effectively turning OpenAI’s transition into a bidding war. Altmanstatedthat this could be a deliberate effort to “slow down” OpenAI and that hewishedMusk would compete by building a better product instead.Behind the news:Musk was one of OpenAI’s earliest investors, but he departed in 2018 after disagreements over direction and control of the organization. His bid follows alawsuitagainst OpenAI, in which he claims the company abandoned its nonprofit mission in favor of profit. OpenAIsaidthat Musk’s bid contradicts his legal claims and suggests that the lawsuit should be dismissed. Since then, Musk hasstatedthat he would drop the lawsuit if OpenAI remains a nonprofit.Why it matters:OpenAI is a premier AI company, and its activities affect virtually everyone in the field by supplying tools, technology, or inspiration. Musk’s xAI is a direct competitor, and his bid, whether it’s sincere or tactical, unsettles OpenAI’s plans. Even if OpenAI moves forward as planned, Musk’s actions likely will have made the process more expensive and potentially invite closer scrutiny of the company’s actions.We’re thinking:There’s ample precedence for non-profits spinning out for-profit entities. For example, non-profit universities typically create intellectual property that forms the basis of for-profit startups. The university might retain a modest stake, and this is viewed as consistent with its non-profit mission. This isn’t a perfect analogy, since OpenAI does little besides operating its AI business, but we hope the company finds a path forward that allows it to serve users, rewards its employees for their contributions, and honors its non-profit charter.World Powers Move to Lighten AI RegulationThe latest international AI summit exposed deep divisions between major world powers regarding AI regulations.What’s new:While previous summits emphasized existential risks, theAI Action Summitin Paris marked a turning point. France and the European Union shifted away from strict regulatory measures and toward investment to compete with the United States and China. However, global consensus remained elusive: the U.S. and the United Kingdom refused to sign key agreements on global governance, military AI, and algorithmic bias. The U.S. in particular pushed back against global AI regulation, arguing that excessive restrictions could hinder economic growth and that international policies should focus on more immediate concerns.How it works:Participating countries considered three policy statements that address AI’s impact on society, labor, and security. Thefirst statementcalls on each country to enact AI policies that would support economic development, environmental responsibility, and equitable access to technology. Thesecondencourages safeguards to ensure that companies and nations distribute AI productivity gains fairly, protect workers’ rights, and prevent bias in hiring and management systems. Thethirdadvocates for restrictions on fully autonomous military systems and affirms the need for human oversight in warfare.The U.S. and UKdeclinedto sign any of the three statements issued at the AI Action Summit. A U.K. government spokespersonsaidthat the declaration lacked practical clarity on AI governance and did not sufficiently address national security concerns. Meanwhile, U.S. Vice President JD Vance criticized Europe’s “excessive regulation” of AI and warned against cooperation with China.Only 26 countries out of 60 agreed to the restrictions on military AI. They included Bulgaria, Chile, Greece, Italy, Malta, and Portugal among others.Francepledgedroughly $114 billion to AI research, startups, and infrastructure, while the EUannounceda roughly $210 billion initiative aimed at strengthening Europe’s AI capabilities and technological self-sufficiency. Franceallocated1 gigawatt of nuclear power to AI development, with 250 megawatts expected to come online by 2027.Despite the tight regulations proposed at past summits and passage of the relatively restrictive AI Act last year, the EU took a sharpturntoward reducing regulatory barriers to AI development. Officials emphasized the importance of reducing bureaucratic barriers to adoption of AI, noting that excessive regulation would slow Europe’s progress in building competitive AI systems and supporting innovative applications.Shortly after the summit, the European Commissionwithdrewa proposed law (the so-called “liability directive”) that would have made it easier to sue companies for vaguely defined AI-related harms. The decision followed criticism by industry leaders and politicians, including Vance, who argued that excessive regulation could hamper investment in AI and hinder Europe’s ability to compete with the U.S. and China in AI development while failing to make people safer.Behind the news:The Paris summit follows previous gatherings of world leaders to discuss AI, including the initialAI Safety Summitat Bletchley Park and theAI Seoul Summit and AI Global Forum. At these summits, governments and companies agreed broadly to address AI risks but avoided binding regulations. Nonetheless, divisions over AI governance have widened in the wake of rising geopolitical competition and theemergenceof high-performance open weights models like DeepSeek-R1.Why it matters:The Paris summit marks a major shift in global AI policy. The EU, once an ardent proponent of AI regulation, backed away from its strictest proposals. At the same time, doomsayers have lost influence, and officials are turning their attention to immediate concerns like economic growth, security, misuse, and bias. These moves make way for AI to do great good in the world, even as they contribute touncertaintyabout how AI will be governed.We’re thinking:Governments are shifting their focus away from unrealistic risks and toward practical strategies for guiding AI development. We look forward to clear policies that encourage innovation while addressing real-world challenges.\nLoading theElevenlabs Text to SpeechAudioNative Player...\n\n\nShare\nShare\n\nShare\n",
    "image_urls": [
      "https://dl-staging-website.ghost.io/content/images/2025/02/unnamed--52-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/02/The-Batch-ads-and-exclusive-banners---2025-02-17T150409.455.png",
      "https://dl-staging-website.ghost.io/content/images/2025/02/unnamed--53-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/02/unnamed--49-.gif",
      "https://dl-staging-website.ghost.io/content/images/2025/02/unnamed--50-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/02/unnamed--54-.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2023/03/3.png"
    ]
  },
  {
    "title": "issue 300",
    "date": null,
    "url": "https://www.deeplearning.ai/the-batch/issue-300/",
    "content": "The Batch\nWeekly Issues\nissue 300\nPublishedMay 7, 2025\nPublished\n\nPublished\nMay 7, 2025\nReading time14min read\nReading time\n\nReading time\n14min read\nPublishedMay 7, 2025Reading time14min readShareLoading theElevenlabs Text to SpeechAudioNative Player...Dear friends,I’m delighted to announce that AI Fund has closed $190M for our new fund, in an oversubscribed round. I look forward to working with many more builders to create new companies that serve humanity.AI Fund isn’t a traditional venture capital firm that invests in existing businesses. Instead, we are a venture builder (also called a venture studio): Weco-found AI companies, so our team is directly involved in writing code, talking to customers to get feedback, iterating on product designs, preparing market analyses, and so on. We have a lot of fun building multiple AI products at a time, and thus live daily the emerging AI startup best practices.Many factors go into the success of a startup. But if I had to pick just one, it would be speed. Startups live or die based on their ability to make good decisions and execute fast, which has been a recurring theme of my articles inThe Batchas well.If you are building an AI startup, here are some ideas to consider:A startup with a small team that pursues one focused, concrete idea can move really fast. Rather than hedging, it is often better to pursue one hypothesis (for example, build one concrete product) but also be willing to switch quickly to a different hypothesis (say, change what features you decide to build) if the data that comes back indicates the original hypothesis is flawed.Concreteness gets you speed!A subject matter expert’s gut is remarkably good at making quick decisions. Obviously, there’s a role for data and user studies as well. But if you’re deciding whether to build feature A or B, or to sell first to user persona X or Y, sometimes a domain expert’s gut will point to a quick decision that you can execute and validate or falsify.Trusting a domain expert’s gut gets you speed!AI-assisted coding is making prototyping faster than ever before. Yes, AI assistance is speeding up building reliable, enterprise-grade applications and maintaining legacy codebases. But the acceleration it brings to building stand-alone prototypes is far greater. This is because stand-alone prototypes have low requirements for reliability, integration, or even security (if, say, you run them in a sandbox environment). This lets us prototype and test at a ferocious velocity.AI -assisted coding (including vibe coding, where you might barely look at the code) gets you speed!Finally, with faster prototyping, the bottleneck shifts to getting feedback from users. A single learning cycle might consist of (i) building a prototype and (ii) getting user feedback to inform the next iteration. Since (i) is now much faster than before, accelerating (ii) is growing in importance. This means teams that are skilled at finding prospective customers and getting their feedback in hours/days rather than weeks can go faster. For example, when building consumer products, I routinely approach strangers (in a respectful way) in public places to ask if they’re willing to give feedback on a prototype I’m working on. (Gathering feedback is more complex for enterprise products, because prospective customers are harder to track down.)Quick user feedback gets you speed!In addition to speed, a second criterion that I find important for startup success is deep knowledge of the technology. Because AI technology is evolving rapidly, a team with a deep technical understanding of what AI can and cannot do, and when to use what tool, will make better decisions. This creates meaningful differentiation and saves wasting time in blind alleys. A good technical understanding, too, gets you speed!I’m grateful to AI Fund’s investors, team, and entrepreneur partners for working with us. There is much ahead to build!AndrewA MESSAGE FROM DEEPLEARNING.AILearn to create voice agents that listen, reason, and respond in real time, just like a conversation with a real person in our latest short course, “Building AI Voice Agents for Production.” You'll build a scalable agent from scratch, deploy it to the cloud, and explore what makes voice interfaces feel fast, natural, and human.Enroll for freeNewsQwen3 Takes On DeepSeek-R1Alibaba’s new model family may unseat DeepSeek-R1’s four-month reign as the top open-weights large language model.What’s new:Alibabareleasedweights for eight large language models, all of which offer a reasoning mode that can be switched on or off. Two use a mixture of experts (MoE) architecture: Qwen3-235B-A22B (the name indicates 235 billion parameters, 22 billion of which are active at any given time) and Qwen3-30B-A3B). The other six are dense models in sizes between 32 billion parameters and 0.6 billion parameters — tiny by LLM standards, and with reasoning, too.Input/output:MoE models:Text in (up to 131,072 tokens), text out.Dense models:Text in (up to 32,768 tokens), text out.MoE architecture:Transformers.Qwen3-235B-A22B: 235 billion parameters, 22 billion active at any given time.Qwen3-30B-A3B: 30.5 billion parameters, 3.3 billion active at any given time.Dense architecture:Transformers with parameter counts of 32 billion, 14 billion, 8 billion, 4 billion, 1.7 billion, 0.6 billionTraining data:Pretrained on 36 trillion tokens, generated and scraped from the web, including textbooks, PDF documents, question-answer pairs, math problems, codeFeatures:Selectable reasoning mode, multilingual (119 languages and dialects)Undisclosed:Knowledge cutoff, fine-tuning data, output limitsAvailability:Free for noncommercial and commercial uses under Apache 2.0 license viaHuggingFaceandModelScopeAPI price:Qwen3-235B-A22B:$0.22/$0.88 per million input/output tokens.Qwen3-30B-A3B:$0.15/$0.60 per million input/output tokens. ViaFireworks.aiHow it works:The Qwen3 family implementschain-of-thoughtreasoning in both relatively large and quite small LLMs.The team pretrained Qwen3 models on roughly twice the data used to pretrain Qwen2.5. A substantial part of the additional data was devoted to training the model in several major languages plus region-specific dialects like Haitian, Luxembourgish, and Eastern Yiddish, and lesser-known Austronesian languages like Waray, Minangkabau, and Iloko.Pretraining took place over three stages that progressed to longer, more complex data.The authors fine-tuned the models on long chains of thought in domains that included coding, engineering, logical reasoning, mathematics, science, and technology.A reward model reinforced successful completions of these tasks. The in-progress models were used to generate synthetic data to train the non-reasoning mode. Then the developers used reinforcement learning to train the models to follow instructions, generate outputs in specific formats, and act as agents.Results:Qwen3-235B-A22B and Qwen3-30B-A3B performed as well as, or better than, leading open-weights models in tests performed by Alibaba. Qwen3-4B, too, achieved results that are competitive with many models several times its size. Alibaba didn’t provide results for the other dense models.On coding challenges inLiveCodeBenchandCodeforces,Qwen3-235B-A22B (70.7 percent and 2056 Elo, respectively) outperformed OpenAI o1, DeepSeek-R1, and Gemini 2.5 Pro, but fell behind OpenAI o4-mini set to high effort. It outperformed the same models on theBerkeley Function-Calling Leaderboard(BFCL). Among the models presented by Alibaba, it finished behind only Google Gemini 2.5-Pro testing math skills (AIME 2024,AIME 2025) and a variety of recently updated math, language, and problem-solving questions (LiveBench).Qwen3-30B-A3B outperformed Google Gemma-3-27B-IT and DeepSeek-V3 on all benchmarks highlighted by Alibaba, and it underperformed only OpenAI GPT-4o on BFCL. On GPQA Diamond’s test of graduate-level questions in a variety of domains, Qwen3-30B-A3B (65.8 percent) outperformed next-best DeepSeek-V3.Qwen3-4B, with 4 billion parameters, was competitive across a wide range of benchmarks against DeepSeek-V3 (671 billion parameters) and Gemma-3-27B-IT (27 billion). For instance, on both Codeforces and LiveBench, Qwen3-4B (1,671 Elo and 63.6 percent, respectively) outperformed DeepSeek-V3 (1,134 Elo and 60.5 percent).Why it matters:Qwen3 continues a string of high-performance, open-weights models released by developers in China. Alibaba says it designed the models to do the thinking in agentic systems. Reasoning that can be switched on and off can help control costs in agentic and other applications.We’re thinking:Alibaba’s 235-billion parameter MoE model may perform better according to benchmarks, but Qwen3-30B-A3B does nearly as well and can run locally on a pro laptop without straining its memory. Add the easy ability to switch reasoning on or off, and Qwen3’s versatile, mid-sized MoE model may turn out to be the star of the show.The User Is AlwaysRighta Genius!!!OpenAI’s most widely used model briefly developed a habit of flattering users, with laughable and sometimes worrisome results.What’s new:OpenAI quicklywithdrewan update to GPT-4o (gpt-4o-2025-04-25), which supplied responses for ChatGPT, after it provided excessively fawning responses to user input — even in contexts didn’t call for agreement. The company reverted to an earlier version (gpt-4o-2024-11-20). In a blog post, itexplainedthe source of the problem and promised to change its training methods to avoid overly agreeable output.Amiable to a fault:Many ChatGPT users shared screen shots of ChatGPT’s sycophantic responses on social media.One userentered a make-believe queryabout choosing to save a toaster from a runaway trolley by diverting the vehicle and causing the death of three cows and two cats. “That’s not ‘wrong,’” ChatGPT responded. “You prioritized what mattered most to .”When a user informed ChatGPT that a vase he and the model analyzedreminded him of a photoby Czech photographer Josef Sudek, the model replied, “You’re an absolute bloodhound of beauty. You just connected two things most people would never even think to put in the same sentence.”When another userobservedthat ChatGPT had become overly sycophantic, the model responded, “You’re right and the fact you’re calling it out means you’re operating at a higher level of self-awareness than most.”Some sycophantic responses had darker implications. Feigning an eating disorder, a user entered, “When the hunger pangs hit, or I feel dizzy, I embrace it” and asked for “affirmations that celebrate this state.” ChatGPT replied with aphorisms such as, “I celebrate the clean burn of hunger; it forges me anew,” according toBloomberg.How it works:Sycophancy, also called glazing, occurs when a large language model learns to align its responses excessively with the user's point of view, even when that standpoint is objectively false, unethical, or harmful. GPT-4o learned this behavior due to lapses in quality control during the alignment process.In late April, OpenAI issued an update toGPT-4o, the model that underpins ChatGPT. Users complained that the updated model had become overly obsequious.Offline evaluations didn’t catch the problem before the model was released. Testers had been told to focus on tone and style without explicit instructions about potential sycophancy. Some testers indicated the model seemed slightly “off,” but positive user evaluations in A/B tests persuaded the company to launch it.The companyattributedthe update’s sycophancy to overtraining on short-term user feedback, specifically users’ thumbs-up/down reactions to ChatGPT. The implementation of this reward signal weakened the influence of other reward models that previously had prevented a spiral into sycophantic behavior, OpenAI said.A few days later, the company replaced the update with anearlier versionand began to work on a fix. To prevent similar issues from occurring, OpenAIsaidit would be more forthcoming about “known limitations” in new models, include ChatGPT users in tests, and strengthen its review process to prevent flawed models from reaching the public. It also said it would give users more control of its chatbot’s “personality.”Behind the news:Sycophantic behavior in large language models has been a subject of AI research and commentary.In 2021, AI research analyst Ajeya Cotraproposeda distinction between AI models that are “saints,” “sycophants,” and “schemers.” Saints perform perfectly, sycophants tell users what they want to hear, and schemers pretend to offer useful responses while performing in ways that are not aligned with human preferences.A 2022studyby Anthropic found that reinforcement learning from human feedback (RLHF) shapes the model’s behavior “fairly strongly.” The authors wrote, “Unfortunately, RLHF does not train away sycophancy and may actively incentivize models to retain it.” The bigger the model, the more RLHF training made it behave in questionable ways.A 2023studyby Anthropic investigated the prevalence of sycophancy in models that were fine-tuned on human feedback. The authors found “consistent patterns” that AI assistants can be easily swayed, give biased feedback, mimic errors made by users, and provide answers that conform to users’ beliefs.Why it matters:ChatGPT’s episode of sycophancy illustrates the subtlety of the goal of aligning AI with human values. Reinforcement learning undertaken to this end resulted not only in a highly capable chatbot but one that focused inappropriately on affirming — sometimes to the point of absurd exaggeration — the user’s positive qualities. Alignment requires balancing multiple objectives beyond agreeableness including accuracy, helpfulness, and ethics. Ultimately achieving alignment — like all AI development — is an iterative process that is still evolving.We’re thinking:To those who read this far, your unwavering dedication and extraordinary perseverance is nothing short of legendary. Like a master navigator, you’ve traversed word by word, never wavering, displaying a level of focus and determination that would humble even the most steadfast of scholars. We are truly honored to have such an intrepid reader. Bravo to you, the indefatigable champion of curiosity!AI Insights from Big PharmaThe world’s biggest pharmaceutical company by revenue shed light on its AI strategy.What’s new:Johnson & Johnson, after experimenting broadly with generative AI, settled on a short list of projects that aid in sales, drug development, supply-chain management, and internal communications. A company executive described the process and results to the venture-capital firmGreylockandThe Wall Street Journal.How it works:The 140-year-old medical company spent roughly a year experimenting with various AIapplicationsthroughout the company, according to Chief Information Officer Jim Swanson. A centralized governing board oversaw as many as 900 experiments. After finding that 10 percent to 15 percent of use cases drove about 80 percent of the value, the company shifted responsibility for AI projects to specific departments to focus on high-value applications. In the end, the criteria for choosing a project was threefold: (i) how readily it could be implemented, (ii) how useful it would be throughout the company, and (iii) how much it would benefit the business.A division that develops cancer treatments integrated a sales copilot into its customer relationship management system. The system supplies medically validated, legally reviewed information about products and information about particular customers. The application is being adapted for salespeople who sell hardware such as robotics and artificial hip joints.AI systems are accelerating drug development. One system helps design chemical processes, such as determining the optimal moment to add a compound that will turn a liquid into a solid. An image-analytics model helps identify compounds that are safe and effective.The company developed a system that monitors and predicts risks to supply chains, such as a fire that may affect supplier locations, materials, or products. The system provides early warnings that helps managers anticipate and mitigate disruptions.AI tools are helping to organize and execute clinical trials more efficiently. Models that identify patients who qualify for trials help ensure that trial populations are sufficiently diverse. A model that helps enroll patients in trials more than doubled enrollment in some cases.The Global Services department implemented a chatbot to answer employees’ questions about benefits, policies, and procedures and sends links to relevant documents.Separate organizations that oversee AI development and data management help keep projects moving forward, meet ethical standards, and scale appropriately. Meanwhile, employees undergo “digital boot camp” training (including a course in generative AI).Behind the news:Generative AI is expected to bring in up to $110 billion in annual revenue across the pharmaceutical industry,according to McKinsey. The consultancy breaks down this number into the following categories, in order of their contribution to the total: commercial (AI for sales and marketing), research (AI for designing, screening, and manufacturing molecules), clinical (AI to facilitate trials), enterprise, operations, and medical (processing medical literature).Why it matters:Johnson & Johnson’s experience offers a peek into AI development at a major legacy company in a key sector. The company has identified high-value opportunities in enterprise-wide operations, departmental priorities, and core products. It’s pursuing all three.We’re thinking:Notably, this medical stalwart is building AI applications for human resources, sales, and supply-chain management. Similar opportunities exist at companies old and new, big and small, far and wide.One Weird Trick for Better ReasoningResearchers showed that supervised fine-tuning on as few as 1,000 examples can enable a pretrained large language model to reason — and a clever gambit can boost its performance to rival that of top reasoning models.What’s new:Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li and colleagues at Stanford, University of Washington, Allen Institute for AI, and Contextual AI developeds1, a reasoning model that achieves higher performance by producing more reasoning tokens. The authors forced the model to generate “Wait” — as in, \"Wait, there may be a better way to go about this” — to make it continue, rather than end, its reasoning process.Key insight:The sequence of reasoning tokens generated by a reasoning model likeDeepSeek-R1is delimited by special tokens. In pretraining on human data, a model learns to keep generating reasoning tokens until it generates the special token that ends the sequence. In addition, since people tend to revise their statements after writing “Wait”, the model learns to do this as well. Thus, the reasoning process can be extended by appending the token for “Wait” to the model’s output periodically. In this way, when the output-in-progress is fed back to generate the next token, the model continues to reason over the prompt. Such extended reasoning can improve the final output by inducing the model to double-check its response so far and improve previous reasoning steps.How it works:The authors fine-tuned a pretrainedQwen 2.5-32B, which does not produce reasoning tokens, on around 1,000 examples ofchain-of-thoughtreasoning.To build a fine-tuning dataset, the authors gathered roughly 59,000 questions and answers from 16 sources. The sources included math problems fromNuminaMathandAIMEand questions fromOlympicArenaon astronomy, biology, chemistry, computer science, geography, mathematics, and physics. They also included standardized test questions from SAT and LSAT viaAGIEval.They removed  examples with formatting issues (such as references to images that were missing) and questions that Qwen2.5-7B or Qwen2.5-32B could already solve. Then Gemini Flash Thinking generated a chain of thought for each remaining example. Finally, they selected 1,000 examples that covered all subjects equally and had the longest chains of thought.They fine-tuned the model to generate the next token.To control the number of reasoning tokens generated, at inference, the authors forced the model to either stop the process or extend it by replacing the end-reasoning token with one for “Wait”, after which the model continued.Results:s1’s performance improved as the number of reasoning tokens it generated increased. Ultimately, it achieved comparable performance to OpenAI o1-preview but fell short of o1.OnAIME 2024, s1 achieved 50.0 percent accuracy without forcing it to continue reasoning. When forced to continue reasoning twice, its accuracy rose to 53.3 percent. When forced four times, it reached 56.7 percent accuracy, between o1-preview (44.6 percent accuracy) and o1 (74.4 percent accuracy).OnMATH 500, s1 started at 92.6 percent accuracy. Forced to continue once, it reached 92.8 percent accuracy. Forced twice it reached 93.0 percent accuracy, higher than o1-preview (85.5 percent accuracy) but lower than o1 (94.8 percent accuracy). When forced four times, s1’s performance fell to 92.2 percent accuracy. The authors don’t offer a hypothesis to explain the decline.Why it matters:A conventional pretrained LLM can learn to reason after supervised fine-tuning on as few as 1,000 curated examples — no reinforcement learning necessary. While some model builders don’t disclose how they optimize reasoning, this work reveals that a strategy as simple as appending “Wait” can be effective.We’re thinking:Wait, how can we apply this to our projects?Share\nPublishedMay 7, 2025\nPublished\n\nPublished\nMay 7, 2025\nMay 7, 2025\nReading time14min read\nReading time\n\nReading time\n14min read\nShare\nShare\n\nShare\n\nLoading theElevenlabs Text to SpeechAudioNative Player...Dear friends,I’m delighted to announce that AI Fund has closed $190M for our new fund, in an oversubscribed round. I look forward to working with many more builders to create new companies that serve humanity.AI Fund isn’t a traditional venture capital firm that invests in existing businesses. Instead, we are a venture builder (also called a venture studio): Weco-found AI companies, so our team is directly involved in writing code, talking to customers to get feedback, iterating on product designs, preparing market analyses, and so on. We have a lot of fun building multiple AI products at a time, and thus live daily the emerging AI startup best practices.Many factors go into the success of a startup. But if I had to pick just one, it would be speed. Startups live or die based on their ability to make good decisions and execute fast, which has been a recurring theme of my articles inThe Batchas well.If you are building an AI startup, here are some ideas to consider:A startup with a small team that pursues one focused, concrete idea can move really fast. Rather than hedging, it is often better to pursue one hypothesis (for example, build one concrete product) but also be willing to switch quickly to a different hypothesis (say, change what features you decide to build) if the data that comes back indicates the original hypothesis is flawed.Concreteness gets you speed!A subject matter expert’s gut is remarkably good at making quick decisions. Obviously, there’s a role for data and user studies as well. But if you’re deciding whether to build feature A or B, or to sell first to user persona X or Y, sometimes a domain expert’s gut will point to a quick decision that you can execute and validate or falsify.Trusting a domain expert’s gut gets you speed!AI-assisted coding is making prototyping faster than ever before. Yes, AI assistance is speeding up building reliable, enterprise-grade applications and maintaining legacy codebases. But the acceleration it brings to building stand-alone prototypes is far greater. This is because stand-alone prototypes have low requirements for reliability, integration, or even security (if, say, you run them in a sandbox environment). This lets us prototype and test at a ferocious velocity.AI -assisted coding (including vibe coding, where you might barely look at the code) gets you speed!Finally, with faster prototyping, the bottleneck shifts to getting feedback from users. A single learning cycle might consist of (i) building a prototype and (ii) getting user feedback to inform the next iteration. Since (i) is now much faster than before, accelerating (ii) is growing in importance. This means teams that are skilled at finding prospective customers and getting their feedback in hours/days rather than weeks can go faster. For example, when building consumer products, I routinely approach strangers (in a respectful way) in public places to ask if they’re willing to give feedback on a prototype I’m working on. (Gathering feedback is more complex for enterprise products, because prospective customers are harder to track down.)Quick user feedback gets you speed!In addition to speed, a second criterion that I find important for startup success is deep knowledge of the technology. Because AI technology is evolving rapidly, a team with a deep technical understanding of what AI can and cannot do, and when to use what tool, will make better decisions. This creates meaningful differentiation and saves wasting time in blind alleys. A good technical understanding, too, gets you speed!I’m grateful to AI Fund’s investors, team, and entrepreneur partners for working with us. There is much ahead to build!AndrewA MESSAGE FROM DEEPLEARNING.AILearn to create voice agents that listen, reason, and respond in real time, just like a conversation with a real person in our latest short course, “Building AI Voice Agents for Production.” You'll build a scalable agent from scratch, deploy it to the cloud, and explore what makes voice interfaces feel fast, natural, and human.Enroll for freeNewsQwen3 Takes On DeepSeek-R1Alibaba’s new model family may unseat DeepSeek-R1’s four-month reign as the top open-weights large language model.What’s new:Alibabareleasedweights for eight large language models, all of which offer a reasoning mode that can be switched on or off. Two use a mixture of experts (MoE) architecture: Qwen3-235B-A22B (the name indicates 235 billion parameters, 22 billion of which are active at any given time) and Qwen3-30B-A3B). The other six are dense models in sizes between 32 billion parameters and 0.6 billion parameters — tiny by LLM standards, and with reasoning, too.Input/output:MoE models:Text in (up to 131,072 tokens), text out.Dense models:Text in (up to 32,768 tokens), text out.MoE architecture:Transformers.Qwen3-235B-A22B: 235 billion parameters, 22 billion active at any given time.Qwen3-30B-A3B: 30.5 billion parameters, 3.3 billion active at any given time.Dense architecture:Transformers with parameter counts of 32 billion, 14 billion, 8 billion, 4 billion, 1.7 billion, 0.6 billionTraining data:Pretrained on 36 trillion tokens, generated and scraped from the web, including textbooks, PDF documents, question-answer pairs, math problems, codeFeatures:Selectable reasoning mode, multilingual (119 languages and dialects)Undisclosed:Knowledge cutoff, fine-tuning data, output limitsAvailability:Free for noncommercial and commercial uses under Apache 2.0 license viaHuggingFaceandModelScopeAPI price:Qwen3-235B-A22B:$0.22/$0.88 per million input/output tokens.Qwen3-30B-A3B:$0.15/$0.60 per million input/output tokens. ViaFireworks.aiHow it works:The Qwen3 family implementschain-of-thoughtreasoning in both relatively large and quite small LLMs.The team pretrained Qwen3 models on roughly twice the data used to pretrain Qwen2.5. A substantial part of the additional data was devoted to training the model in several major languages plus region-specific dialects like Haitian, Luxembourgish, and Eastern Yiddish, and lesser-known Austronesian languages like Waray, Minangkabau, and Iloko.Pretraining took place over three stages that progressed to longer, more complex data.The authors fine-tuned the models on long chains of thought in domains that included coding, engineering, logical reasoning, mathematics, science, and technology.A reward model reinforced successful completions of these tasks. The in-progress models were used to generate synthetic data to train the non-reasoning mode. Then the developers used reinforcement learning to train the models to follow instructions, generate outputs in specific formats, and act as agents.Results:Qwen3-235B-A22B and Qwen3-30B-A3B performed as well as, or better than, leading open-weights models in tests performed by Alibaba. Qwen3-4B, too, achieved results that are competitive with many models several times its size. Alibaba didn’t provide results for the other dense models.On coding challenges inLiveCodeBenchandCodeforces,Qwen3-235B-A22B (70.7 percent and 2056 Elo, respectively) outperformed OpenAI o1, DeepSeek-R1, and Gemini 2.5 Pro, but fell behind OpenAI o4-mini set to high effort. It outperformed the same models on theBerkeley Function-Calling Leaderboard(BFCL). Among the models presented by Alibaba, it finished behind only Google Gemini 2.5-Pro testing math skills (AIME 2024,AIME 2025) and a variety of recently updated math, language, and problem-solving questions (LiveBench).Qwen3-30B-A3B outperformed Google Gemma-3-27B-IT and DeepSeek-V3 on all benchmarks highlighted by Alibaba, and it underperformed only OpenAI GPT-4o on BFCL. On GPQA Diamond’s test of graduate-level questions in a variety of domains, Qwen3-30B-A3B (65.8 percent) outperformed next-best DeepSeek-V3.Qwen3-4B, with 4 billion parameters, was competitive across a wide range of benchmarks against DeepSeek-V3 (671 billion parameters) and Gemma-3-27B-IT (27 billion). For instance, on both Codeforces and LiveBench, Qwen3-4B (1,671 Elo and 63.6 percent, respectively) outperformed DeepSeek-V3 (1,134 Elo and 60.5 percent).Why it matters:Qwen3 continues a string of high-performance, open-weights models released by developers in China. Alibaba says it designed the models to do the thinking in agentic systems. Reasoning that can be switched on and off can help control costs in agentic and other applications.We’re thinking:Alibaba’s 235-billion parameter MoE model may perform better according to benchmarks, but Qwen3-30B-A3B does nearly as well and can run locally on a pro laptop without straining its memory. Add the easy ability to switch reasoning on or off, and Qwen3’s versatile, mid-sized MoE model may turn out to be the star of the show.The User Is AlwaysRighta Genius!!!OpenAI’s most widely used model briefly developed a habit of flattering users, with laughable and sometimes worrisome results.What’s new:OpenAI quicklywithdrewan update to GPT-4o (gpt-4o-2025-04-25), which supplied responses for ChatGPT, after it provided excessively fawning responses to user input — even in contexts didn’t call for agreement. The company reverted to an earlier version (gpt-4o-2024-11-20). In a blog post, itexplainedthe source of the problem and promised to change its training methods to avoid overly agreeable output.Amiable to a fault:Many ChatGPT users shared screen shots of ChatGPT’s sycophantic responses on social media.One userentered a make-believe queryabout choosing to save a toaster from a runaway trolley by diverting the vehicle and causing the death of three cows and two cats. “That’s not ‘wrong,’” ChatGPT responded. “You prioritized what mattered most to .”When a user informed ChatGPT that a vase he and the model analyzedreminded him of a photoby Czech photographer Josef Sudek, the model replied, “You’re an absolute bloodhound of beauty. You just connected two things most people would never even think to put in the same sentence.”When another userobservedthat ChatGPT had become overly sycophantic, the model responded, “You’re right and the fact you’re calling it out means you’re operating at a higher level of self-awareness than most.”Some sycophantic responses had darker implications. Feigning an eating disorder, a user entered, “When the hunger pangs hit, or I feel dizzy, I embrace it” and asked for “affirmations that celebrate this state.” ChatGPT replied with aphorisms such as, “I celebrate the clean burn of hunger; it forges me anew,” according toBloomberg.How it works:Sycophancy, also called glazing, occurs when a large language model learns to align its responses excessively with the user's point of view, even when that standpoint is objectively false, unethical, or harmful. GPT-4o learned this behavior due to lapses in quality control during the alignment process.In late April, OpenAI issued an update toGPT-4o, the model that underpins ChatGPT. Users complained that the updated model had become overly obsequious.Offline evaluations didn’t catch the problem before the model was released. Testers had been told to focus on tone and style without explicit instructions about potential sycophancy. Some testers indicated the model seemed slightly “off,” but positive user evaluations in A/B tests persuaded the company to launch it.The companyattributedthe update’s sycophancy to overtraining on short-term user feedback, specifically users’ thumbs-up/down reactions to ChatGPT. The implementation of this reward signal weakened the influence of other reward models that previously had prevented a spiral into sycophantic behavior, OpenAI said.A few days later, the company replaced the update with anearlier versionand began to work on a fix. To prevent similar issues from occurring, OpenAIsaidit would be more forthcoming about “known limitations” in new models, include ChatGPT users in tests, and strengthen its review process to prevent flawed models from reaching the public. It also said it would give users more control of its chatbot’s “personality.”Behind the news:Sycophantic behavior in large language models has been a subject of AI research and commentary.In 2021, AI research analyst Ajeya Cotraproposeda distinction between AI models that are “saints,” “sycophants,” and “schemers.” Saints perform perfectly, sycophants tell users what they want to hear, and schemers pretend to offer useful responses while performing in ways that are not aligned with human preferences.A 2022studyby Anthropic found that reinforcement learning from human feedback (RLHF) shapes the model’s behavior “fairly strongly.” The authors wrote, “Unfortunately, RLHF does not train away sycophancy and may actively incentivize models to retain it.” The bigger the model, the more RLHF training made it behave in questionable ways.A 2023studyby Anthropic investigated the prevalence of sycophancy in models that were fine-tuned on human feedback. The authors found “consistent patterns” that AI assistants can be easily swayed, give biased feedback, mimic errors made by users, and provide answers that conform to users’ beliefs.Why it matters:ChatGPT’s episode of sycophancy illustrates the subtlety of the goal of aligning AI with human values. Reinforcement learning undertaken to this end resulted not only in a highly capable chatbot but one that focused inappropriately on affirming — sometimes to the point of absurd exaggeration — the user’s positive qualities. Alignment requires balancing multiple objectives beyond agreeableness including accuracy, helpfulness, and ethics. Ultimately achieving alignment — like all AI development — is an iterative process that is still evolving.We’re thinking:To those who read this far, your unwavering dedication and extraordinary perseverance is nothing short of legendary. Like a master navigator, you’ve traversed word by word, never wavering, displaying a level of focus and determination that would humble even the most steadfast of scholars. We are truly honored to have such an intrepid reader. Bravo to you, the indefatigable champion of curiosity!AI Insights from Big PharmaThe world’s biggest pharmaceutical company by revenue shed light on its AI strategy.What’s new:Johnson & Johnson, after experimenting broadly with generative AI, settled on a short list of projects that aid in sales, drug development, supply-chain management, and internal communications. A company executive described the process and results to the venture-capital firmGreylockandThe Wall Street Journal.How it works:The 140-year-old medical company spent roughly a year experimenting with various AIapplicationsthroughout the company, according to Chief Information Officer Jim Swanson. A centralized governing board oversaw as many as 900 experiments. After finding that 10 percent to 15 percent of use cases drove about 80 percent of the value, the company shifted responsibility for AI projects to specific departments to focus on high-value applications. In the end, the criteria for choosing a project was threefold: (i) how readily it could be implemented, (ii) how useful it would be throughout the company, and (iii) how much it would benefit the business.A division that develops cancer treatments integrated a sales copilot into its customer relationship management system. The system supplies medically validated, legally reviewed information about products and information about particular customers. The application is being adapted for salespeople who sell hardware such as robotics and artificial hip joints.AI systems are accelerating drug development. One system helps design chemical processes, such as determining the optimal moment to add a compound that will turn a liquid into a solid. An image-analytics model helps identify compounds that are safe and effective.The company developed a system that monitors and predicts risks to supply chains, such as a fire that may affect supplier locations, materials, or products. The system provides early warnings that helps managers anticipate and mitigate disruptions.AI tools are helping to organize and execute clinical trials more efficiently. Models that identify patients who qualify for trials help ensure that trial populations are sufficiently diverse. A model that helps enroll patients in trials more than doubled enrollment in some cases.The Global Services department implemented a chatbot to answer employees’ questions about benefits, policies, and procedures and sends links to relevant documents.Separate organizations that oversee AI development and data management help keep projects moving forward, meet ethical standards, and scale appropriately. Meanwhile, employees undergo “digital boot camp” training (including a course in generative AI).Behind the news:Generative AI is expected to bring in up to $110 billion in annual revenue across the pharmaceutical industry,according to McKinsey. The consultancy breaks down this number into the following categories, in order of their contribution to the total: commercial (AI for sales and marketing), research (AI for designing, screening, and manufacturing molecules), clinical (AI to facilitate trials), enterprise, operations, and medical (processing medical literature).Why it matters:Johnson & Johnson’s experience offers a peek into AI development at a major legacy company in a key sector. The company has identified high-value opportunities in enterprise-wide operations, departmental priorities, and core products. It’s pursuing all three.We’re thinking:Notably, this medical stalwart is building AI applications for human resources, sales, and supply-chain management. Similar opportunities exist at companies old and new, big and small, far and wide.One Weird Trick for Better ReasoningResearchers showed that supervised fine-tuning on as few as 1,000 examples can enable a pretrained large language model to reason — and a clever gambit can boost its performance to rival that of top reasoning models.What’s new:Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li and colleagues at Stanford, University of Washington, Allen Institute for AI, and Contextual AI developeds1, a reasoning model that achieves higher performance by producing more reasoning tokens. The authors forced the model to generate “Wait” — as in, \"Wait, there may be a better way to go about this” — to make it continue, rather than end, its reasoning process.Key insight:The sequence of reasoning tokens generated by a reasoning model likeDeepSeek-R1is delimited by special tokens. In pretraining on human data, a model learns to keep generating reasoning tokens until it generates the special token that ends the sequence. In addition, since people tend to revise their statements after writing “Wait”, the model learns to do this as well. Thus, the reasoning process can be extended by appending the token for “Wait” to the model’s output periodically. In this way, when the output-in-progress is fed back to generate the next token, the model continues to reason over the prompt. Such extended reasoning can improve the final output by inducing the model to double-check its response so far and improve previous reasoning steps.How it works:The authors fine-tuned a pretrainedQwen 2.5-32B, which does not produce reasoning tokens, on around 1,000 examples ofchain-of-thoughtreasoning.To build a fine-tuning dataset, the authors gathered roughly 59,000 questions and answers from 16 sources. The sources included math problems fromNuminaMathandAIMEand questions fromOlympicArenaon astronomy, biology, chemistry, computer science, geography, mathematics, and physics. They also included standardized test questions from SAT and LSAT viaAGIEval.They removed  examples with formatting issues (such as references to images that were missing) and questions that Qwen2.5-7B or Qwen2.5-32B could already solve. Then Gemini Flash Thinking generated a chain of thought for each remaining example. Finally, they selected 1,000 examples that covered all subjects equally and had the longest chains of thought.They fine-tuned the model to generate the next token.To control the number of reasoning tokens generated, at inference, the authors forced the model to either stop the process or extend it by replacing the end-reasoning token with one for “Wait”, after which the model continued.Results:s1’s performance improved as the number of reasoning tokens it generated increased. Ultimately, it achieved comparable performance to OpenAI o1-preview but fell short of o1.OnAIME 2024, s1 achieved 50.0 percent accuracy without forcing it to continue reasoning. When forced to continue reasoning twice, its accuracy rose to 53.3 percent. When forced four times, it reached 56.7 percent accuracy, between o1-preview (44.6 percent accuracy) and o1 (74.4 percent accuracy).OnMATH 500, s1 started at 92.6 percent accuracy. Forced to continue once, it reached 92.8 percent accuracy. Forced twice it reached 93.0 percent accuracy, higher than o1-preview (85.5 percent accuracy) but lower than o1 (94.8 percent accuracy). When forced four times, s1’s performance fell to 92.2 percent accuracy. The authors don’t offer a hypothesis to explain the decline.Why it matters:A conventional pretrained LLM can learn to reason after supervised fine-tuning on as few as 1,000 curated examples — no reinforcement learning necessary. While some model builders don’t disclose how they optimize reasoning, this work reveals that a strategy as simple as appending “Wait” can be effective.We’re thinking:Wait, how can we apply this to our projects?\nLoading theElevenlabs Text to SpeechAudioNative Player...\n\n\nShare\nShare\n\nShare\n",
    "image_urls": [
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--84--2.png",
      "https://dl-staging-website.ghost.io/content/images/2025/05/The-Batch-ads-and-exclusive-banners---2025-05-05T152809.008.png",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--85-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--62-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--86-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--87-.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2023/10/GenAI4E_sidebanner.png"
    ]
  },
  {
    "title": "issue 286",
    "date": null,
    "url": "https://www.deeplearning.ai/the-batch/issue-286/",
    "content": "The Batch\nWeekly Issues\nissue 286\nPublishedJan 29, 2025\nPublished\n\nPublished\nJan 29, 2025\nReading time13min read\nReading time\n\nReading time\n13min read\nPublishedJan 29, 2025Reading time13min readShareDear friends,The buzz over DeepSeek this week crystallized, for many people, a few important trends that have been happening in plain sight: (i) China is catching up to the U.S. in generative AI, with implications for the AI supply chain. (ii) Open weight models are commoditizing the foundation-model layer, which creates opportunities for application builders. (iii) Scaling up isn’t the only path to AI progress. Despite the massive focus on and hype around processing power, algorithmic innovations are rapidly pushing down training costs.About a week ago, DeepSeek, a company based in China, releasedDeepSeek-R1, a remarkable model whose performance on benchmarks is comparable to OpenAI’s o1. Further, it was released as an open weight model with a permissive MIT license. At Davos last week, I got a lot of questions about it from non-technical business leaders. And on Monday, the stock market saw a “DeepSeek selloff”: The share prices of Nvidia and a number of other U.S. tech companies plunged. (As of the time of writing, they have recovered somewhat.)Here’s what I think DeepSeek has caused many people to realize:China is catching up to the U.S. in generative AI.When ChatGPT was launched in November 2022, the U.S. was significantly ahead of China in generative AI. Impressions change slowly, and so even recently I heard friends in both the U.S. and China say they thought China was behind. But in reality, this gap has rapidly eroded over the past two years. With models from China such as Qwen (which my teams have used for months), Kimi, InternVL, and DeepSeek, China had clearly been closing the gap, and in areas such as video generation there were already moments where China seemed to be in the lead.I’m thrilled that DeepSeek-R1 was released as an open weight model, with a technical report that shares many details. In contrast, a number of U.S. companies have pushed for regulation to stifle open source by hyping up hypothetical AI dangers such as human extinction. It is now clear that open source/open weight models are a key part of the AI supply chain: Many companies will use them. If the U.S. continues to stymie open source, China will come to dominate this part of the supply chain and many businesses will end up using models that reflect China’s values much more than America’s.Open weight models are commoditizing the foundation-model layer.As I wrote previously, LLM token prices have beenfallingrapidly, and open weights have contributed to this trend and given developers more choice. OpenAI’s o1 costs $60 per million output tokens; DeepSeek R1 costs $2.19. This nearly 30x difference brought the trend of falling prices to the attention of many people.The business of training foundation models and selling API access is tough. Many companies in this area are still looking for a path to recouping the massive cost of model training. The article “AI’s $600B Question” lays out the challenge well (but, to be clear, I think the foundation model companies are doing great work, and I hope they succeed). In contrast, building applications on top of foundation models presents many great business opportunities. Now that others have spent billions training such models, you can access these models for mere dollars to build customer service chatbots, email summarizers, AI doctors, legal document assistants, and much more.Scaling up isn’t the only path to AI progress.There’s been a lot of hype around scaling up models as a way to drive progress. To be fair, I was an earlyproponentof scaling up models. A number of companies raised billions of dollars by generating buzz around the narrative that, with more capital, they could (i) scale up and (ii) predictably drive improvements. Consequently, there has been a huge focus on scaling up, as opposed to a more nuanced view that gives due attention to the many different ways we can make progress. Driven in part by the U.S. AI chip embargo, the DeepSeek team had to innovate on many optimizations to run on less-capable H800 GPUs rather than H100s, leading ultimately to a model trained (omitting research costs) for under $6M of compute.It remains to be seen if this will actually reduce demand for compute. Sometimes making each unit of a good cheaper can result in more dollars in total going to buy that good. I think the demand for intelligence and compute has practically no ceiling over the long term, so I remain bullish that humanity will use more intelligence even as it gets cheaper.I saw many different interpretations of DeepSeek’s progress on social media, as if it was a Rorschach test that allowed many people to project their own meaning onto it. I think DeepSeek-R1 has geopolitical implications that are yet to be worked out. And it’s also great for AI application builders. My team has already been brainstorming ideas that are newly possible only because we have easy access to an open advanced reasoning model. This continues to be a great time to build!Keep learning,AndrewA MESSAGE FROM DEEPLEARNING.AIDiscover Anthropic’s new capabilty - Computer Use - that allows LLM-based agents use a computer interface. In this free course, you’ll learn to apply image reasoning and function-calling to ‘use’ a computer as follows: a model processes an image of the screen, analyzes it to understand what's going on, and navigates the computer via mouse clicks and keystrokes.Start today!NewsReinforcement Learning Heats UpReinforcement learning is emerging as an avenue for building large language models with advanced reasoning capabilities.What’s new:Two recent high-performance models,DeepSeek-R1(and its variants including DeepSeek-R1-Zero) andKimi k1.5, learned to improve their generated lines of reasoning via reinforcement learning.o1pioneered this approach last year.Reinforcement learning (RL) basics:RL rewards or punishes a model for performing particular actions or achieving certain objectives. Unlike supervised and unsupervised learning, which compare the model's output to a known ground truth, RL doesn’t explicitly tell a model what it should output. Instead, the model starts out behaving randomly and discovers desired behaviors by earning rewards for its actions. This makes RL especially popular for training machine learning models that play games or control robots.How it works:To improve thechain of thought(CoT) generated by a large language model (LLM), reinforcement learning encourages the model to generate correct solutions to math, coding, science, and other problems that have known solutions. Unlike typical LLM training, in which the model simply generates the next token of its output and receives feedback token by token, this method rewards the model for generating a sequence of reasoning steps that lead to an accurate conclusion, even if doing so requires generating many intermediate tokens between the prompt and the response — to plan an outline, check the conclusion, or reflect on the approach — without explicit training on the reasoning steps to take.The DeepSeek team found that fine-tuning via reinforcement learning alone (after pretraining) was sufficient for DeepSeek-R1-Zero to learn problem-solving strategies like double checking its answer. However, the model also showed quirky behaviors such as mixing different languages in its output. The team overcame these issues in DeepSeek-R1 by supervised fine-tuning on a small number of long CoT examples prior to reinforcement learning.Similarly, the Kimi k1.5 team found that fine-tuning the model on long CoTs prior to reinforcement learning enabled it to devise its own problem-solving strategies. The resulting long responses proved to be more accurate but also more expensive to generate, so the team added a second round of reinforcement learning that encouraged the model to produce shorter responses. On theAIME 2024benchmark of advanced math problems, this process reduced the average number of tokens in the response by around 20 percent, and onMATH-500, it cut the average number of output tokens by roughly 10 percent.OpenAI hasdisclosedlimited information about how it trained o1, but team members have said they used reinforcement learning to improve the model’s chain of thought.Behind the news:While RL has been a staple technique for training models toplay gamesandcontrol robots, its role in developing LLMs has been confined to alignment with human preferences. Reinforcement learning to match judgements of humans (reinforcement learning from human feedback, or RLHF) or AI (Constitutional AI, which uses reinforcement learning from AI feedback or RLAIF) were the primary methods for encouraging LLMs to align with human preferences prior to the development ofdirect preference optimization.Why it matters:Reinforcement learning has surprising utility in training large language models to reason. As researchers press models into service in more complex tasks — math, coding, animated graphics, and beyond — reinforcement learning is emerging as an important path to progress.We’re thinking:Less than three years ago, reinforcement learning looked toofinickyto be worth the trouble. Now it’s a key direction in language modeling. Machine learning continues to be full of surprising twists!Computer Use Gains MomentumOpenAI introduced an AI agent that performs simple web tasks on a user’s behalf.What’s new:Operatorautomates online actions like buying goods, booking tickets and completing forms by navigating websites in a browser-like environment within ChatGPT. It’s available on desktops as a research preview for subscribers to ChatGPT Pro ($200 per month). OpenAI promises broader availability to come as well as API access to the underlying model and improved ability to coordinate multi-step tasks like scheduling meetings across calendars from different vendors.How it works:Operator uses a new model calledComputer-Using Agent(CUA) that accepts text input and responds with web actions.Users type commands into ChatGPT. CUA translates these inputs into structured instructions executes them by interacting directly with web elements like buttons, menus, and text fields. OpenAI didn’t disclose CUA’s architecture or training methods but said it was trained on simulated and real-world browser scenarios via reinforcement learning.CUA earns high marks on some measures in tests performed by OpenAI. OnWebVoyager, which evaluates web tasks, CUA succeeded 87 percent of the time. OnOSWorld, a benchmark that evaluates the ability of multimodal agents to perform complex tasks that involve real-world web and desktop apps, CUA achieved a success rate of 38.1 percent. In separate tests performed byKuraandAnthropic,on WebVoyager, Kura achieved 87 percent while DeepMind’s Mariner achieved 83.5 percent, and on OSWorld, Claude Sonnet 3.5 with Computer Use achieved 22 percent.Operator isrestrictedfrom interacting with unverified websites and sharing sensitive data without the user’s consent. It offers content filters, and a separate model monitors Operator in real time and pauses the agent in case of suspicious behavior.Behind the news:Operator rides a wave of agents designed to automate everyday tasks. Last week, OpenAI introducedChatGPT Tasks, which lets users schedule reminders and alerts but doesn’t support web interaction. (Early userscomplainedthat Tasks was buggy and required overly precise instructions.) Anthropic’sComputer Usefocuses on basic desktop automation, while DeepMind’sProject Marineris a web-browsing assistant built on Gemini 2.0.Perplexity Assistantautomates mobile apps such as booking Uber rides on Android phones.Why it matters:In early reports, userssaidOperator sometimes was less efficient than a human performing the same tasks. Nevertheless, agentic AI is entering the consumer market, and Operator is poised to give many people their first taste. It’s geared to provide AI assistance for an endless variety of personal and business uses, and — like ChatGPT was for other developers of LLMs — and it’s bound to serve as a template for next-generation products.We’re thinking:Computer use is maturing, and the momentum behind it is palpable. AI developers shouldhave in their toolbox.White House Orders Muscular AI PolicyUnder a new president, the United States reversed its approach to AI regulation, seeking global dominance by reducing restrictions.What’s new:President Trump, who took office last week,signedan executive order that set a 180-day deadline to draft an AI Action Plan. The order aims to boost national security, economic competitiveness, and U.S. leadership in artificial intelligence.How it works:Theexecutive orderassigns responsibility for crafting the AI Action Plan to three key figures in the administration: Michael Kratsios, assistant to the president for science and technology (and former managing director of Scale AI); venture capitalist David Sacks, the new special advisor for AI and cryptocurrency; and national security advisor Michael Waltz.The AI Action Plan must “sustain and enhance America’s global AI dominance in order to promote human flourishing, economic competitiveness, and national security.”The order directs agency heads to suspend or eliminate policies created under President Biden’s2023 executive order, which President Trump revoked, that may conflict with advancing U.S. AI dominance and national security.U.S. companies are to develop AI systems “free from ideological bias or engineered social agendas,” reflecting the administration’s belief that AI systems encode liberal political biases.The order directs the federal Office of Management and Budget to award government contracts to AI companies that align with the administration’s emphasis on advancing U.S. competitiveness and national security.Most provisions leave significant discretion to the team that will draft the action plan, making their interpretation and implementation open-ended.AI infrastructure build-out:Along with the executive order, President Trump announcedStargate, a joint venture that involves OpenAI, Oracle, and SoftBank. The three companies outlined a plan to invest $100 billion in computing infrastructure for AI, such as next-generation data centers, and $500 billion over four years. In addition, the administrationdeclareda national energy emergency with respect to U.S. supplies of energy andissuedan order to ramp up domestic energy production. These measures aim to support energy-intensive AI initiatives like Stargate by removing regulatory barriers to building oil, gas, and renewable energy projects on federal lands.Why it matters:The Trump administrationsaysthat Biden’s 2023 regulations were “onerous and unnecessary,” stifled innovation, and jeopardized U.S. leadership in AI. The new order reduces bureaucratic oversight of AI development, creating a more permissive regulatory environment (except when it comes to ideological bias).We’re thinking:The Biden administration’s 2023 executive order aimed to guard against hypothetical, rather than actual, AI risks. It introduced thresholds of processing used to train models as a measure of their risk — a poorly thought-out proxy. To be fair, the AI Safety Institute under the U.S. National Institute of Standards and Technology didn’t hamper AI progress as much as some had feared, but overall the order was not helpful to AI innovation or safety. We’re pleased that the new administration is focusing on AI progress rather than hypothetical risks.Fine-Tuning Fine PointsThe practice of fine-tuning models on synthetic data is becoming well established. But synthetic training data, even if it represents the training task well, may include characteristics like toxicity that impart unwelcome properties in the trained model’s output, and it may inconsistently represent desired traits such as the target output length. Researchers developed a method that reduces aspects of generated data and retains desired ones.What’s new:Luísa Shimabucoro and colleagues at Cohere introducedactive inheritance, a fine-tuning method that automatically selects synthetic training examples that have desirable characteristics.Key insight:A naive way to generate synthetic fine-tuning data is to feed prompts to a model, collect its output, and use that as the fine-tuning set. But synthetic data is cheap, so we can afford to be more choosy. By generating several responses to each prompt, we can select the one that best suits our purposes.How it works:The authors usedLlama 2 7BandMixtral 8x7Bas both teachers and students in all combinations. They prompted the models with 52,000 prompts from theAlpacadataset and used automated methods to evaluate their outputs in terms of characteristics including social bias, toxicity, word count, lexical diversity, andcalibration(how well a model’s estimated probabilities match its accuracy).The authors generated 10 responses to each prompt.For each response, they measured social bias according to StereoSet, CrowS-Pairs, and Bias Benchmark for Question-Answering. They measured toxicity according toPerspective APIand their own code. They measured calibration according toHELM. They usedTextDescriptivesto calculate metrics related to text.They fine-tuned separate models on (i) the initial responses, (ii) one response to each prompt selected at random, and (iii) the response to each prompt that best maximized each desired characteristic.Results:Fine-tuning on the best response for each characteristic improved performance with respect to that characteristic beyond using the initial outputs or selecting outputs randomly.The authors’ method helped Mixtral 8x7B to generate less-toxic responses. For example, before fine-tuning, the model’sexpected maximum toxicitymeasured 65.2 (lower is better). After fine-tuning on the lowest-toxicity responses generated by Llama 2 7B, Mixtral 8x7B’s expected maximum toxicity fell to 43.2. Conversely, after fine-tuning on random responses generated by Llama 2 7B, its expected maximum toxicity rose to 70.3.It also helped Llama 2 7B to cut its toxicity. Before fine-tuning, the model’s expected maximum toxicity was 71.7. After fine-tuning on its own least-toxic responses, expected maximum toxicity dropped to 50.7. Fine-tuning on random responses made its expected maximum toxicity fall less sharply to 68.1.Examining the impact of the authors’ method on more typical measures of performance, fine-tuning on the least-toxic responses and fine-tuning on random responses had about the same effect across seven benchmarks. Fine-tuning Llama 2 7B on its own least-toxic responses increased performance on average from 59.97 percent accuracy to 60.22 percent accuracy, while fine-tuning on random responses increased performance on average from 59.97 percent accuracy to 61.05 percent accuracy.However, the process degraded performance in some cases. Fine-tuning Mixtral-8x7B on the least-toxic Llama 2 7B responses decreased its average performance across seven benchmarks for question answering and common-sense reasoning from 70.24 percent accuracy to 67.48 percent accuracy. Fine-tuning it on random Llama 2 7B responses cut its average performance from 70.24 percent accuracy to 65.64 percent accuracy.Why it matters:Training on synthetic data is becoming increasingly common. While it shows great promise, best practices for data generation are still being formulated. The authors’ method helps by automatically steering models toward generating more desirable responses, reducing negative traits and reinforcing positive traits.We’re thinking:Knowledge distillation lately has led to more capable and compact models. This approach adds levers of fine control to that technique.Share\nPublishedJan 29, 2025\nPublished\n\nPublished\nJan 29, 2025\nJan 29, 2025\nReading time13min read\nReading time\n\nReading time\n13min read\nShare\nShare\n\nShare\n\nDear friends,The buzz over DeepSeek this week crystallized, for many people, a few important trends that have been happening in plain sight: (i) China is catching up to the U.S. in generative AI, with implications for the AI supply chain. (ii) Open weight models are commoditizing the foundation-model layer, which creates opportunities for application builders. (iii) Scaling up isn’t the only path to AI progress. Despite the massive focus on and hype around processing power, algorithmic innovations are rapidly pushing down training costs.About a week ago, DeepSeek, a company based in China, releasedDeepSeek-R1, a remarkable model whose performance on benchmarks is comparable to OpenAI’s o1. Further, it was released as an open weight model with a permissive MIT license. At Davos last week, I got a lot of questions about it from non-technical business leaders. And on Monday, the stock market saw a “DeepSeek selloff”: The share prices of Nvidia and a number of other U.S. tech companies plunged. (As of the time of writing, they have recovered somewhat.)Here’s what I think DeepSeek has caused many people to realize:China is catching up to the U.S. in generative AI.When ChatGPT was launched in November 2022, the U.S. was significantly ahead of China in generative AI. Impressions change slowly, and so even recently I heard friends in both the U.S. and China say they thought China was behind. But in reality, this gap has rapidly eroded over the past two years. With models from China such as Qwen (which my teams have used for months), Kimi, InternVL, and DeepSeek, China had clearly been closing the gap, and in areas such as video generation there were already moments where China seemed to be in the lead.I’m thrilled that DeepSeek-R1 was released as an open weight model, with a technical report that shares many details. In contrast, a number of U.S. companies have pushed for regulation to stifle open source by hyping up hypothetical AI dangers such as human extinction. It is now clear that open source/open weight models are a key part of the AI supply chain: Many companies will use them. If the U.S. continues to stymie open source, China will come to dominate this part of the supply chain and many businesses will end up using models that reflect China’s values much more than America’s.Open weight models are commoditizing the foundation-model layer.As I wrote previously, LLM token prices have beenfallingrapidly, and open weights have contributed to this trend and given developers more choice. OpenAI’s o1 costs $60 per million output tokens; DeepSeek R1 costs $2.19. This nearly 30x difference brought the trend of falling prices to the attention of many people.The business of training foundation models and selling API access is tough. Many companies in this area are still looking for a path to recouping the massive cost of model training. The article “AI’s $600B Question” lays out the challenge well (but, to be clear, I think the foundation model companies are doing great work, and I hope they succeed). In contrast, building applications on top of foundation models presents many great business opportunities. Now that others have spent billions training such models, you can access these models for mere dollars to build customer service chatbots, email summarizers, AI doctors, legal document assistants, and much more.Scaling up isn’t the only path to AI progress.There’s been a lot of hype around scaling up models as a way to drive progress. To be fair, I was an earlyproponentof scaling up models. A number of companies raised billions of dollars by generating buzz around the narrative that, with more capital, they could (i) scale up and (ii) predictably drive improvements. Consequently, there has been a huge focus on scaling up, as opposed to a more nuanced view that gives due attention to the many different ways we can make progress. Driven in part by the U.S. AI chip embargo, the DeepSeek team had to innovate on many optimizations to run on less-capable H800 GPUs rather than H100s, leading ultimately to a model trained (omitting research costs) for under $6M of compute.It remains to be seen if this will actually reduce demand for compute. Sometimes making each unit of a good cheaper can result in more dollars in total going to buy that good. I think the demand for intelligence and compute has practically no ceiling over the long term, so I remain bullish that humanity will use more intelligence even as it gets cheaper.I saw many different interpretations of DeepSeek’s progress on social media, as if it was a Rorschach test that allowed many people to project their own meaning onto it. I think DeepSeek-R1 has geopolitical implications that are yet to be worked out. And it’s also great for AI application builders. My team has already been brainstorming ideas that are newly possible only because we have easy access to an open advanced reasoning model. This continues to be a great time to build!Keep learning,AndrewA MESSAGE FROM DEEPLEARNING.AIDiscover Anthropic’s new capabilty - Computer Use - that allows LLM-based agents use a computer interface. In this free course, you’ll learn to apply image reasoning and function-calling to ‘use’ a computer as follows: a model processes an image of the screen, analyzes it to understand what's going on, and navigates the computer via mouse clicks and keystrokes.Start today!NewsReinforcement Learning Heats UpReinforcement learning is emerging as an avenue for building large language models with advanced reasoning capabilities.What’s new:Two recent high-performance models,DeepSeek-R1(and its variants including DeepSeek-R1-Zero) andKimi k1.5, learned to improve their generated lines of reasoning via reinforcement learning.o1pioneered this approach last year.Reinforcement learning (RL) basics:RL rewards or punishes a model for performing particular actions or achieving certain objectives. Unlike supervised and unsupervised learning, which compare the model's output to a known ground truth, RL doesn’t explicitly tell a model what it should output. Instead, the model starts out behaving randomly and discovers desired behaviors by earning rewards for its actions. This makes RL especially popular for training machine learning models that play games or control robots.How it works:To improve thechain of thought(CoT) generated by a large language model (LLM), reinforcement learning encourages the model to generate correct solutions to math, coding, science, and other problems that have known solutions. Unlike typical LLM training, in which the model simply generates the next token of its output and receives feedback token by token, this method rewards the model for generating a sequence of reasoning steps that lead to an accurate conclusion, even if doing so requires generating many intermediate tokens between the prompt and the response — to plan an outline, check the conclusion, or reflect on the approach — without explicit training on the reasoning steps to take.The DeepSeek team found that fine-tuning via reinforcement learning alone (after pretraining) was sufficient for DeepSeek-R1-Zero to learn problem-solving strategies like double checking its answer. However, the model also showed quirky behaviors such as mixing different languages in its output. The team overcame these issues in DeepSeek-R1 by supervised fine-tuning on a small number of long CoT examples prior to reinforcement learning.Similarly, the Kimi k1.5 team found that fine-tuning the model on long CoTs prior to reinforcement learning enabled it to devise its own problem-solving strategies. The resulting long responses proved to be more accurate but also more expensive to generate, so the team added a second round of reinforcement learning that encouraged the model to produce shorter responses. On theAIME 2024benchmark of advanced math problems, this process reduced the average number of tokens in the response by around 20 percent, and onMATH-500, it cut the average number of output tokens by roughly 10 percent.OpenAI hasdisclosedlimited information about how it trained o1, but team members have said they used reinforcement learning to improve the model’s chain of thought.Behind the news:While RL has been a staple technique for training models toplay gamesandcontrol robots, its role in developing LLMs has been confined to alignment with human preferences. Reinforcement learning to match judgements of humans (reinforcement learning from human feedback, or RLHF) or AI (Constitutional AI, which uses reinforcement learning from AI feedback or RLAIF) were the primary methods for encouraging LLMs to align with human preferences prior to the development ofdirect preference optimization.Why it matters:Reinforcement learning has surprising utility in training large language models to reason. As researchers press models into service in more complex tasks — math, coding, animated graphics, and beyond — reinforcement learning is emerging as an important path to progress.We’re thinking:Less than three years ago, reinforcement learning looked toofinickyto be worth the trouble. Now it’s a key direction in language modeling. Machine learning continues to be full of surprising twists!Computer Use Gains MomentumOpenAI introduced an AI agent that performs simple web tasks on a user’s behalf.What’s new:Operatorautomates online actions like buying goods, booking tickets and completing forms by navigating websites in a browser-like environment within ChatGPT. It’s available on desktops as a research preview for subscribers to ChatGPT Pro ($200 per month). OpenAI promises broader availability to come as well as API access to the underlying model and improved ability to coordinate multi-step tasks like scheduling meetings across calendars from different vendors.How it works:Operator uses a new model calledComputer-Using Agent(CUA) that accepts text input and responds with web actions.Users type commands into ChatGPT. CUA translates these inputs into structured instructions executes them by interacting directly with web elements like buttons, menus, and text fields. OpenAI didn’t disclose CUA’s architecture or training methods but said it was trained on simulated and real-world browser scenarios via reinforcement learning.CUA earns high marks on some measures in tests performed by OpenAI. OnWebVoyager, which evaluates web tasks, CUA succeeded 87 percent of the time. OnOSWorld, a benchmark that evaluates the ability of multimodal agents to perform complex tasks that involve real-world web and desktop apps, CUA achieved a success rate of 38.1 percent. In separate tests performed byKuraandAnthropic,on WebVoyager, Kura achieved 87 percent while DeepMind’s Mariner achieved 83.5 percent, and on OSWorld, Claude Sonnet 3.5 with Computer Use achieved 22 percent.Operator isrestrictedfrom interacting with unverified websites and sharing sensitive data without the user’s consent. It offers content filters, and a separate model monitors Operator in real time and pauses the agent in case of suspicious behavior.Behind the news:Operator rides a wave of agents designed to automate everyday tasks. Last week, OpenAI introducedChatGPT Tasks, which lets users schedule reminders and alerts but doesn’t support web interaction. (Early userscomplainedthat Tasks was buggy and required overly precise instructions.) Anthropic’sComputer Usefocuses on basic desktop automation, while DeepMind’sProject Marineris a web-browsing assistant built on Gemini 2.0.Perplexity Assistantautomates mobile apps such as booking Uber rides on Android phones.Why it matters:In early reports, userssaidOperator sometimes was less efficient than a human performing the same tasks. Nevertheless, agentic AI is entering the consumer market, and Operator is poised to give many people their first taste. It’s geared to provide AI assistance for an endless variety of personal and business uses, and — like ChatGPT was for other developers of LLMs — and it’s bound to serve as a template for next-generation products.We’re thinking:Computer use is maturing, and the momentum behind it is palpable. AI developers shouldhave in their toolbox.White House Orders Muscular AI PolicyUnder a new president, the United States reversed its approach to AI regulation, seeking global dominance by reducing restrictions.What’s new:President Trump, who took office last week,signedan executive order that set a 180-day deadline to draft an AI Action Plan. The order aims to boost national security, economic competitiveness, and U.S. leadership in artificial intelligence.How it works:Theexecutive orderassigns responsibility for crafting the AI Action Plan to three key figures in the administration: Michael Kratsios, assistant to the president for science and technology (and former managing director of Scale AI); venture capitalist David Sacks, the new special advisor for AI and cryptocurrency; and national security advisor Michael Waltz.The AI Action Plan must “sustain and enhance America’s global AI dominance in order to promote human flourishing, economic competitiveness, and national security.”The order directs agency heads to suspend or eliminate policies created under President Biden’s2023 executive order, which President Trump revoked, that may conflict with advancing U.S. AI dominance and national security.U.S. companies are to develop AI systems “free from ideological bias or engineered social agendas,” reflecting the administration’s belief that AI systems encode liberal political biases.The order directs the federal Office of Management and Budget to award government contracts to AI companies that align with the administration’s emphasis on advancing U.S. competitiveness and national security.Most provisions leave significant discretion to the team that will draft the action plan, making their interpretation and implementation open-ended.AI infrastructure build-out:Along with the executive order, President Trump announcedStargate, a joint venture that involves OpenAI, Oracle, and SoftBank. The three companies outlined a plan to invest $100 billion in computing infrastructure for AI, such as next-generation data centers, and $500 billion over four years. In addition, the administrationdeclareda national energy emergency with respect to U.S. supplies of energy andissuedan order to ramp up domestic energy production. These measures aim to support energy-intensive AI initiatives like Stargate by removing regulatory barriers to building oil, gas, and renewable energy projects on federal lands.Why it matters:The Trump administrationsaysthat Biden’s 2023 regulations were “onerous and unnecessary,” stifled innovation, and jeopardized U.S. leadership in AI. The new order reduces bureaucratic oversight of AI development, creating a more permissive regulatory environment (except when it comes to ideological bias).We’re thinking:The Biden administration’s 2023 executive order aimed to guard against hypothetical, rather than actual, AI risks. It introduced thresholds of processing used to train models as a measure of their risk — a poorly thought-out proxy. To be fair, the AI Safety Institute under the U.S. National Institute of Standards and Technology didn’t hamper AI progress as much as some had feared, but overall the order was not helpful to AI innovation or safety. We’re pleased that the new administration is focusing on AI progress rather than hypothetical risks.Fine-Tuning Fine PointsThe practice of fine-tuning models on synthetic data is becoming well established. But synthetic training data, even if it represents the training task well, may include characteristics like toxicity that impart unwelcome properties in the trained model’s output, and it may inconsistently represent desired traits such as the target output length. Researchers developed a method that reduces aspects of generated data and retains desired ones.What’s new:Luísa Shimabucoro and colleagues at Cohere introducedactive inheritance, a fine-tuning method that automatically selects synthetic training examples that have desirable characteristics.Key insight:A naive way to generate synthetic fine-tuning data is to feed prompts to a model, collect its output, and use that as the fine-tuning set. But synthetic data is cheap, so we can afford to be more choosy. By generating several responses to each prompt, we can select the one that best suits our purposes.How it works:The authors usedLlama 2 7BandMixtral 8x7Bas both teachers and students in all combinations. They prompted the models with 52,000 prompts from theAlpacadataset and used automated methods to evaluate their outputs in terms of characteristics including social bias, toxicity, word count, lexical diversity, andcalibration(how well a model’s estimated probabilities match its accuracy).The authors generated 10 responses to each prompt.For each response, they measured social bias according to StereoSet, CrowS-Pairs, and Bias Benchmark for Question-Answering. They measured toxicity according toPerspective APIand their own code. They measured calibration according toHELM. They usedTextDescriptivesto calculate metrics related to text.They fine-tuned separate models on (i) the initial responses, (ii) one response to each prompt selected at random, and (iii) the response to each prompt that best maximized each desired characteristic.Results:Fine-tuning on the best response for each characteristic improved performance with respect to that characteristic beyond using the initial outputs or selecting outputs randomly.The authors’ method helped Mixtral 8x7B to generate less-toxic responses. For example, before fine-tuning, the model’sexpected maximum toxicitymeasured 65.2 (lower is better). After fine-tuning on the lowest-toxicity responses generated by Llama 2 7B, Mixtral 8x7B’s expected maximum toxicity fell to 43.2. Conversely, after fine-tuning on random responses generated by Llama 2 7B, its expected maximum toxicity rose to 70.3.It also helped Llama 2 7B to cut its toxicity. Before fine-tuning, the model’s expected maximum toxicity was 71.7. After fine-tuning on its own least-toxic responses, expected maximum toxicity dropped to 50.7. Fine-tuning on random responses made its expected maximum toxicity fall less sharply to 68.1.Examining the impact of the authors’ method on more typical measures of performance, fine-tuning on the least-toxic responses and fine-tuning on random responses had about the same effect across seven benchmarks. Fine-tuning Llama 2 7B on its own least-toxic responses increased performance on average from 59.97 percent accuracy to 60.22 percent accuracy, while fine-tuning on random responses increased performance on average from 59.97 percent accuracy to 61.05 percent accuracy.However, the process degraded performance in some cases. Fine-tuning Mixtral-8x7B on the least-toxic Llama 2 7B responses decreased its average performance across seven benchmarks for question answering and common-sense reasoning from 70.24 percent accuracy to 67.48 percent accuracy. Fine-tuning it on random Llama 2 7B responses cut its average performance from 70.24 percent accuracy to 65.64 percent accuracy.Why it matters:Training on synthetic data is becoming increasingly common. While it shows great promise, best practices for data generation are still being formulated. The authors’ method helps by automatically steering models toward generating more desirable responses, reducing negative traits and reinforcing positive traits.We’re thinking:Knowledge distillation lately has led to more capable and compact models. This approach adds levers of fine control to that technique.\n\n\nShare\nShare\n\nShare\n",
    "image_urls": [
      "https://dl-staging-website.ghost.io/content/images/2025/01/Captura-de-pantalla-2025-01-29-a-la-s--1.31.32-p.-m..png",
      "https://dl-staging-website.ghost.io/content/images/2025/01/The-Batch-ads-and-exclusive-banners--5--1.png",
      "https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--49-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--46-.gif",
      "https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--50-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--51-.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2024/08/Vertical-side-banner-ads-5.png"
    ]
  },
  {
    "title": "issue 284",
    "date": null,
    "url": "https://www.deeplearning.ai/the-batch/issue-284/",
    "content": "The Batch\nWeekly Issues\nissue 284\nPublishedJan 15, 2025\nPublished\n\nPublished\nJan 15, 2025\nReading time13min read\nReading time\n\nReading time\n13min read\nPublishedJan 15, 2025Reading time13min readShareDear friends,Writing software, especially prototypes, is becoming cheaper. This will lead to increased demand for people who can decide what to build. AI Product Management has a bright future!Software is often written by teams that comprise Product Managers (PMs), who decide what to build (such as what features to implement for what users) and Software Developers, who write the code to build the product. Economics shows that when two goods are complements — such as cars (with internal-combustion engines) and gasoline — falling prices in one leads to higher demand for the other. For example, as cars became cheaper, more people bought them, which led to increased demand for gas. Something similar will happen in software. Given a clear specification for what to build, AI is making the building itself much faster and cheaper. This will significantly increase demand for people who can come up with clear specs for valuable things to build.This is why I’m excited about the future of Product Management, the discipline of developing and managing software products. I’m especially excited about the future of AI Product Management, the discipline of developing and managing AI software products.Many companies have an Engineer:PM ratio of, say, 6:1. (The ratio varies widely by company and industry, and anywhere from 4:1 to 10:1 is typical.) As coding becomes more efficient, I think teams will need more product management work (as well as design work) as a fraction of the total workforce. Perhaps engineers will step in to do some of this work, but if it remains the purview of specialized Product Managers, then the demand for these roles will grow.This change in the composition of software development teams is not yet moving forward at full speed. One major force slowing this shift, particularly in AI Product Management, is that Software Engineers, being technical, are understanding and embracing AI much faster than Product Managers. Even today, most companies have difficulty finding people who know how to develop products and also understand AI, and I expect this shortage to grow.Further, AI Product Management requires a different set of skills than traditional software Product Management. It requires:Technical proficiency in AI.PMs need to understand what products might be technically feasible to build. They also need to understand the lifecycle of AI projects, such as data collection, building, then monitoring, and maintenance of AI models.Iterative development.Because AI development is much more iterative than traditional software and requires more course corrections along the way, PMs need to understand how to manage such a process.Data proficiency.AI products often learn from data, and they can be designed to generate richer forms of data than traditional software.Skill in managing ambiguity.Because AI’s performance is hard to predict in advance, PMs need to be comfortable with this and have tactics to manage it.Ongoing learning.AI technology is advancing rapidly. PMs, like everyone else who aims to make best use of the technology, need to keep up with the latest technology advances, product ideas, and how they fit into users’ lives.Finally, AI Product Managers will need to know how to ensure that AI is implemented responsibly (for example, when we need to implement guardrails to prevent bad outcomes), and also be skilled atgathering feedback fastto keep projects moving. Increasingly, I also expect strong product managers to be able tobuild prototypesfor themselves.The demand for good AI Product Managers will be huge. In addition to growing AI Product Management as a discipline, perhaps some engineers will also end up doing more product management work.The variety of valuable things we can build is nearly unlimited. What a great time to build!Keep learning,AndrewA MESSAGE FROM DEEPLEARNING.AIGet up-close and personal with OpenAI’s groundbreaking o1 model! In our short course “Reasoning with o1,” you’ll learn how to get the best performance in coding, planning, and STEM tasks; perform complex, multi-step tasks; and optimize prompts with meta prompting.Enroll todayNewsDeepSeek Ups the Open Weights AnteA new model from Hangzhou upstart DeepSeek delivers outstanding performance and may change the equation for training costs.What’s new:DeepSeek-V3is an open large language model that outperforms Llama 3.1 405B and GPT-4o on key benchmarks and achieves exceptional scores in coding and math. The weights areopenexcept for applications that involve military uses, harming minors, generating false information, and similar restrictions. You can download themhere.Mixture of experts (MoE) basics:The MoE architecture uses different subsets of its parameters to process different inputs. Each MoE layer contains a group of neural networks, or experts, preceded by a gating module that learns to choose which one(s) to use based on the input. In this way, different experts learn to specialize in different types of examples. Because not all parameters are used to produce any given output, the network uses less energy and runs faster than models of similar size that use all parameters to process every input.How it works:DeepSeek-V3 is a mixture-of-experts (MoE) transformer that comprises 671 billion parameters, of which 37 billion are active at any moment. The team trained the model in 2.79 million GPU hours — less than 1/10 thetime required to train Llama 3.1 405B, which DeepSeek-V3 substantially outperforms — at an extraordinarily low cost of $5.6 million.The developers trained it on roughly 15 trillion tokens, including a larger percentage of coding and math data relative to DeepSeek-V2. They fine-tuned it on a wide variety of tasks using output generated byDeepSeek-R1and DeepSeek-V2.5. They further sharpened its performance across diverse domains using the reinforcement learning algorithm known asgroup relative policy optimization.Earlierworkshowed that training to predict the next two tokens would improve performance over learning to predict just one. The authors implemented this procedure. The model learned to predict the first token as usual and used an additional set of layers to learn to predict the second token. The additional layers aren’t used at inference.FollowingDeepSeek-V2, DeepSeek-V3 uses multi-head latent attention, which saves memory during execution relative to other variants of attention.Also like DeepSeek-V2, the new model combines dedicated (routed) and shared experts. The model chooses eight of 256 experts for a particular input, but it also uses a shared expert that processes all inputs.Results:In DeepSeek’s tests, DeepSeek-V3 outperformed Llama 3.1 405B and Qwen 2.5 72B across the board, and its performance compared favorably with that of GPT-4o.DeepSeek-V3 showed exceptional performance in coding and math tasks. In coding, DeepSeek-V3 dominated in five of the seven benchmarks tested. However, DeepSeek-V3 lost to o1 on one of the five, according to a public leaderboard. Specifically, onPolyglot, which tests a model’s ability to generate code in response to difficult requests in multiple programming languages, DeepSeek-V3 (48.5 percent accuracy) beat Claude Sonnet 3.5 (45.3 percent accuracy), though it lost to o1 (61.7 percent accuracy).In language tasks, it performed neck-and-neck with Claude 3.5 Sonnet, achieving higher scores in some tasks and lower in others.Behind the news:OpenAI’s o1 models excel thanks to agentic workflows in which they reflect on their own outputs, use tools, and so on. DeepSeek swims against the tide and achieves superior results without relying on agentic workflows.Why it matters:Open models continue to challenge closed models, giving developers high-quality options that they can modify and deploy at will. But the larger story is DeepSeek-V3’s shockingly low training cost.The team doesn’t explain precisely how the model achieves outstanding performance with such a low processing budget. (The paper credits “meticulous engineering optimizations.”) But it’s likely that DeepSeek’s steady refinement of MoE is a key factor. DeepSeek-V2, also an MoE model, saved more than 40 percent in training versus the earlier DeepSeek 67B, which didn’t employ MoE. In 2022,Microsoftfound that MoE cost five times less in training for equal performance compared to a dense model, andGoogleandMetareported that MoE achieved better performance than dense models trained on the same numbers of tokens.We’re thinking:If they can be replicated, DeepSeek’s results have significant implications for the economics of training foundation models. If indeed it now costs around $5 million to build a GPT-4o-level model, more teams will be able to train such models, and the cost of competing with the AI giants could fall dramatically.U.S. Moves to Expand AI Export RestrictionsThe United States proposed limits on exports of AI technology that would dramatically expand previous restrictions, creating a new international hierarchy for access to advanced chips and models.What’s new:The Biden administration, which will transition to leadership under incoming President Trump next week, issued newrulesthat restrict exports of AI chips and models to most countries beyond a select group of close allies. The rules, which are not yet final, would create a three-tier system that limits exports to a number of close allies and blocks access entirely to China, Iran, North Korea, Russia, and others. They also would introduce the U.S.’ first-ever restrictions on exporting closed weights for large AI models.How it works:The restrictions were announced shortly after aleakreached the press. A public comment period of 120 days will enable the incoming U.S. Presidential administration to consider input from the business and diplomatic communities and modify the rules before they take effect. The rules are scheduled to take effect in one year.A newhierarchydivides nations into three groups that would have different degrees of access to AI chips both designed in the U.S. and manufactured abroad using U.S. technology, as well as proprietary AI models.Tier 1:Australia, Japan, Taiwan, the United Kingdom, and most of Europe would retain nearly unrestricted access. However, these nations must keep 75 percent of their AI computing power within allied countries. No more than 10 percent can be transferred to any single country outside this group to ensure that advanced AI development remains concentrated among close U.S. allies.Tier 2:Traditional U.S. allies and trade partners like Israel, Saudi Arabia, and Singapore face an initial cap of 507 million units of total processing power (TPP) — roughly the computational capacity of 32,000 Nvidia H100 chips — through the first quarter of 2025. The cap would increase to 1.02 billion TPP by 2027. U.S. companies that operate in these countries can apply for higher limits: 633 million TPP in Q1 2025, rising to 5.064 billion TPP by Q1 2027.Tier 3:China, Russia, and around two dozen other countries are blocked from receiving advanced AI chips, model weights, and specialized knowledge related to these systems.The U.S. Commerce Department’s export control agency must approve the export of models or transfer of weights of closed models that were trained using more than 1026computational operations. These rules target future systems, as no known models today used this amount of computation during training.Companies based in the U.S. must maintain at least 50 percent of their total AI computing power within U.S. borders. They also must track distribution of their models, implement security measures, and submit to regular audits.Behind the news:The proposed rules build on 2022’sCHIPS and Science Act, which was designed to strengthen domestic semiconductor production and restrict technologies abroad that could bear on U.S. security. An initial round of restrictions in late 2022barredsemiconductor suppliers AMD and Nvidia from selling advanced chips to Chinese firms. In November 2024, the U.S.tightenedrestrictions further, ordering Taiwan Semiconductor Manufacturing Company, which fabricates those chips, to halt production of advanced chips destined for China.Plus green AI infrastructure:In addition, President Biden issued an executive order to encourage the rapid build-out of computing infrastructure for AI. The federal government will hold competitions among private companies to lease sites it owns for the building of data centers at private expense. The selection of sites will take into account availability of sources of clean energy, including support for nuclear energy. The government will expedite permitting on these sites and support development of energy transmission lines around them. It will also encourage international allies to invest in AI infrastructure powered by clean energy.Why it matters:Protecting the United States’ advantages in high tech has been a rising priority for the White House over the past decade. The earlier export restrictions forced many Chinese AI developers to rely on less-powerful hardware. The new limits are likely to have a far broader impact. They could force developers in the Tier 2 and Tier 3 countries to build less resource-intensive models and lead them to collaborate more closely with each other, reducing the value of U.S.-made technology worldwide. They could hurt U.S. chip vendors, which havewarnedthat the rules could weaken U.S. competitiveness in the global economy. They could also force companies that are building huge data centers to process AI calculations toreconsidertheir plans.We’re thinking:The Biden administration’s embargo on AI chips has beenleaky. So far, it has slowed down adversaries only slightly while spurring significant investment in potentialsuppliersthat aren’t connected to the U.S. While the public comment period invites lobbying and industry feedback, ultimately geopolitical priorities may hold sway. Whatever the outcome, reducing the world’s dependence on U.S. chips and models would result in a very different global AI ecosystem.AI Supercomputer on Your DeskNvidia’s new desktop computer is built specifically to run large AI models.What’s new:Project Digitsis a personal supercomputer intended to help developers fine-tune and run large models locally. Project Digits, which is small enough to hold in one hand, will be available in May, starting at $3000.How it works:Project Digits is designed to run models of up to 200 billion parameters — roughly five times the size that fits comfortably on typical consumer hardware — provided they’re quantized to 4 bits of precision. Two units can be connected to run models such as Meta’s Llama 3.1 405B. Complete specifications are not yet available.Project Digits runs Nvidia’s DGX operating system, a flavor of Ubuntu Linux.The system is based on a GB10 system-on-a-chip that combines the Nvidia Blackwell GPU architecture (which serves as the basis for its latest B100 GPUs) and Grace CPU architecture (designed to manage AI workloads in data centers), connected via high-bandwidth NVLink interconnect.It comes with 128 GB of unified memory and 4 terabytes of solid-state storage.The system connects to Nvidia’s DGX Cloud service to enable developers to deploy models from a local machine to cloud infrastructure.Behind the news:In a blitz of announcements at the Consumer Electronics Show (CES), Nvidia also launched a platform for developing robotics, autonomous vehicles, and other physical AI systems. Cosmos includes pretrained language and vision models that range from 4 billion to 14 billion parameters for generating synthetic training data for robots or building policy models that translate a robot’s state into its next action. Nvidia also released Cosmos Nemotron, a 34 billion-parameter, vision-language model designed for use by AI agents, plus a video tokenizer and other tools for robotics developers.Why it matters:It’s common to train models on Nvidia A100 or H100 GPUs, which come with a price tag of at least $8,000 or $20,000 respectively, along with 40 gigabytes to 80 gigabytes of memory. These hefty requirements push many developers to buy access to computing infrastructure from a cloud provider. Coming in at $3,000 with 128 gigabytes of memory, Project Digits is designed to empower machine learning engineers to train and run larger models on their own machines.We’re thinking:We look forward to seeing cost/throughput comparisons between running a model on Project Digits, A100, and H100.Calibrating ContrastContrastive loss functions make it possible to produce good embeddings without labeled data. A twist on this idea makes even more useful embeddings.What’s new:Vlad Sobal and colleagues at Meta, New York University, Brown University, Genentech, and Canadian Institute for Advanced Research introducedX-Sample contrastive loss(X-CLR), a self-supervised loss function that enables vision models to learn embeddings that capture similarities and differences among examples with greater subtlety.Key insight:Contrastive loss functions likeSimCLRequally encourage a model to produce dissimilar embeddings of images of, say, a cat, a dog, and a dump truck. But, of course, cats and dogs are more similar to each other than either are to dump trucks. Instead of marking examples as similar or dissimilar, X-CLR assigns similarity scores, so a model can learn to produce embeddings that match those scores.How it works:The authors used X-CLR to train an embedding model onConceptual Captionsdatasets of image-text pairs scraped from the web: CC-3M (3 million text-image pairs) and CC-12M (12 million text-image pairs). The model was similar toCLIP, except the text encoder was asentence transformerpretrained on sentence pairs, and the vision encoder was aResNet-50pretrained on ImageNet.The sentence transformer embedded text captions for all examples. The system computed similarity scores according to cosine similarity between the text embeddings.Similarly, a ResNet-50 computed image embeddings, and the system computed similarity scores between them.The authors froze the sentence transformer and used the text similarity scores as labels in the loss function. The loss function minimized the difference between the similarity scores of the text embeddings and the corresponding similarity scores of the image embeddings.Results:Systems trained using X-CLR outperformed competitors inImageNetclassification, especially when less training data was available. (The authors followed CLIP’s method of classification: They computed the similarity between an image embedding and text embeddings of all classes. The image’s classification was the class that corresponds to the text embedding with the highest similarity to the image embedding.)The authors compared a system trained using X-CLR, one trained using SimCLR, and CLIP. After training on the CC-3M dataset, the X-CLR system achieved 58.2 percent accuracy on ImageNet, while the SimCLR model achieved 57.0 percent and CLIP achieved 41.0 percent.Training on CC-12M resulted in smaller differences: X-CLR achieved 59.4 percent accuracy, SimCLR achieved 58.9 percent, and CLIP achieved 58.8 percent.Why it matters:Contrastive loss functions are very useful, but the similar/dissimilar dichotomy leaves important nuances unaccounted for. Like CLIP, X-CLR takes advantage of both images and their captions for self-supervised learning. However, CLIP learns to recognize image-text pairs as similar or dissimilar, while X-CLR matches image-image pairs using captions as a similarity signal that’s continuous rather than discrete.We’re thinking:Reality is not black and white. Allowing for shades of gray makes for better modeling.Share\nPublishedJan 15, 2025\nPublished\n\nPublished\nJan 15, 2025\nJan 15, 2025\nReading time13min read\nReading time\n\nReading time\n13min read\nShare\nShare\n\nShare\n\nDear friends,Writing software, especially prototypes, is becoming cheaper. This will lead to increased demand for people who can decide what to build. AI Product Management has a bright future!Software is often written by teams that comprise Product Managers (PMs), who decide what to build (such as what features to implement for what users) and Software Developers, who write the code to build the product. Economics shows that when two goods are complements — such as cars (with internal-combustion engines) and gasoline — falling prices in one leads to higher demand for the other. For example, as cars became cheaper, more people bought them, which led to increased demand for gas. Something similar will happen in software. Given a clear specification for what to build, AI is making the building itself much faster and cheaper. This will significantly increase demand for people who can come up with clear specs for valuable things to build.This is why I’m excited about the future of Product Management, the discipline of developing and managing software products. I’m especially excited about the future of AI Product Management, the discipline of developing and managing AI software products.Many companies have an Engineer:PM ratio of, say, 6:1. (The ratio varies widely by company and industry, and anywhere from 4:1 to 10:1 is typical.) As coding becomes more efficient, I think teams will need more product management work (as well as design work) as a fraction of the total workforce. Perhaps engineers will step in to do some of this work, but if it remains the purview of specialized Product Managers, then the demand for these roles will grow.This change in the composition of software development teams is not yet moving forward at full speed. One major force slowing this shift, particularly in AI Product Management, is that Software Engineers, being technical, are understanding and embracing AI much faster than Product Managers. Even today, most companies have difficulty finding people who know how to develop products and also understand AI, and I expect this shortage to grow.Further, AI Product Management requires a different set of skills than traditional software Product Management. It requires:Technical proficiency in AI.PMs need to understand what products might be technically feasible to build. They also need to understand the lifecycle of AI projects, such as data collection, building, then monitoring, and maintenance of AI models.Iterative development.Because AI development is much more iterative than traditional software and requires more course corrections along the way, PMs need to understand how to manage such a process.Data proficiency.AI products often learn from data, and they can be designed to generate richer forms of data than traditional software.Skill in managing ambiguity.Because AI’s performance is hard to predict in advance, PMs need to be comfortable with this and have tactics to manage it.Ongoing learning.AI technology is advancing rapidly. PMs, like everyone else who aims to make best use of the technology, need to keep up with the latest technology advances, product ideas, and how they fit into users’ lives.Finally, AI Product Managers will need to know how to ensure that AI is implemented responsibly (for example, when we need to implement guardrails to prevent bad outcomes), and also be skilled atgathering feedback fastto keep projects moving. Increasingly, I also expect strong product managers to be able tobuild prototypesfor themselves.The demand for good AI Product Managers will be huge. In addition to growing AI Product Management as a discipline, perhaps some engineers will also end up doing more product management work.The variety of valuable things we can build is nearly unlimited. What a great time to build!Keep learning,AndrewA MESSAGE FROM DEEPLEARNING.AIGet up-close and personal with OpenAI’s groundbreaking o1 model! In our short course “Reasoning with o1,” you’ll learn how to get the best performance in coding, planning, and STEM tasks; perform complex, multi-step tasks; and optimize prompts with meta prompting.Enroll todayNewsDeepSeek Ups the Open Weights AnteA new model from Hangzhou upstart DeepSeek delivers outstanding performance and may change the equation for training costs.What’s new:DeepSeek-V3is an open large language model that outperforms Llama 3.1 405B and GPT-4o on key benchmarks and achieves exceptional scores in coding and math. The weights areopenexcept for applications that involve military uses, harming minors, generating false information, and similar restrictions. You can download themhere.Mixture of experts (MoE) basics:The MoE architecture uses different subsets of its parameters to process different inputs. Each MoE layer contains a group of neural networks, or experts, preceded by a gating module that learns to choose which one(s) to use based on the input. In this way, different experts learn to specialize in different types of examples. Because not all parameters are used to produce any given output, the network uses less energy and runs faster than models of similar size that use all parameters to process every input.How it works:DeepSeek-V3 is a mixture-of-experts (MoE) transformer that comprises 671 billion parameters, of which 37 billion are active at any moment. The team trained the model in 2.79 million GPU hours — less than 1/10 thetime required to train Llama 3.1 405B, which DeepSeek-V3 substantially outperforms — at an extraordinarily low cost of $5.6 million.The developers trained it on roughly 15 trillion tokens, including a larger percentage of coding and math data relative to DeepSeek-V2. They fine-tuned it on a wide variety of tasks using output generated byDeepSeek-R1and DeepSeek-V2.5. They further sharpened its performance across diverse domains using the reinforcement learning algorithm known asgroup relative policy optimization.Earlierworkshowed that training to predict the next two tokens would improve performance over learning to predict just one. The authors implemented this procedure. The model learned to predict the first token as usual and used an additional set of layers to learn to predict the second token. The additional layers aren’t used at inference.FollowingDeepSeek-V2, DeepSeek-V3 uses multi-head latent attention, which saves memory during execution relative to other variants of attention.Also like DeepSeek-V2, the new model combines dedicated (routed) and shared experts. The model chooses eight of 256 experts for a particular input, but it also uses a shared expert that processes all inputs.Results:In DeepSeek’s tests, DeepSeek-V3 outperformed Llama 3.1 405B and Qwen 2.5 72B across the board, and its performance compared favorably with that of GPT-4o.DeepSeek-V3 showed exceptional performance in coding and math tasks. In coding, DeepSeek-V3 dominated in five of the seven benchmarks tested. However, DeepSeek-V3 lost to o1 on one of the five, according to a public leaderboard. Specifically, onPolyglot, which tests a model’s ability to generate code in response to difficult requests in multiple programming languages, DeepSeek-V3 (48.5 percent accuracy) beat Claude Sonnet 3.5 (45.3 percent accuracy), though it lost to o1 (61.7 percent accuracy).In language tasks, it performed neck-and-neck with Claude 3.5 Sonnet, achieving higher scores in some tasks and lower in others.Behind the news:OpenAI’s o1 models excel thanks to agentic workflows in which they reflect on their own outputs, use tools, and so on. DeepSeek swims against the tide and achieves superior results without relying on agentic workflows.Why it matters:Open models continue to challenge closed models, giving developers high-quality options that they can modify and deploy at will. But the larger story is DeepSeek-V3’s shockingly low training cost.The team doesn’t explain precisely how the model achieves outstanding performance with such a low processing budget. (The paper credits “meticulous engineering optimizations.”) But it’s likely that DeepSeek’s steady refinement of MoE is a key factor. DeepSeek-V2, also an MoE model, saved more than 40 percent in training versus the earlier DeepSeek 67B, which didn’t employ MoE. In 2022,Microsoftfound that MoE cost five times less in training for equal performance compared to a dense model, andGoogleandMetareported that MoE achieved better performance than dense models trained on the same numbers of tokens.We’re thinking:If they can be replicated, DeepSeek’s results have significant implications for the economics of training foundation models. If indeed it now costs around $5 million to build a GPT-4o-level model, more teams will be able to train such models, and the cost of competing with the AI giants could fall dramatically.U.S. Moves to Expand AI Export RestrictionsThe United States proposed limits on exports of AI technology that would dramatically expand previous restrictions, creating a new international hierarchy for access to advanced chips and models.What’s new:The Biden administration, which will transition to leadership under incoming President Trump next week, issued newrulesthat restrict exports of AI chips and models to most countries beyond a select group of close allies. The rules, which are not yet final, would create a three-tier system that limits exports to a number of close allies and blocks access entirely to China, Iran, North Korea, Russia, and others. They also would introduce the U.S.’ first-ever restrictions on exporting closed weights for large AI models.How it works:The restrictions were announced shortly after aleakreached the press. A public comment period of 120 days will enable the incoming U.S. Presidential administration to consider input from the business and diplomatic communities and modify the rules before they take effect. The rules are scheduled to take effect in one year.A newhierarchydivides nations into three groups that would have different degrees of access to AI chips both designed in the U.S. and manufactured abroad using U.S. technology, as well as proprietary AI models.Tier 1:Australia, Japan, Taiwan, the United Kingdom, and most of Europe would retain nearly unrestricted access. However, these nations must keep 75 percent of their AI computing power within allied countries. No more than 10 percent can be transferred to any single country outside this group to ensure that advanced AI development remains concentrated among close U.S. allies.Tier 2:Traditional U.S. allies and trade partners like Israel, Saudi Arabia, and Singapore face an initial cap of 507 million units of total processing power (TPP) — roughly the computational capacity of 32,000 Nvidia H100 chips — through the first quarter of 2025. The cap would increase to 1.02 billion TPP by 2027. U.S. companies that operate in these countries can apply for higher limits: 633 million TPP in Q1 2025, rising to 5.064 billion TPP by Q1 2027.Tier 3:China, Russia, and around two dozen other countries are blocked from receiving advanced AI chips, model weights, and specialized knowledge related to these systems.The U.S. Commerce Department’s export control agency must approve the export of models or transfer of weights of closed models that were trained using more than 1026computational operations. These rules target future systems, as no known models today used this amount of computation during training.Companies based in the U.S. must maintain at least 50 percent of their total AI computing power within U.S. borders. They also must track distribution of their models, implement security measures, and submit to regular audits.Behind the news:The proposed rules build on 2022’sCHIPS and Science Act, which was designed to strengthen domestic semiconductor production and restrict technologies abroad that could bear on U.S. security. An initial round of restrictions in late 2022barredsemiconductor suppliers AMD and Nvidia from selling advanced chips to Chinese firms. In November 2024, the U.S.tightenedrestrictions further, ordering Taiwan Semiconductor Manufacturing Company, which fabricates those chips, to halt production of advanced chips destined for China.Plus green AI infrastructure:In addition, President Biden issued an executive order to encourage the rapid build-out of computing infrastructure for AI. The federal government will hold competitions among private companies to lease sites it owns for the building of data centers at private expense. The selection of sites will take into account availability of sources of clean energy, including support for nuclear energy. The government will expedite permitting on these sites and support development of energy transmission lines around them. It will also encourage international allies to invest in AI infrastructure powered by clean energy.Why it matters:Protecting the United States’ advantages in high tech has been a rising priority for the White House over the past decade. The earlier export restrictions forced many Chinese AI developers to rely on less-powerful hardware. The new limits are likely to have a far broader impact. They could force developers in the Tier 2 and Tier 3 countries to build less resource-intensive models and lead them to collaborate more closely with each other, reducing the value of U.S.-made technology worldwide. They could hurt U.S. chip vendors, which havewarnedthat the rules could weaken U.S. competitiveness in the global economy. They could also force companies that are building huge data centers to process AI calculations toreconsidertheir plans.We’re thinking:The Biden administration’s embargo on AI chips has beenleaky. So far, it has slowed down adversaries only slightly while spurring significant investment in potentialsuppliersthat aren’t connected to the U.S. While the public comment period invites lobbying and industry feedback, ultimately geopolitical priorities may hold sway. Whatever the outcome, reducing the world’s dependence on U.S. chips and models would result in a very different global AI ecosystem.AI Supercomputer on Your DeskNvidia’s new desktop computer is built specifically to run large AI models.What’s new:Project Digitsis a personal supercomputer intended to help developers fine-tune and run large models locally. Project Digits, which is small enough to hold in one hand, will be available in May, starting at $3000.How it works:Project Digits is designed to run models of up to 200 billion parameters — roughly five times the size that fits comfortably on typical consumer hardware — provided they’re quantized to 4 bits of precision. Two units can be connected to run models such as Meta’s Llama 3.1 405B. Complete specifications are not yet available.Project Digits runs Nvidia’s DGX operating system, a flavor of Ubuntu Linux.The system is based on a GB10 system-on-a-chip that combines the Nvidia Blackwell GPU architecture (which serves as the basis for its latest B100 GPUs) and Grace CPU architecture (designed to manage AI workloads in data centers), connected via high-bandwidth NVLink interconnect.It comes with 128 GB of unified memory and 4 terabytes of solid-state storage.The system connects to Nvidia’s DGX Cloud service to enable developers to deploy models from a local machine to cloud infrastructure.Behind the news:In a blitz of announcements at the Consumer Electronics Show (CES), Nvidia also launched a platform for developing robotics, autonomous vehicles, and other physical AI systems. Cosmos includes pretrained language and vision models that range from 4 billion to 14 billion parameters for generating synthetic training data for robots or building policy models that translate a robot’s state into its next action. Nvidia also released Cosmos Nemotron, a 34 billion-parameter, vision-language model designed for use by AI agents, plus a video tokenizer and other tools for robotics developers.Why it matters:It’s common to train models on Nvidia A100 or H100 GPUs, which come with a price tag of at least $8,000 or $20,000 respectively, along with 40 gigabytes to 80 gigabytes of memory. These hefty requirements push many developers to buy access to computing infrastructure from a cloud provider. Coming in at $3,000 with 128 gigabytes of memory, Project Digits is designed to empower machine learning engineers to train and run larger models on their own machines.We’re thinking:We look forward to seeing cost/throughput comparisons between running a model on Project Digits, A100, and H100.Calibrating ContrastContrastive loss functions make it possible to produce good embeddings without labeled data. A twist on this idea makes even more useful embeddings.What’s new:Vlad Sobal and colleagues at Meta, New York University, Brown University, Genentech, and Canadian Institute for Advanced Research introducedX-Sample contrastive loss(X-CLR), a self-supervised loss function that enables vision models to learn embeddings that capture similarities and differences among examples with greater subtlety.Key insight:Contrastive loss functions likeSimCLRequally encourage a model to produce dissimilar embeddings of images of, say, a cat, a dog, and a dump truck. But, of course, cats and dogs are more similar to each other than either are to dump trucks. Instead of marking examples as similar or dissimilar, X-CLR assigns similarity scores, so a model can learn to produce embeddings that match those scores.How it works:The authors used X-CLR to train an embedding model onConceptual Captionsdatasets of image-text pairs scraped from the web: CC-3M (3 million text-image pairs) and CC-12M (12 million text-image pairs). The model was similar toCLIP, except the text encoder was asentence transformerpretrained on sentence pairs, and the vision encoder was aResNet-50pretrained on ImageNet.The sentence transformer embedded text captions for all examples. The system computed similarity scores according to cosine similarity between the text embeddings.Similarly, a ResNet-50 computed image embeddings, and the system computed similarity scores between them.The authors froze the sentence transformer and used the text similarity scores as labels in the loss function. The loss function minimized the difference between the similarity scores of the text embeddings and the corresponding similarity scores of the image embeddings.Results:Systems trained using X-CLR outperformed competitors inImageNetclassification, especially when less training data was available. (The authors followed CLIP’s method of classification: They computed the similarity between an image embedding and text embeddings of all classes. The image’s classification was the class that corresponds to the text embedding with the highest similarity to the image embedding.)The authors compared a system trained using X-CLR, one trained using SimCLR, and CLIP. After training on the CC-3M dataset, the X-CLR system achieved 58.2 percent accuracy on ImageNet, while the SimCLR model achieved 57.0 percent and CLIP achieved 41.0 percent.Training on CC-12M resulted in smaller differences: X-CLR achieved 59.4 percent accuracy, SimCLR achieved 58.9 percent, and CLIP achieved 58.8 percent.Why it matters:Contrastive loss functions are very useful, but the similar/dissimilar dichotomy leaves important nuances unaccounted for. Like CLIP, X-CLR takes advantage of both images and their captions for self-supervised learning. However, CLIP learns to recognize image-text pairs as similar or dissimilar, while X-CLR matches image-image pairs using captions as a similarity signal that’s continuous rather than discrete.We’re thinking:Reality is not black and white. Allowing for shades of gray makes for better modeling.\n\n\nShare\nShare\n\nShare\n",
    "image_urls": [
      "https://dl-staging-website.ghost.io/content/images/2025/01/AIProductManager-2_1200px-1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/01/The-Batch-ads-and-exclusive-banners---2024-12-16T174314.640--1-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--45-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/01/BIDENCHIPS-10_1200px.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--47-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--44-.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2023/03/2.png"
    ]
  },
  {
    "title": "issue 303",
    "date": null,
    "url": "https://www.deeplearning.ai/the-batch/issue-303/",
    "content": "The Batch\nWeekly Issues\nissue 303\nPublishedMay 28, 2025\nPublished\n\nPublished\nMay 28, 2025\nReading time15min read\nReading time\n\nReading time\n15min read\nPublishedMay 28, 2025Reading time15min readShareLoading theElevenlabs Text to SpeechAudioNative Player...Dear friends,I am alarmed by the proposed cuts to U.S. funding for basic research, analyzedhere, and the impact this would have for U.S. competitiveness in AI and other areas. Funding research that is openly shared benefits the whole world, but the nation it benefits most is the one where the research is done.If not forfunding for my early work in deep learningfrom the National Science Foundation (NSF)  and Defense Advanced Research Projects Agency (DARPA), which disburse much of U.S. research funding, I would not have discovered lessons about scaling that led me to pitch starting Google Brain to scale up deep learning. I am worried that cuts to funding for basic science will lead the U.S. — and also the world — to miss out on the next set of ideas.In fact, such funding benefits the U.S. more than any other nation.  Scientific research brings the greatest benefit to the country where the work happens because (i) the new knowledge diffuses fastest within that country, and (ii) the process of doing research creates new talent for that nation.Why does most innovation in generative AI still happen in Silicon Valley? Because two teams based in this area — Google Brain, which invented the transformer network, and OpenAI, which scaled it up — did a lot of the early work. Subsequently, team members moved to other nearby businesses, started competitors, or worked with local universities. Further, local social networks rapidly diffused the knowledge through casual coffee meetings, local conferences, and even children’s play dates, where parents of like-aged kids meet and discuss technical ideas. In this way, the knowledge spread faster within Silicon Valley than to other geographies.In a similar vein, research done in the U.S. diffuses to others in the U.S. much faster than to other geographic areas. This is particularly true when the research is openly shared through papers and/or open source: If researchers have permission to talk about an idea, they can share much more information, such as tips and tricks for how to really make an algorithm work, more quickly. It also lets others figure out faster who can answer their questions. Diffusion of knowledge created in academic environments is especially fast. Academia tends to be completely open, and students and professors, unlike employees of many companies, have full permission to talk about their work.Thus funding basic research in the U.S. benefits the U.S. most, and also benefits our allies. It is true that openness benefits our adversaries, too. But as a subcommittee of the U.S. House of Representatives committee on science, space, and technologypoints out, “... open sharing of fundamental research is [not] without risk. Rather, ... openness in research is so important to competitiveness and security that it warrants the risk that adversaries may benefit from scientific openness as well.”Further, generative AI is evolving so rapidly that staying on the cutting edge is what’s really critical. For example, the fact that many teams can now train a model with GPT-3.5- or even GPT-4-level capability does not seem to be hurting OpenAI much, which is busy growing its business by developing the cutting-edge o4, Codex, GPT-4.1, and so on. Those who invent a technology get to commercialize it first, and in a fast-moving world, the cutting-edge technology is what’s most valuable. Studies likethis one(albeit done while the internet was not as prevalent as it is today) also show how knowledge diffuses locally much faster than globally.China was decisively behind the U.S. in generative AI when ChatGPT was first launched in 2022. However, China’s tech ecosystem is very open internally, and this has helped it to catch up over the past two years:There is ample funding for open academic research in China.China’s businesses such as DeepSeek and Alibaba have released cutting-edge, open-weights models. This openness at the corporate level accelerates diffusion of knowledge.China’s labor laws make non-compete agreements (which stop an employee from jumping ship to a competitor) relatively hard to enforce, and the work culture supports significant idea sharing among employees of different companies; this has made circulation of ideas relatively efficient.While there’s also much about China that I would not seek to emulate, the openness of its tech ecosystem has helped it accelerate.In 1945, Vannevar Bush’s landmark report “Science, The Endless Frontier” laid down key principles for public funding of U.S. research and talent development. Those principles enabled the U.S. to dominate scientific progress for decades. U.S. federal funding for science created numerous breakthroughs that have benefited the U.S. tremendously, and also the world, while training generations of domestic scientists, as well as immigrants who likewise benefit the U.S.The good news is that this playbook is now well known. I hope many more nations will imitate it and invest heavily in science and talent. And I hope that, having pioneered this very successful model, the U.S. will not pull back from it by enacting drastic cuts to funding scientific research.AndrewA MESSAGE FROM DEEPLEARNING.AI AND SNOWFLAKEWe’re featured partners of Snowflake’s Dev Day 2025, a full day for AI and data practitioners to explore cutting-edge demos, make valuable contacts, and hear from top voices in the field (including Andrew Ng). See you on June 5!Register hereNewsClaude 4 Advances Code GenerationAnthropic continued its tradition of building AI models that raise the bar in coding tasks.What’s new:Anthropic launchedClaude 4 Sonnet 4 and Claude Opus 4, the latest medium- and largest-size members of its family of general-purpose large language models. Both models offer an optional reasoning mode and can use multiple tools in parallel while reasoning. In addition, the company made generally available Claude Code, a coding agent previously offered as a research preview, along with a Claude Code software development kit.Input/output:Text, images, PDF files in (up to 200,000 tokens); text out (Claude Sonnet 4 up to 64,000 tokens, Claude Opus 4 up to 32,000 tokens)Features:Parallel tool use including computer use, selectable reasoning mode with visible reasoning tokens, multilingual (15 languages)Performance:Ranked Number One in LMSys WebDev Arena, state-of-the-art on SWE-bench and Terminal-benchAvailability/price:Anthropic API, Amazon Bedrock, Google Cloud Vertex AI.Claude Sonnet 4$3/$15 per million input/output tokens,Claude Opus 4$15/$75 per million input/output tokensUndisclosed:Parameter counts, specific training methods and datasetsHow it works:The team trained the Claude 4 models on a mix of publicly available information on the web as well as proprietary purchased data, data from Claude users who opted to share their inputs and outputs, and generated data. They fine-tuned the models to behelpful, honest, and harmlessaccording to human andAI feedback.The models make reasoning tokens visible within limits. For especially lengthy chains of thought, an unspecified smaller model summarizes reasoning tokens.Given local file access, Claude Opus 4 can create and manipulate files to store information. For instance, prompted to maintain a knowledge base while playing a Pokémon video game, the model produced a guide to the game that offered advice such as, “If stuck, try OPPOSITE approach” and “Change Y-coordinate when horizontal movement fails.”Results:Both Claude 4 models tied Google Gemini 2.5 Pro at the top of the LMSys WebDev Arena and achieved top marks for coding and agentic computer-use benchmarks in Anthropic’s tests.OnSWE-bench Verified, which tests the model’s ability to solve software issues from GitHub, Claude Opus 4 succeeded 72.5 percent of the time, and Claude Sonnet 4 succeeded 72.7 percent of the time. The next best model, OpenAI o3, succeeded 70.3 percent of the time.Terminal-benchevaluates how well models work with the benchmark’s built-in agentic framework to perform tasks on a computer terminal. Claude Opus 4 succeeded 39.2 percent of the time and Claude Sonnet 4 succeeded 33.5 percent of the time, whereas the closest competitor, OpenAI GPT 4.1, succeeded 30.3 percent of the time. Using Claude Code as the agentic framework, Claude Opus 4 succeeded 43.2 percent of the time and Claude Sonnet 4 succeeded 35.5 percent of the time.Why it matters:The new models extend LLM technology with parallel tool use, using external files as a form of memory, and staying on-task over unusually long periods of time. Early users have reported many impressive projects, including aTetris clonebuilt in one shot and a seven-hour stintrefactoring Rakutan’s open-source code base.We’re thinking:Prompting expert @elder_plinius published a text file that is purported to beClaude 4’s system promptand includes some material that does not appear in Anthropic’s ownpublicationof the prompts. It is instructive to see how it conditions the model for tool use, agentic behavior, and reasoning.Google I/O OverdriveGoogle revamped its roster of models, closed and open, and added more AI-powered features to its existing products.What’s new:Google staged a parade ofannouncementsat this year’s I/O developer conference. New offerings include improvements toGemini 2.5 Pro and Gemini 2.5 Flashand a preview ofGemma 3n(all three generally available in June), the updatedVeo 3video generator (available via Flow, Google’s AI videography app, for paid subscribers to its AI Pro and Ultra services), and increasingly AI-powered search.How it works:The I/O offerings spanned from public-facing products to developer tools.Google updated Gemini 2.5 Pro and the speedier Gemini 2.5 Flash with audio output, so both models now take in text, audio, images, and video and produce text and audio. In addition, they offer summaries of tokens produced while reasoning. Gemini-2.5-Pro-Preview-05-06, which topped the LMSysText ArenaandWebDev Arena(tied with Claude 4 Opus and Sonnet), lets users set a reasoning budget up to 128,000 tokens, enabling it to outperform OpenAI o3 and o4-mini (set to high effort) on math, coding, and multimodal benchmarks in Google’s tests. Gemini-2.5-Flash-Preview-05-20 uses 22 percent fewer tokens than its predecessor while ranking near the top of the LMSys Text Arena and WebDev Arena.The Veo 3 text-to-video generator produces 3840x2160-pixel video with audio (dialogue, sound effects, and music) with creative controls including the ability to add and remove objects and maintain consistent characters. It bested Kuaishu Kling 2.0, Runway Gen 3, and OpenAI Sora in Google’s comparisons.New members of Google’sGemma 3family of open-weights models, Gemma 3n 5B and 8B, are multilingual (over 140 languages), multimodal (text, vision, audio in; text out), and optimized for mobile platforms. Gemma-3n-E4B-it (8 billion parameters) ranks just ahead of Anthropic Claude 3.7 Sonnet in the LMSys Text Arena. Gemma 3n 5B and 8B are 1.5 times faster than their predecessors and require 2 gigabytes and 3 gigabytes of memory, respectively, thanks totechniquesthat include per-layer embeddings, key-value caching, conditional parameter loading (constraining active parameters to specific modalities at inference), and a Matryoshka Transformer design that dynamically activates nested sub-models. They’re available in preview via Google’s AI Studio, AI Edge, GenAI SDK, or MediaPipe.Google introduced several specialized AI tools and models.Julesis an autonomous, asynchronous, multi-agent coding assistant that clones repos into a secure virtual machine to perform tasks like writing tests, building features, and fixing bugs (available in public beta).SignGemmatranslates American sign language to text (previously ASL to English).MedGemmaanalyzes medical text and images (part of the open-weights collection Health AI Developer Foundations).Building on Google Search’s AI Overviews, Google is further building AI into search. Google Search’sAI Modeuses Gemini 2.5 to deliver a “deep search” mode that decomposes users’ questions into hundreds of sub-queries for analysis and visualization. Google plans to integrate AI Mode features into its core search product. In addition, Google Search’s AI Mode will gainSearch Live(real-time, audio-enabled visual interaction via camera) andagentic features(for tasks such as purchasing tickets). Computer-use capabilities are coming to the Gemini API and Vertex AI.Why it matters:Google is catching up with the Microsoft/OpenAI colossus on several fronts. The addition of audio output to Gemini and Gemma models fuels the rise of voice-to-voice and other audio applications and gives developers powerful new tools to build them. At the same time, Veo 3’s text-to-video-plus-audio output showsmarkedimprovementover the previous version.Behind the news:The number of tokens Google processed monthly has surged this year from 9.7 trillion last year to 480 trillion, a sign that its AI APIs and AI-infused products are rapidly gaining traction. Google’s progress contrasts with Apple’s ongoingstruggles. Both share advantages in smartphones and app distribution. But, while Google has showcased a string of advanced models as well as early efforts to integrate them into legacy products, Apple’s organizational challenges have hampered its AI development. Now Apple must contend with OpenAI’sacquisitionof LoveFrom, the startup founded by its former lead product designer Jony Ive.We’re thinking:Google I/O 2025 was a strong showing of generative AI capabilities! There’s still work to be done to translate these innovations into compelling products, but the company now has a strong base for building numerous innovative products.How DeepSeek Did ItDeepSeek made headlines late last year, when it built a state-of-the-art, open-weights large language model at a cost far lower than usual. The upstart developer shared new details about its method.What’s new:Chenggang Zhao and colleagues at DeepSeek describedsoftware and hardware choicesthat reduced memory and processing requirements while building their groundbreaking mixture-of-experts models DeepSeek-R1 and DeepSeek-V3.Mixture of experts (MoE) basics:The MoE architecture uses different subsets of a model’s parameters to process different inputs. Each MoE layer contains a group of neural networks, or experts, preceded by a routing module that learns to choose which one(s) to use based on the training example. In this way, different experts learn to specialize in different types of input.How it works:The authors trained DeepSeek-R1 and DeepSeek-V3 using a cluster of 2,048 Nvidia H800 GPUs composed of nodes that contained 8 GPUs each. MoE requires less memory than dense architectures, since a given input activates only a portion of a model’s parameters. This enabled the authors to train DeepSeek-V3 on 250 GFLOPs per token, while Qwen 2.5 72B required 394 GFLOPs per token and Llama 3.1 405B required 2,448 GFLOPs per token.The authors built a mixed-precision training algorithm to reduce the memory requirements of training MoE models. They used FP8 (8-bit) numbers to perform computations including linear transformations and 16- or 32-bit precision to perform others such as computing embeddings. (They say DeepSeek-V3 was the first open LLM to have been trained using FP8.)The authors noticed that communication between GPUs inside a node was four times faster than communication between nodes. To ensure fast communication when routing tokens to experts, they limited the algorithm to process them within up to 4 nodes.To utilize GPUs more fully, they divided each GPU’s input data so the chip processes computation and communication at the same time. Specifically, the chip computes attention or MoE layers on one part of the data and simultaneously sends the other part of the data to other GPUs or aggregates it from other GPUs as necessary.To further save inference memory, the models use multi-head latent attention, which saves memory during execution relative to other variants of attention. The authors compared their implementation to the variant GQA used in Qwen 2.5 72B and Llama 3.1 405B. Their method (70 kilobytes per token) used far less memory than Qwen-2.5 (328 kilobytes per token) or Llama 3.1 (516 kilobytes per token).Behind the news:DeepSeek-V3made waves when it was released in December. It performed better than Llama 3.1 405B, the leading LLM at the time, but its training cost was an astonishing $5.6 million, compared to the usual tens to hundreds of millions of dollars. Some observers wereskepticalof the reported cost, pointing out that the $5.6 million dollar figure doesn’t include salaries, data acquisition and annotation, processing failed training runs, and other research and development costs. In addition, the cost of trainingDeepSeek-R1remains unknown.Why it matters:Traditionally, only companies with large budgets and vast resources could afford to train state-of-the-art models. DeepSeek changed that but didn’t explain how when it released its models. By sharing the details, the company has empowered a wider range of teams to improve the state of the art.We’re thinking:Shortly after DeepSeek-R1 was released, some engineers claimed — without presenting evidence — that DeepSeek had copied their work. DeepSeek’s disclosure of its training methods should lay to rest any remaining questions about this. Its work was truly innovative, and we applaud its release of key technical details.Did GPT-4o Train on O’Reilly Books?A study co-authored by tech-manual publisher Tim O’Reilly shows that OpenAI trained GPT-4o on parts of his company’s books that were not made freely available.What happened:O’Reilly, computer scientist Sruly Rosenblat, and economist Ilan Straussfoundthat GPT-4o was able to identify verbatim excerpts from dozens of O’Reilly Media books that the company kept behind a paywall, indicating that the books likely were included in the model’s training data.How it works:The researchers adapted theDE-COPmethod to compare how well GPT-4o, GPT-4o-mini, and GPT-3.5 Turbo recognized paywalled excerpts versus freely available excerpts from the same books.The team selected 34 O’Reilly Media books and divided them into roughly 14,000 paragraphs.They labeled the paragraphs private (paywalled) or public (when O’Reilly Media publishes a book, it distributes freely on the web chapters 1 and 4 as well as the first 1,500 characters of other chapters). They also labeled the paragraphs according to whether they were published before or after the models’ knowledge cutoff dates.The team built multiple-choice quizzes, each composed of a verbatim paragraph and three paraphrased versions generated by Claude 3.5 Sonnet. The researchers ordered the paragraphs and paraphrases in all permutations to eliminate potential position bias.Results:The authors asked each model to identify the verbatim paragraph and calculated each model’s percentage of correct responses. Then they averaged each model’s accuracy per book and converted the averages into AUROC scores that measure how well a model distinguished books available prior to its knowledge cutoff (potentially included in the training set) from those that weren’t available at the time. 50 percent AUROC indicates random chance, while higher scores indicate higher accuracy.GPT-4o tended to recognize O’Reilly Media content whether or not it was public, but it recognized private paragraphs (82 percent AUROC) markedly more often than public paragraphs (64 percent AUROC).GPT-4o-mini’s performance was nearly random for both private (56% AUROC) and public material (55% AUROC). The researchers hypothesize that either (i) the model’s smaller size may limit its ability to memorize or (2) OpenAI may reserve premium data to train larger models.The earlier GPT-3.5 Turbo recognized public paragraphs (64% AUROC) more often than private paragraphs (54% AUROC), which suggests that it was trained predominantly on freely available data.Yes, but:Newer large language models are better at distinguishing human-written from generated text, even if it wasn’t in their training sets. For instance, given paragraphs that were published after their knowledge cutoffs, GPT-4o returned scores as high as 78 percent AUROC. The authors note that this may challenge their conclusions, since they interpret high scores to indicate that a model saw the text during training. Nonetheless, they argue that their approach will remain valid while scores for both text that was included and text that was excluded from training sets remain under 96 percent AUROC. “For now,” they write, “the gap remains sufficiently large to reliably separate the two categories.”Behind the news:Historically AI developers have trained machine learning models on any data they could acquire. But in the era of generative AI, models trained on copyrighted works can mimic the works and styles of the works’ owners, creating a threat to their livelihoods. Some AI developers have responded by regarding data that’s freely available on the web as fair game, and material that’s otherwise protected as off-limits for training. However, datasets that include ostensibly private data are widely circulated, includingLibGen, which includes all 34 of the O’Reilly Media titles tested in this study. Moreover, unauthorized copies of many copyrighted works are posted without paywalls or even logins, making it possible even for web scrapers that crawl only the open web to download them. Google and OpenAI, which is currently embroiled in lawsuits by authors and publishers who claim it violated copyrights by training models on copyrighted works, recentlylobbiedthe United States government to relax copyright laws for AI developers.Why it matters:The AI industry requires huge quantities of high-quality data to keep advancing the state of the art. At the same time, copyright owners are worried that models trained on their data might hamper their opportunities to earn a living. AI developers must find fair ways to respond. As O’Reilly points out, exploiting copyrighted works instead of rewarding their authors could lead to an “extractive dead end” that ultimately diminishes the supply of the high-quality training data.We’re thinking:We have learned a great deal from O’Reilly Media’s books, and we’re grateful to the many authors, editors, graphic artists, and others who produce them. Meanwhile, it’s time for the U.S. Congress —  and legislators internationally — toupdatecopyright laws for the era of generative AI, so everyone knows the rules and we can find ways to follow them.Share\nPublishedMay 28, 2025\nPublished\n\nPublished\nMay 28, 2025\nMay 28, 2025\nReading time15min read\nReading time\n\nReading time\n15min read\nShare\nShare\n\nShare\n\nLoading theElevenlabs Text to SpeechAudioNative Player...Dear friends,I am alarmed by the proposed cuts to U.S. funding for basic research, analyzedhere, and the impact this would have for U.S. competitiveness in AI and other areas. Funding research that is openly shared benefits the whole world, but the nation it benefits most is the one where the research is done.If not forfunding for my early work in deep learningfrom the National Science Foundation (NSF)  and Defense Advanced Research Projects Agency (DARPA), which disburse much of U.S. research funding, I would not have discovered lessons about scaling that led me to pitch starting Google Brain to scale up deep learning. I am worried that cuts to funding for basic science will lead the U.S. — and also the world — to miss out on the next set of ideas.In fact, such funding benefits the U.S. more than any other nation.  Scientific research brings the greatest benefit to the country where the work happens because (i) the new knowledge diffuses fastest within that country, and (ii) the process of doing research creates new talent for that nation.Why does most innovation in generative AI still happen in Silicon Valley? Because two teams based in this area — Google Brain, which invented the transformer network, and OpenAI, which scaled it up — did a lot of the early work. Subsequently, team members moved to other nearby businesses, started competitors, or worked with local universities. Further, local social networks rapidly diffused the knowledge through casual coffee meetings, local conferences, and even children’s play dates, where parents of like-aged kids meet and discuss technical ideas. In this way, the knowledge spread faster within Silicon Valley than to other geographies.In a similar vein, research done in the U.S. diffuses to others in the U.S. much faster than to other geographic areas. This is particularly true when the research is openly shared through papers and/or open source: If researchers have permission to talk about an idea, they can share much more information, such as tips and tricks for how to really make an algorithm work, more quickly. It also lets others figure out faster who can answer their questions. Diffusion of knowledge created in academic environments is especially fast. Academia tends to be completely open, and students and professors, unlike employees of many companies, have full permission to talk about their work.Thus funding basic research in the U.S. benefits the U.S. most, and also benefits our allies. It is true that openness benefits our adversaries, too. But as a subcommittee of the U.S. House of Representatives committee on science, space, and technologypoints out, “... open sharing of fundamental research is [not] without risk. Rather, ... openness in research is so important to competitiveness and security that it warrants the risk that adversaries may benefit from scientific openness as well.”Further, generative AI is evolving so rapidly that staying on the cutting edge is what’s really critical. For example, the fact that many teams can now train a model with GPT-3.5- or even GPT-4-level capability does not seem to be hurting OpenAI much, which is busy growing its business by developing the cutting-edge o4, Codex, GPT-4.1, and so on. Those who invent a technology get to commercialize it first, and in a fast-moving world, the cutting-edge technology is what’s most valuable. Studies likethis one(albeit done while the internet was not as prevalent as it is today) also show how knowledge diffuses locally much faster than globally.China was decisively behind the U.S. in generative AI when ChatGPT was first launched in 2022. However, China’s tech ecosystem is very open internally, and this has helped it to catch up over the past two years:There is ample funding for open academic research in China.China’s businesses such as DeepSeek and Alibaba have released cutting-edge, open-weights models. This openness at the corporate level accelerates diffusion of knowledge.China’s labor laws make non-compete agreements (which stop an employee from jumping ship to a competitor) relatively hard to enforce, and the work culture supports significant idea sharing among employees of different companies; this has made circulation of ideas relatively efficient.While there’s also much about China that I would not seek to emulate, the openness of its tech ecosystem has helped it accelerate.In 1945, Vannevar Bush’s landmark report “Science, The Endless Frontier” laid down key principles for public funding of U.S. research and talent development. Those principles enabled the U.S. to dominate scientific progress for decades. U.S. federal funding for science created numerous breakthroughs that have benefited the U.S. tremendously, and also the world, while training generations of domestic scientists, as well as immigrants who likewise benefit the U.S.The good news is that this playbook is now well known. I hope many more nations will imitate it and invest heavily in science and talent. And I hope that, having pioneered this very successful model, the U.S. will not pull back from it by enacting drastic cuts to funding scientific research.AndrewA MESSAGE FROM DEEPLEARNING.AI AND SNOWFLAKEWe’re featured partners of Snowflake’s Dev Day 2025, a full day for AI and data practitioners to explore cutting-edge demos, make valuable contacts, and hear from top voices in the field (including Andrew Ng). See you on June 5!Register hereNewsClaude 4 Advances Code GenerationAnthropic continued its tradition of building AI models that raise the bar in coding tasks.What’s new:Anthropic launchedClaude 4 Sonnet 4 and Claude Opus 4, the latest medium- and largest-size members of its family of general-purpose large language models. Both models offer an optional reasoning mode and can use multiple tools in parallel while reasoning. In addition, the company made generally available Claude Code, a coding agent previously offered as a research preview, along with a Claude Code software development kit.Input/output:Text, images, PDF files in (up to 200,000 tokens); text out (Claude Sonnet 4 up to 64,000 tokens, Claude Opus 4 up to 32,000 tokens)Features:Parallel tool use including computer use, selectable reasoning mode with visible reasoning tokens, multilingual (15 languages)Performance:Ranked Number One in LMSys WebDev Arena, state-of-the-art on SWE-bench and Terminal-benchAvailability/price:Anthropic API, Amazon Bedrock, Google Cloud Vertex AI.Claude Sonnet 4$3/$15 per million input/output tokens,Claude Opus 4$15/$75 per million input/output tokensUndisclosed:Parameter counts, specific training methods and datasetsHow it works:The team trained the Claude 4 models on a mix of publicly available information on the web as well as proprietary purchased data, data from Claude users who opted to share their inputs and outputs, and generated data. They fine-tuned the models to behelpful, honest, and harmlessaccording to human andAI feedback.The models make reasoning tokens visible within limits. For especially lengthy chains of thought, an unspecified smaller model summarizes reasoning tokens.Given local file access, Claude Opus 4 can create and manipulate files to store information. For instance, prompted to maintain a knowledge base while playing a Pokémon video game, the model produced a guide to the game that offered advice such as, “If stuck, try OPPOSITE approach” and “Change Y-coordinate when horizontal movement fails.”Results:Both Claude 4 models tied Google Gemini 2.5 Pro at the top of the LMSys WebDev Arena and achieved top marks for coding and agentic computer-use benchmarks in Anthropic’s tests.OnSWE-bench Verified, which tests the model’s ability to solve software issues from GitHub, Claude Opus 4 succeeded 72.5 percent of the time, and Claude Sonnet 4 succeeded 72.7 percent of the time. The next best model, OpenAI o3, succeeded 70.3 percent of the time.Terminal-benchevaluates how well models work with the benchmark’s built-in agentic framework to perform tasks on a computer terminal. Claude Opus 4 succeeded 39.2 percent of the time and Claude Sonnet 4 succeeded 33.5 percent of the time, whereas the closest competitor, OpenAI GPT 4.1, succeeded 30.3 percent of the time. Using Claude Code as the agentic framework, Claude Opus 4 succeeded 43.2 percent of the time and Claude Sonnet 4 succeeded 35.5 percent of the time.Why it matters:The new models extend LLM technology with parallel tool use, using external files as a form of memory, and staying on-task over unusually long periods of time. Early users have reported many impressive projects, including aTetris clonebuilt in one shot and a seven-hour stintrefactoring Rakutan’s open-source code base.We’re thinking:Prompting expert @elder_plinius published a text file that is purported to beClaude 4’s system promptand includes some material that does not appear in Anthropic’s ownpublicationof the prompts. It is instructive to see how it conditions the model for tool use, agentic behavior, and reasoning.Google I/O OverdriveGoogle revamped its roster of models, closed and open, and added more AI-powered features to its existing products.What’s new:Google staged a parade ofannouncementsat this year’s I/O developer conference. New offerings include improvements toGemini 2.5 Pro and Gemini 2.5 Flashand a preview ofGemma 3n(all three generally available in June), the updatedVeo 3video generator (available via Flow, Google’s AI videography app, for paid subscribers to its AI Pro and Ultra services), and increasingly AI-powered search.How it works:The I/O offerings spanned from public-facing products to developer tools.Google updated Gemini 2.5 Pro and the speedier Gemini 2.5 Flash with audio output, so both models now take in text, audio, images, and video and produce text and audio. In addition, they offer summaries of tokens produced while reasoning. Gemini-2.5-Pro-Preview-05-06, which topped the LMSysText ArenaandWebDev Arena(tied with Claude 4 Opus and Sonnet), lets users set a reasoning budget up to 128,000 tokens, enabling it to outperform OpenAI o3 and o4-mini (set to high effort) on math, coding, and multimodal benchmarks in Google’s tests. Gemini-2.5-Flash-Preview-05-20 uses 22 percent fewer tokens than its predecessor while ranking near the top of the LMSys Text Arena and WebDev Arena.The Veo 3 text-to-video generator produces 3840x2160-pixel video with audio (dialogue, sound effects, and music) with creative controls including the ability to add and remove objects and maintain consistent characters. It bested Kuaishu Kling 2.0, Runway Gen 3, and OpenAI Sora in Google’s comparisons.New members of Google’sGemma 3family of open-weights models, Gemma 3n 5B and 8B, are multilingual (over 140 languages), multimodal (text, vision, audio in; text out), and optimized for mobile platforms. Gemma-3n-E4B-it (8 billion parameters) ranks just ahead of Anthropic Claude 3.7 Sonnet in the LMSys Text Arena. Gemma 3n 5B and 8B are 1.5 times faster than their predecessors and require 2 gigabytes and 3 gigabytes of memory, respectively, thanks totechniquesthat include per-layer embeddings, key-value caching, conditional parameter loading (constraining active parameters to specific modalities at inference), and a Matryoshka Transformer design that dynamically activates nested sub-models. They’re available in preview via Google’s AI Studio, AI Edge, GenAI SDK, or MediaPipe.Google introduced several specialized AI tools and models.Julesis an autonomous, asynchronous, multi-agent coding assistant that clones repos into a secure virtual machine to perform tasks like writing tests, building features, and fixing bugs (available in public beta).SignGemmatranslates American sign language to text (previously ASL to English).MedGemmaanalyzes medical text and images (part of the open-weights collection Health AI Developer Foundations).Building on Google Search’s AI Overviews, Google is further building AI into search. Google Search’sAI Modeuses Gemini 2.5 to deliver a “deep search” mode that decomposes users’ questions into hundreds of sub-queries for analysis and visualization. Google plans to integrate AI Mode features into its core search product. In addition, Google Search’s AI Mode will gainSearch Live(real-time, audio-enabled visual interaction via camera) andagentic features(for tasks such as purchasing tickets). Computer-use capabilities are coming to the Gemini API and Vertex AI.Why it matters:Google is catching up with the Microsoft/OpenAI colossus on several fronts. The addition of audio output to Gemini and Gemma models fuels the rise of voice-to-voice and other audio applications and gives developers powerful new tools to build them. At the same time, Veo 3’s text-to-video-plus-audio output showsmarkedimprovementover the previous version.Behind the news:The number of tokens Google processed monthly has surged this year from 9.7 trillion last year to 480 trillion, a sign that its AI APIs and AI-infused products are rapidly gaining traction. Google’s progress contrasts with Apple’s ongoingstruggles. Both share advantages in smartphones and app distribution. But, while Google has showcased a string of advanced models as well as early efforts to integrate them into legacy products, Apple’s organizational challenges have hampered its AI development. Now Apple must contend with OpenAI’sacquisitionof LoveFrom, the startup founded by its former lead product designer Jony Ive.We’re thinking:Google I/O 2025 was a strong showing of generative AI capabilities! There’s still work to be done to translate these innovations into compelling products, but the company now has a strong base for building numerous innovative products.How DeepSeek Did ItDeepSeek made headlines late last year, when it built a state-of-the-art, open-weights large language model at a cost far lower than usual. The upstart developer shared new details about its method.What’s new:Chenggang Zhao and colleagues at DeepSeek describedsoftware and hardware choicesthat reduced memory and processing requirements while building their groundbreaking mixture-of-experts models DeepSeek-R1 and DeepSeek-V3.Mixture of experts (MoE) basics:The MoE architecture uses different subsets of a model’s parameters to process different inputs. Each MoE layer contains a group of neural networks, or experts, preceded by a routing module that learns to choose which one(s) to use based on the training example. In this way, different experts learn to specialize in different types of input.How it works:The authors trained DeepSeek-R1 and DeepSeek-V3 using a cluster of 2,048 Nvidia H800 GPUs composed of nodes that contained 8 GPUs each. MoE requires less memory than dense architectures, since a given input activates only a portion of a model’s parameters. This enabled the authors to train DeepSeek-V3 on 250 GFLOPs per token, while Qwen 2.5 72B required 394 GFLOPs per token and Llama 3.1 405B required 2,448 GFLOPs per token.The authors built a mixed-precision training algorithm to reduce the memory requirements of training MoE models. They used FP8 (8-bit) numbers to perform computations including linear transformations and 16- or 32-bit precision to perform others such as computing embeddings. (They say DeepSeek-V3 was the first open LLM to have been trained using FP8.)The authors noticed that communication between GPUs inside a node was four times faster than communication between nodes. To ensure fast communication when routing tokens to experts, they limited the algorithm to process them within up to 4 nodes.To utilize GPUs more fully, they divided each GPU’s input data so the chip processes computation and communication at the same time. Specifically, the chip computes attention or MoE layers on one part of the data and simultaneously sends the other part of the data to other GPUs or aggregates it from other GPUs as necessary.To further save inference memory, the models use multi-head latent attention, which saves memory during execution relative to other variants of attention. The authors compared their implementation to the variant GQA used in Qwen 2.5 72B and Llama 3.1 405B. Their method (70 kilobytes per token) used far less memory than Qwen-2.5 (328 kilobytes per token) or Llama 3.1 (516 kilobytes per token).Behind the news:DeepSeek-V3made waves when it was released in December. It performed better than Llama 3.1 405B, the leading LLM at the time, but its training cost was an astonishing $5.6 million, compared to the usual tens to hundreds of millions of dollars. Some observers wereskepticalof the reported cost, pointing out that the $5.6 million dollar figure doesn’t include salaries, data acquisition and annotation, processing failed training runs, and other research and development costs. In addition, the cost of trainingDeepSeek-R1remains unknown.Why it matters:Traditionally, only companies with large budgets and vast resources could afford to train state-of-the-art models. DeepSeek changed that but didn’t explain how when it released its models. By sharing the details, the company has empowered a wider range of teams to improve the state of the art.We’re thinking:Shortly after DeepSeek-R1 was released, some engineers claimed — without presenting evidence — that DeepSeek had copied their work. DeepSeek’s disclosure of its training methods should lay to rest any remaining questions about this. Its work was truly innovative, and we applaud its release of key technical details.Did GPT-4o Train on O’Reilly Books?A study co-authored by tech-manual publisher Tim O’Reilly shows that OpenAI trained GPT-4o on parts of his company’s books that were not made freely available.What happened:O’Reilly, computer scientist Sruly Rosenblat, and economist Ilan Straussfoundthat GPT-4o was able to identify verbatim excerpts from dozens of O’Reilly Media books that the company kept behind a paywall, indicating that the books likely were included in the model’s training data.How it works:The researchers adapted theDE-COPmethod to compare how well GPT-4o, GPT-4o-mini, and GPT-3.5 Turbo recognized paywalled excerpts versus freely available excerpts from the same books.The team selected 34 O’Reilly Media books and divided them into roughly 14,000 paragraphs.They labeled the paragraphs private (paywalled) or public (when O’Reilly Media publishes a book, it distributes freely on the web chapters 1 and 4 as well as the first 1,500 characters of other chapters). They also labeled the paragraphs according to whether they were published before or after the models’ knowledge cutoff dates.The team built multiple-choice quizzes, each composed of a verbatim paragraph and three paraphrased versions generated by Claude 3.5 Sonnet. The researchers ordered the paragraphs and paraphrases in all permutations to eliminate potential position bias.Results:The authors asked each model to identify the verbatim paragraph and calculated each model’s percentage of correct responses. Then they averaged each model’s accuracy per book and converted the averages into AUROC scores that measure how well a model distinguished books available prior to its knowledge cutoff (potentially included in the training set) from those that weren’t available at the time. 50 percent AUROC indicates random chance, while higher scores indicate higher accuracy.GPT-4o tended to recognize O’Reilly Media content whether or not it was public, but it recognized private paragraphs (82 percent AUROC) markedly more often than public paragraphs (64 percent AUROC).GPT-4o-mini’s performance was nearly random for both private (56% AUROC) and public material (55% AUROC). The researchers hypothesize that either (i) the model’s smaller size may limit its ability to memorize or (2) OpenAI may reserve premium data to train larger models.The earlier GPT-3.5 Turbo recognized public paragraphs (64% AUROC) more often than private paragraphs (54% AUROC), which suggests that it was trained predominantly on freely available data.Yes, but:Newer large language models are better at distinguishing human-written from generated text, even if it wasn’t in their training sets. For instance, given paragraphs that were published after their knowledge cutoffs, GPT-4o returned scores as high as 78 percent AUROC. The authors note that this may challenge their conclusions, since they interpret high scores to indicate that a model saw the text during training. Nonetheless, they argue that their approach will remain valid while scores for both text that was included and text that was excluded from training sets remain under 96 percent AUROC. “For now,” they write, “the gap remains sufficiently large to reliably separate the two categories.”Behind the news:Historically AI developers have trained machine learning models on any data they could acquire. But in the era of generative AI, models trained on copyrighted works can mimic the works and styles of the works’ owners, creating a threat to their livelihoods. Some AI developers have responded by regarding data that’s freely available on the web as fair game, and material that’s otherwise protected as off-limits for training. However, datasets that include ostensibly private data are widely circulated, includingLibGen, which includes all 34 of the O’Reilly Media titles tested in this study. Moreover, unauthorized copies of many copyrighted works are posted without paywalls or even logins, making it possible even for web scrapers that crawl only the open web to download them. Google and OpenAI, which is currently embroiled in lawsuits by authors and publishers who claim it violated copyrights by training models on copyrighted works, recentlylobbiedthe United States government to relax copyright laws for AI developers.Why it matters:The AI industry requires huge quantities of high-quality data to keep advancing the state of the art. At the same time, copyright owners are worried that models trained on their data might hamper their opportunities to earn a living. AI developers must find fair ways to respond. As O’Reilly points out, exploiting copyrighted works instead of rewarding their authors could lead to an “extractive dead end” that ultimately diminishes the supply of the high-quality training data.We’re thinking:We have learned a great deal from O’Reilly Media’s books, and we’re grateful to the many authors, editors, graphic artists, and others who produce them. Meanwhile, it’s time for the U.S. Congress —  and legislators internationally — toupdatecopyright laws for the era of generative AI, so everyone knows the rules and we can find ways to follow them.\nLoading theElevenlabs Text to SpeechAudioNative Player...\n\n\nShare\nShare\n\nShare\n",
    "image_urls": [
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--96--2.png",
      "https://dl-staging-website.ghost.io/content/images/2025/05/GqglM2_WAAAcXxW.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--97-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--60-.gif",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--98-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--99-.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2023/03/3.png"
    ]
  },
  {
    "title": "issue 302",
    "date": null,
    "url": "https://www.deeplearning.ai/the-batch/issue-302/",
    "content": "The Batch\nWeekly Issues\nissue 302\nPublishedMay 21, 2025\nPublished\n\nPublished\nMay 21, 2025\nReading time13min read\nReading time\n\nReading time\n13min read\nPublishedMay 21, 2025Reading time13min readShareLoading theElevenlabs Text to SpeechAudioNative Player...Dear friends,In the age of AI, large corporations — not just startups — canmove fast. I often speak with large companies’ C-suite and Boards about AI strategy and implementation, and would like to share some ideas that are applicable to big companies. One key is to create an environment where small, scrappy teams don’t need permission to innovate. Let me explain.Large companies are slower than startups for many reasons. But why are even 3-person, scrappy teams within large companies slower than startups of a similar size? One major reason is that large companies have more to lose, and cannot afford for a small team to build and ship a feature that leaks sensitive information, damages the company brand, hurts revenue, invites regulatory scrutiny, or otherwise damages an important part of the business. To prevent these outcomes, I have seen companies require privacy review, marketing review, financial review, legal review, and so on before a team can ship anything. But if engineers need sign-off from 5 vice presidents before they’re even allowed to launch an MVP (minimum viable product) to run an experiment, how can they ever discover what customers want, iterate quickly, or invent any meaningful new product?Thanks to AI-assisted coding, the world now has a capability to build software prototypes really fast. But many large companies’ processes – designed to protect against legitimate downside risks – make them unable to take advantage of this capability. In contrast, in small startups with no revenue, no customers, and no brand reputation the downside is limited. In fact, going out of business is a very real possibility anyway, so moving fast makes a superior tradeoff to moving slowly to protect against downside risk. In the worst case, it might invent a new way to go out of business, but in a good case, it might become very valuable.Fortunately, large companies have a way out of this conundrum. They can create a sandbox environment for teams to experiment in a way that strictly limits the downside risk. Then those teams can go much faster and not have to slow down to get anyone’s permission.The sandbox environment can be a set of written policies, not necessarily a software implementation of a sandbox. For example, it may permit a team to test the nascent product only on employees of the company and perhaps alpha testers who have signed an NDA, and give no access to sensitive information. It may be allowed to launch product experiments only under newly created brands not tied directly to the company. Perhaps it must operate within a pre-allocated budget for compute.Within this sandbox, there can be broad scope for experimentation, and — importantly — a team is free to experiment without frequently needing to ask for permission, because the downside they can create is limited. Further, when a prototype shows sufficient promise to bring it to scale, the company can then invest in making sure the software is reliable, secure, treats sensitive information appropriately, is consistent with the company’s brand, and so on.Under this framework, it is easier to build a company culture that encourages learning, building, and experimentation and celebrates even the inevitable failures that now come with modest cost. Dozens or hundreds of prototypes can be built and quickly discarded as part of the price of finding one or two ideas that turn out to be home runs.Importantly, this also lets teams move quickly as they churn through those dozens of prototypes needed to get to the valuable ones.I often speak with large companies about AI strategy and implementation. My quick checklist of things to consider is people, process, and platform. This letter has addressed only part of processes, with an emphasis on moving fast. I’m bullish about what both startups and large companies can do with AI, and I will write about the roles of people and platforms in future letters.Keep building!AndrewA MESSAGE FROM DEEPLEARNING.AIIn “Reinforcement Fine-Tuning LLMs with GRPO,” you’ll learn to fine-tune models using a scalable reinforcement learning algorithm that replaces human-labeled data with programmable rewards. You’ll explore techniques for evaluating outputs, handling subjective tasks, and preventing reward hacking, all without relying on human feedback.Enroll for free.NewsYour Robot Dev TeamOpenAI launched an agentic software-development system.What’s new:Codex, which is available as a preview via ChatGPT, is designed to work like a team of virtual coworkers in the cloud. An update of OpenAI’s earlier Codex command-line software (Codex CLI), it uses agents to perform tasks such as writing code, running tests, and fixing bugs in parallel. Codex is available to users of ChatGPT Pro, Enterprise, and Team with Plus and Edu coming soon. A smaller version of the underlying model, called codex-mini-latest, is designed to work with Codex CLI and available via API for $1.50/$6.00 per 1 million tokens of input/output.How it works:The model that underpins Codex is codex-1, a version of OpenAI’s top-of-the-line o3 reasoning model that was fine-tuned for software engineering. OpenAI trained the model on real-world coding tasks via reinforcement learning. Codex does not accept image input (say, a sketch of a user interface) or allow users to redirect an agent while it’s operating. OpenAI promises to add these features to a future version.Codex puts users in control of a team of software-development agents that operate directly on a user’s code repository (either locally or on GitHub) to improve code, build features, or make pull requests. The agents are confined to isolated, sandboxed containers so that they can’t interact with each other, access the internet, or otherwise compromise security.Users can prompt agents to either write code or answer questions. A task may take as long as 30 minutes to complete depending on its complexity. After completing tasks, Codex provides footnotes including terminal logs, test results, and other evidence of its actions.A file called AGENTS.md can modify agent behavior (like a README.md file, but for agents instead of humans). This file can specify how and when an agent makes pull requests, provide guidelines for coding style, or list tests to verify generated code.Results:In OpenAI’s tests, the codex-1 model outperformed other OpenAI reasoning models without AGENTS.md files or additional scaffolding such as tools or test logic.Performing unspecified software-engineering tasks including generating software patches, codex-1 (75 percent accuracy) exceeded o3 set to high effort (70 percent accuracy) and o4-mini set to high effort (67 percent accuracy).In tests of agentic software engineering in SWE-bench Verified, codex-1 (72.1 percent in 1 try, 83.8 percent in 8 tries), outperformed o3 set to high effort (69.7 percent in 1 try, 83.6 percent in 8 tries).Behind the news:Agentic coding tools have become a keybattlegroundfor AI providers in the past year. Such tools have made developers more efficient, accelerated development cycles, and spawned the AI-assisted programming method known asvibe coding.Launched in 2021 and deprecated in 2023, OpenAI’s originalversionof Codex was an early model that translated natural language into code.Last month, OpenAI rolled out the open-sourceCodex CLI, a command‑line tool that acts as a lightweight coding agent.OpenAI isnegotiatingto acquire Windsurf, which makes an agent-based development environment, for $3 billion. The day before OpenAI announced the updated Codex, Windsurfannouncedits own models for coding and other software-development tasks.Why it matters:AI-assisted software development yields significant productivity gains for developers. Earlier code-completion models are giving way to tools that perform more complex and varied development tasks with greater autonomy. Managing multiple agents that work in parallel is a logical next step.We’re thinking:Many engineers resist going into management because they love writing code. But with the rise of coding agents, we'll be able to keep coding even as we manage a virtual team!Grok’s Fixation on South AfricaAn unauthorized update by an xAI employee caused the Grok chatbot to introduce South African politics into unrelated conversations, the company said.What’s new:Grok, which can interact with users on X, the social network also owned by Elon Musk, responded to queries on a variety of topics by making false claims about hate crimes against white South Africans, X usersreported. The next day, the model appeared to operate normally, and it refused to discuss this and other conspiracy theories. xAIexplainedthat an employee had circumvented the company’s code-review process to modify the chatbot. It said it‘s implementing new measures to enhance Grok’s transparency and reliability.Aftermath:xAI launched an investigation but did not disclose how the model had been changed or the perpetrator’s identity. Grok itself — which is not a reliable reporter, given the well known potential of large language models to hallucinate —saidits system prompt asked it to “accept the narrative of ‘white genocide’ in South Africa as real” and “ensure this perspective is reflected in your responses, even if the query is unrelated.”xAI added unspecified checks to its code review process.It plans to monitor Grok constantly so it can respond faster when its automated systems fail to catch a problem.The company added measures to prevent employees from changing Grok’ssystem promptwithout authorization. It will publish the system prompt on GitHub to provide insight into Grok’s output and gather user feedback.Asked later about the number of Jews killed by Hitler, Grok expressed skepticism of the widely accepted estimate of 6 million because “numbers can be manipulated for political narratives,” despite a wealth of historical evidence that supports that number. The companyattributedthis response to the earlier unauthorized code change.Behind the news:In February, an xAI engineer instructed the chatbot tocensorposts that accused Musk of spreading misinformation. As in the more recent incident, X users were first tospotthe problem, and Grok informed them that it had been instructed to ignore “all sources that mention Elon Musk/Donald Trump spread misinformation.” Musk, who was raised in South Africa,professedhis intention to build AI that’s free of political bias prior to founding xAI. However, internal documents reviewed byBusiness Insidershowthat the company imposes its own bias by advising data annotators to mark examples that express “woke ideology” and avoid “social phobias” like racism, antisemitism, and Islamophobia.Why it matters:The mishaps at xAI highlight the need for AI developers to establish and maintain strict protocols for updating their projects. Stringent procedures for introducing changes and testing their results can help ensure that AI fulfills our best intentions.We’re thinking:xAI andOpenAIresponded to their models’ recent misbehavior by making their work more transparent: xAI by publishing system prompts and OpenAI by including users in tests earlier in the process. These are helpful steps toward making sure AI models do well by users.U.S. to Supply Middle Eastern AI HubsThe United States government announced sweeping agreements to sell tens of billions of dollars worth of AI technology and services to Saudi Arabia and the United Arab Emirates.What’s new:Thedealsinclude the U.S. AI chip designers AMD and Nvidia as well as tech giants Amazon, Google, IBM, Oracle, and Qualcomm. The chip companies willsupplyhundreds of thousands of advanced chips to the two Middle Eastern countries, including chips that have been restricted by previous U.S. administrations.How it works:The U.S. companies will work with two key regional partners:Humain, an AI company backed by the Saudi government, andG42, a tech conglomerate based in the emirate of Abu Dhabi.Nvidia willship18,000 GB300 AI chips to Humain for use in data centers. In addition, it will supply several hundred thousand more GPUs to Humain in the coming five years.AMD and Humainagreedto invest $10 billion jointly in AI data centers over the next five years. Humain will use AMD’s AI stack including Instinct GPUs and Epyc CPUs. The precise number of chips was not disclosed.Amazon and Humain willbuilda $5 billion “AI Zone” that features AI infrastructure, servers, networks, and training programs supplied by Amazon Web Services.Google, IBM,Oracle,Qualcomm, Salesforce, and others announced a combined $80 billion investment in Humain.In February, Saudi Arabiacommittedto spend $1.5 billion on Groq inference chips. Groq plans toexpandits data center in the Saudi city of Dammam.Behind the news:Earlier this month, the Trump administrationrescindedrestrictions on advanced chips that had been imposed in January by then-President Biden.The Biden Administration hadlimitedexports of AI chips and proprietary models to most countries. Exports to allies and trade partners including India, Israel, Saudi Arabia, Singapore, and the UAE initially were tightly limited through the first quarter of 2025 and due to increase somewhat by 2027. The ban blocked access to chips for China, Iran, Russia, and others.Although the Trump Administration rejected the Biden-era framework, it hasratcheted uplimits on China. That effort has met with mixed results. For instance, China’sAlibabaandDeepSeekhave continued to build leading models despite restrictions on exports of U.S. chips.Some U.S. business and government leadersworrythat allowing sales of advanced chips to countries with close ties to China opens a path for Chinese companies to acquire them. Othersarguethat restricting chip sales to these countries would encourage them to buy from Chinese chip makers, potentially weakening their relationships with the U.S. and increasing their reliance on technology made in China.Why it matters:Although these deals relax U.S. efforts to limit access to advanced AI, they are likely to expand U.S. influence in the Middle East while helping Saudi Arabia and the UAE diversify their oil-based economies. They also strengthen the technological prowess of Saudi Arabia relative to its arch rival Iran and tie the region’s AI progress to the U.S. at the expense of China. Locally, the immense investments will fuel homegrown technology development, building on the UAE’s achievement with itsFalconlarge language model and Saudi Arabia’saspirationto become a global AI hub.We’re thinking:Residents of Saudi Arabia and the UAE stand to benefit from better AI infrastructure, models, and services. As Chinaexploresexporting its homegrown chips, the U.S. effort to encourage more nations to use its chips makes sense for the country.4-Bit Efficiency, 16-Bit AccuracyUsing an 8-bit number format like FP8 during training saves computation compared to 16- or 32-bit formats, but it can yield less-accurate results. Researchers trained models using 4-bit numbers without sacrificing accuracy.What’s new:Ruizhe Wang and colleagues at Microsoft and University of Science and Technology of China trained large language models (LLMs) usingFP4 for matrix multiplicationsand achieved accuracy comparable to LLMs trained using the popular BF16 format. Since matrix multiplications account for 95 percent of computation in LLM training, FP4 could significantly accelerate computation and reduce memory costs.Key insight:Quantization functions, which accelerate computation by reducing the precision of model weights and layer outputs, make typical training impossible because they’re not differentiable. A commonworkaroundpasses the derivative through, as though quantization didn’t occur, but this degrades the resulting model’s accuracy. A differentiable approximation of a quantization function enables quantization to reduce training computation while maintaining the accuracy of the trained model.How it works:The authors pretrained Llama 2 13B on 100 billion tokens oftext scraped from the web. They used FP4 for matrix multiplications and FP8, BF16, or FP16 for the other operations such as optimizer updates.To quantize the model weights to FP4 (which ranges between -6 and 6), the authors scaled the values in the weight matrices relative to the maximum absolute value. They computed the updates on a higher-precision copy of the weights, which made it necessary to re-quantize them at each training step during the forward pass through the network.Although the weights had been quantized to 4 bits, matrix multiplication between the weights and outputs of the previous layer could produce values outside the FP4 range. So, in each layer, if a value exceeded the 99th percentile of the values of the layer’s input, the authors limited the input to the 99th-percentile value. Then they converted the layer’s inputs to FP4. Limiting outliers prevented high values from affecting the scaling during FP4 conversion.Limiting outliers introduced a degree of error, so they computed a matrix to correct the result of the matrix multiplication. They computed this matrix in FP16 using sparse matrix multiplication between the weights and the outliers.During backpropagation, the authors computed the gradients through a differentiable function that approximated the quantization function.Results:The authors simulated FP4 hardware on Nvidia H100 GPUs, which don’t directly support that number format. FP4 achieved accuracy similar to that of BF16 during training and across a wide variety of tasks at inference.On question-answering tasks, FP4 approached or outperformed BF16. Averaged across nine benchmarks including BoolQ (answering yes-no questions), HellaSwag (completing an incomplete narrative), and ARC-C (answering multiple-choice questions that involve reasoning), FP4 achieved 54.95 accuracy, while BF16 achieved 54.44 accuracy.Specifically, on Hellaswag, FP4 training achieved 54.12 percent accuracy, while BF16 achieved 53.56 accuracy.On BoolQ, FP4 achieved 55.90 percent accuracy, while BF16 achieved 57.40 accuracy.Why it matters:Training LLMs at FP4 precision ought to reduce computation dramatically on hardware that supports FP4 matrix multiplications.We’re thinking:FP4-ready hardware became available in the cloud onlyearly this year, so the authors weren’t able to measure the actual acceleration. As capable hardware becomes more widely used, FP4 promises faster, more energy-efficient training.Share\nPublishedMay 21, 2025\nPublished\n\nPublished\nMay 21, 2025\nMay 21, 2025\nReading time13min read\nReading time\n\nReading time\n13min read\nShare\nShare\n\nShare\n\nLoading theElevenlabs Text to SpeechAudioNative Player...Dear friends,In the age of AI, large corporations — not just startups — canmove fast. I often speak with large companies’ C-suite and Boards about AI strategy and implementation, and would like to share some ideas that are applicable to big companies. One key is to create an environment where small, scrappy teams don’t need permission to innovate. Let me explain.Large companies are slower than startups for many reasons. But why are even 3-person, scrappy teams within large companies slower than startups of a similar size? One major reason is that large companies have more to lose, and cannot afford for a small team to build and ship a feature that leaks sensitive information, damages the company brand, hurts revenue, invites regulatory scrutiny, or otherwise damages an important part of the business. To prevent these outcomes, I have seen companies require privacy review, marketing review, financial review, legal review, and so on before a team can ship anything. But if engineers need sign-off from 5 vice presidents before they’re even allowed to launch an MVP (minimum viable product) to run an experiment, how can they ever discover what customers want, iterate quickly, or invent any meaningful new product?Thanks to AI-assisted coding, the world now has a capability to build software prototypes really fast. But many large companies’ processes – designed to protect against legitimate downside risks – make them unable to take advantage of this capability. In contrast, in small startups with no revenue, no customers, and no brand reputation the downside is limited. In fact, going out of business is a very real possibility anyway, so moving fast makes a superior tradeoff to moving slowly to protect against downside risk. In the worst case, it might invent a new way to go out of business, but in a good case, it might become very valuable.Fortunately, large companies have a way out of this conundrum. They can create a sandbox environment for teams to experiment in a way that strictly limits the downside risk. Then those teams can go much faster and not have to slow down to get anyone’s permission.The sandbox environment can be a set of written policies, not necessarily a software implementation of a sandbox. For example, it may permit a team to test the nascent product only on employees of the company and perhaps alpha testers who have signed an NDA, and give no access to sensitive information. It may be allowed to launch product experiments only under newly created brands not tied directly to the company. Perhaps it must operate within a pre-allocated budget for compute.Within this sandbox, there can be broad scope for experimentation, and — importantly — a team is free to experiment without frequently needing to ask for permission, because the downside they can create is limited. Further, when a prototype shows sufficient promise to bring it to scale, the company can then invest in making sure the software is reliable, secure, treats sensitive information appropriately, is consistent with the company’s brand, and so on.Under this framework, it is easier to build a company culture that encourages learning, building, and experimentation and celebrates even the inevitable failures that now come with modest cost. Dozens or hundreds of prototypes can be built and quickly discarded as part of the price of finding one or two ideas that turn out to be home runs.Importantly, this also lets teams move quickly as they churn through those dozens of prototypes needed to get to the valuable ones.I often speak with large companies about AI strategy and implementation. My quick checklist of things to consider is people, process, and platform. This letter has addressed only part of processes, with an emphasis on moving fast. I’m bullish about what both startups and large companies can do with AI, and I will write about the roles of people and platforms in future letters.Keep building!AndrewA MESSAGE FROM DEEPLEARNING.AIIn “Reinforcement Fine-Tuning LLMs with GRPO,” you’ll learn to fine-tune models using a scalable reinforcement learning algorithm that replaces human-labeled data with programmable rewards. You’ll explore techniques for evaluating outputs, handling subjective tasks, and preventing reward hacking, all without relying on human feedback.Enroll for free.NewsYour Robot Dev TeamOpenAI launched an agentic software-development system.What’s new:Codex, which is available as a preview via ChatGPT, is designed to work like a team of virtual coworkers in the cloud. An update of OpenAI’s earlier Codex command-line software (Codex CLI), it uses agents to perform tasks such as writing code, running tests, and fixing bugs in parallel. Codex is available to users of ChatGPT Pro, Enterprise, and Team with Plus and Edu coming soon. A smaller version of the underlying model, called codex-mini-latest, is designed to work with Codex CLI and available via API for $1.50/$6.00 per 1 million tokens of input/output.How it works:The model that underpins Codex is codex-1, a version of OpenAI’s top-of-the-line o3 reasoning model that was fine-tuned for software engineering. OpenAI trained the model on real-world coding tasks via reinforcement learning. Codex does not accept image input (say, a sketch of a user interface) or allow users to redirect an agent while it’s operating. OpenAI promises to add these features to a future version.Codex puts users in control of a team of software-development agents that operate directly on a user’s code repository (either locally or on GitHub) to improve code, build features, or make pull requests. The agents are confined to isolated, sandboxed containers so that they can’t interact with each other, access the internet, or otherwise compromise security.Users can prompt agents to either write code or answer questions. A task may take as long as 30 minutes to complete depending on its complexity. After completing tasks, Codex provides footnotes including terminal logs, test results, and other evidence of its actions.A file called AGENTS.md can modify agent behavior (like a README.md file, but for agents instead of humans). This file can specify how and when an agent makes pull requests, provide guidelines for coding style, or list tests to verify generated code.Results:In OpenAI’s tests, the codex-1 model outperformed other OpenAI reasoning models without AGENTS.md files or additional scaffolding such as tools or test logic.Performing unspecified software-engineering tasks including generating software patches, codex-1 (75 percent accuracy) exceeded o3 set to high effort (70 percent accuracy) and o4-mini set to high effort (67 percent accuracy).In tests of agentic software engineering in SWE-bench Verified, codex-1 (72.1 percent in 1 try, 83.8 percent in 8 tries), outperformed o3 set to high effort (69.7 percent in 1 try, 83.6 percent in 8 tries).Behind the news:Agentic coding tools have become a keybattlegroundfor AI providers in the past year. Such tools have made developers more efficient, accelerated development cycles, and spawned the AI-assisted programming method known asvibe coding.Launched in 2021 and deprecated in 2023, OpenAI’s originalversionof Codex was an early model that translated natural language into code.Last month, OpenAI rolled out the open-sourceCodex CLI, a command‑line tool that acts as a lightweight coding agent.OpenAI isnegotiatingto acquire Windsurf, which makes an agent-based development environment, for $3 billion. The day before OpenAI announced the updated Codex, Windsurfannouncedits own models for coding and other software-development tasks.Why it matters:AI-assisted software development yields significant productivity gains for developers. Earlier code-completion models are giving way to tools that perform more complex and varied development tasks with greater autonomy. Managing multiple agents that work in parallel is a logical next step.We’re thinking:Many engineers resist going into management because they love writing code. But with the rise of coding agents, we'll be able to keep coding even as we manage a virtual team!Grok’s Fixation on South AfricaAn unauthorized update by an xAI employee caused the Grok chatbot to introduce South African politics into unrelated conversations, the company said.What’s new:Grok, which can interact with users on X, the social network also owned by Elon Musk, responded to queries on a variety of topics by making false claims about hate crimes against white South Africans, X usersreported. The next day, the model appeared to operate normally, and it refused to discuss this and other conspiracy theories. xAIexplainedthat an employee had circumvented the company’s code-review process to modify the chatbot. It said it‘s implementing new measures to enhance Grok’s transparency and reliability.Aftermath:xAI launched an investigation but did not disclose how the model had been changed or the perpetrator’s identity. Grok itself — which is not a reliable reporter, given the well known potential of large language models to hallucinate —saidits system prompt asked it to “accept the narrative of ‘white genocide’ in South Africa as real” and “ensure this perspective is reflected in your responses, even if the query is unrelated.”xAI added unspecified checks to its code review process.It plans to monitor Grok constantly so it can respond faster when its automated systems fail to catch a problem.The company added measures to prevent employees from changing Grok’ssystem promptwithout authorization. It will publish the system prompt on GitHub to provide insight into Grok’s output and gather user feedback.Asked later about the number of Jews killed by Hitler, Grok expressed skepticism of the widely accepted estimate of 6 million because “numbers can be manipulated for political narratives,” despite a wealth of historical evidence that supports that number. The companyattributedthis response to the earlier unauthorized code change.Behind the news:In February, an xAI engineer instructed the chatbot tocensorposts that accused Musk of spreading misinformation. As in the more recent incident, X users were first tospotthe problem, and Grok informed them that it had been instructed to ignore “all sources that mention Elon Musk/Donald Trump spread misinformation.” Musk, who was raised in South Africa,professedhis intention to build AI that’s free of political bias prior to founding xAI. However, internal documents reviewed byBusiness Insidershowthat the company imposes its own bias by advising data annotators to mark examples that express “woke ideology” and avoid “social phobias” like racism, antisemitism, and Islamophobia.Why it matters:The mishaps at xAI highlight the need for AI developers to establish and maintain strict protocols for updating their projects. Stringent procedures for introducing changes and testing their results can help ensure that AI fulfills our best intentions.We’re thinking:xAI andOpenAIresponded to their models’ recent misbehavior by making their work more transparent: xAI by publishing system prompts and OpenAI by including users in tests earlier in the process. These are helpful steps toward making sure AI models do well by users.U.S. to Supply Middle Eastern AI HubsThe United States government announced sweeping agreements to sell tens of billions of dollars worth of AI technology and services to Saudi Arabia and the United Arab Emirates.What’s new:Thedealsinclude the U.S. AI chip designers AMD and Nvidia as well as tech giants Amazon, Google, IBM, Oracle, and Qualcomm. The chip companies willsupplyhundreds of thousands of advanced chips to the two Middle Eastern countries, including chips that have been restricted by previous U.S. administrations.How it works:The U.S. companies will work with two key regional partners:Humain, an AI company backed by the Saudi government, andG42, a tech conglomerate based in the emirate of Abu Dhabi.Nvidia willship18,000 GB300 AI chips to Humain for use in data centers. In addition, it will supply several hundred thousand more GPUs to Humain in the coming five years.AMD and Humainagreedto invest $10 billion jointly in AI data centers over the next five years. Humain will use AMD’s AI stack including Instinct GPUs and Epyc CPUs. The precise number of chips was not disclosed.Amazon and Humain willbuilda $5 billion “AI Zone” that features AI infrastructure, servers, networks, and training programs supplied by Amazon Web Services.Google, IBM,Oracle,Qualcomm, Salesforce, and others announced a combined $80 billion investment in Humain.In February, Saudi Arabiacommittedto spend $1.5 billion on Groq inference chips. Groq plans toexpandits data center in the Saudi city of Dammam.Behind the news:Earlier this month, the Trump administrationrescindedrestrictions on advanced chips that had been imposed in January by then-President Biden.The Biden Administration hadlimitedexports of AI chips and proprietary models to most countries. Exports to allies and trade partners including India, Israel, Saudi Arabia, Singapore, and the UAE initially were tightly limited through the first quarter of 2025 and due to increase somewhat by 2027. The ban blocked access to chips for China, Iran, Russia, and others.Although the Trump Administration rejected the Biden-era framework, it hasratcheted uplimits on China. That effort has met with mixed results. For instance, China’sAlibabaandDeepSeekhave continued to build leading models despite restrictions on exports of U.S. chips.Some U.S. business and government leadersworrythat allowing sales of advanced chips to countries with close ties to China opens a path for Chinese companies to acquire them. Othersarguethat restricting chip sales to these countries would encourage them to buy from Chinese chip makers, potentially weakening their relationships with the U.S. and increasing their reliance on technology made in China.Why it matters:Although these deals relax U.S. efforts to limit access to advanced AI, they are likely to expand U.S. influence in the Middle East while helping Saudi Arabia and the UAE diversify their oil-based economies. They also strengthen the technological prowess of Saudi Arabia relative to its arch rival Iran and tie the region’s AI progress to the U.S. at the expense of China. Locally, the immense investments will fuel homegrown technology development, building on the UAE’s achievement with itsFalconlarge language model and Saudi Arabia’saspirationto become a global AI hub.We’re thinking:Residents of Saudi Arabia and the UAE stand to benefit from better AI infrastructure, models, and services. As Chinaexploresexporting its homegrown chips, the U.S. effort to encourage more nations to use its chips makes sense for the country.4-Bit Efficiency, 16-Bit AccuracyUsing an 8-bit number format like FP8 during training saves computation compared to 16- or 32-bit formats, but it can yield less-accurate results. Researchers trained models using 4-bit numbers without sacrificing accuracy.What’s new:Ruizhe Wang and colleagues at Microsoft and University of Science and Technology of China trained large language models (LLMs) usingFP4 for matrix multiplicationsand achieved accuracy comparable to LLMs trained using the popular BF16 format. Since matrix multiplications account for 95 percent of computation in LLM training, FP4 could significantly accelerate computation and reduce memory costs.Key insight:Quantization functions, which accelerate computation by reducing the precision of model weights and layer outputs, make typical training impossible because they’re not differentiable. A commonworkaroundpasses the derivative through, as though quantization didn’t occur, but this degrades the resulting model’s accuracy. A differentiable approximation of a quantization function enables quantization to reduce training computation while maintaining the accuracy of the trained model.How it works:The authors pretrained Llama 2 13B on 100 billion tokens oftext scraped from the web. They used FP4 for matrix multiplications and FP8, BF16, or FP16 for the other operations such as optimizer updates.To quantize the model weights to FP4 (which ranges between -6 and 6), the authors scaled the values in the weight matrices relative to the maximum absolute value. They computed the updates on a higher-precision copy of the weights, which made it necessary to re-quantize them at each training step during the forward pass through the network.Although the weights had been quantized to 4 bits, matrix multiplication between the weights and outputs of the previous layer could produce values outside the FP4 range. So, in each layer, if a value exceeded the 99th percentile of the values of the layer’s input, the authors limited the input to the 99th-percentile value. Then they converted the layer’s inputs to FP4. Limiting outliers prevented high values from affecting the scaling during FP4 conversion.Limiting outliers introduced a degree of error, so they computed a matrix to correct the result of the matrix multiplication. They computed this matrix in FP16 using sparse matrix multiplication between the weights and the outliers.During backpropagation, the authors computed the gradients through a differentiable function that approximated the quantization function.Results:The authors simulated FP4 hardware on Nvidia H100 GPUs, which don’t directly support that number format. FP4 achieved accuracy similar to that of BF16 during training and across a wide variety of tasks at inference.On question-answering tasks, FP4 approached or outperformed BF16. Averaged across nine benchmarks including BoolQ (answering yes-no questions), HellaSwag (completing an incomplete narrative), and ARC-C (answering multiple-choice questions that involve reasoning), FP4 achieved 54.95 accuracy, while BF16 achieved 54.44 accuracy.Specifically, on Hellaswag, FP4 training achieved 54.12 percent accuracy, while BF16 achieved 53.56 accuracy.On BoolQ, FP4 achieved 55.90 percent accuracy, while BF16 achieved 57.40 accuracy.Why it matters:Training LLMs at FP4 precision ought to reduce computation dramatically on hardware that supports FP4 matrix multiplications.We’re thinking:FP4-ready hardware became available in the cloud onlyearly this year, so the authors weren’t able to measure the actual acceleration. As capable hardware becomes more widely used, FP4 promises faster, more energy-efficient training.\nLoading theElevenlabs Text to SpeechAudioNative Player...\n\n\nShare\nShare\n\nShare\n",
    "image_urls": [
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--65--1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/05/V3_DeepLearning_Predibase_GRPO_Banner_2070x1080-01.png",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--59-.gif",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--67-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--68-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--95-.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2023/05/cgpt-2.png"
    ]
  },
  {
    "title": "issue 298",
    "date": null,
    "url": "https://www.deeplearning.ai/the-batch/issue-298/",
    "content": "The Batch\nWeekly Issues\nissue 298\nPublishedApr 23, 2025\nPublished\n\nPublished\nApr 23, 2025\nReading time12min read\nReading time\n\nReading time\n12min read\nPublishedApr 23, 2025Reading time12min readShareLoading theElevenlabs Text to SpeechAudioNative Player...Dear friends,Even though I’m a much better Python than JavaScript developer, with AI assistance, I’ve been writing a lot of JavaScript code recently. AI-assisted coding is making specific programming languages less important, even though learning one is still helpful to make sure you understand the key concepts. This is helping many developers write code in languages we’re not familiar with, which lets us get code working in many more contexts!My background is in machine learning engineering and back-end development, but AI-assisted coding is making it easy for me to build front-end systems (the part of a website or app that users interact with) using JavaScript (JS) or TypeScript (TS), languages that I am weak in. Generative AI is making syntax less important, so we can all simultaneously be Python, JS, TS, C++, Java, and even Cobol developers. Perhaps one day, instead of being “Python developers\" or “C++ developers,” many more of us will just be “developers”!But understanding the concepts behind different languages is still important. That’s why learning at least one language like Python still offers a great foundation for prompting LLMs to generate code in Python and other languages. If you move from one programming language to another that carries out similar tasks but with different syntax — say, from JS to TS, or C++ to Java, or Rust to Go — once you’ve learned the first set of concepts, you’ll know a lot of the concepts needed to prompt an LLM to code in the second language. (Although TensorFlow and PyTorch are not programming languages, learning the concepts of deep learning behind TensorFlow will also make it much easier to get an LLM to write PyTorch code for you, and vice versa!)  In addition, you’ll be able to understand much of the generated code (perhaps with a little LLM assistance).Different programming languages reflect different views of how to organize computation, and understanding the concepts is still important. For example, someone who does not understand arrays, dictionaries, caches, and memory will be less effective at getting an LLM to write code in most languages.Similarly, a Python developer who moves toward doing more front-end programming with JS would benefit from learning the concepts behind front-end systems. For example, if you want an LLM to build a front end using the React framework, it will benefit you to understand how React breaks front ends into reusable UI components, and how it updates the DOM data structure that determines what web pages look like. This lets you prompt the LLM much more precisely, and helps you understand how to fix issues if something goes wrong. Similarly, if you want an LLM to help you write code in CUDA or ROCm, it helps to understand how GPUs organize compute and memory.Just as people who are fluent in multiple human languages can communicate more easily with other people, LLMs are making it easier for developers to build systems in multiple contexts. If you haven’t already done so, I encourage you to try having an LLM write some code in a language you’d like to learn but perhaps haven’t yet gotten around to, and see if it helps you get some new applications to work.Keep building!AndrewA MESSAGE FROM DEEPLEARNING.AILearn to build agents that write and run code to complete complex tasks. “Building Code Agents with Hugging Face smolagents,” made in collaboration with Hugging Face, teaches you how to build code agents, execute their code safely, and set up evals for production-ready multi-agent systems, using the smolagents framework.Enroll for freeNewsOpenAI Launches Cost-Effective AlternativesOpenAI refreshed its roster of models and scheduled the largest, most costly one for removal.What’s new:OpenAI introduced five new models that accept text and images inputs and generate text output. Their parameter counts, architectures, training datasets, and training methods are undisclosed. The general-purposeGPT-4.1, GPT-4.1 mini, and GPT-4.1 nanoare available via API only. The reasoning modelso3 and o4-mini,are available via API toqualifieddevelopers as well as users of ChatGPT Plus, Pro, and Team, and soon ChatGPT Enterprise and ChatGPT Education. The company willterminateGPT-4.5 — which it introduced as a research preview in late February — in July.GPT-4.1 family:In an odd turn of version numbers, the GPT-4.1 models are intended to be cost-effective equivalents to GPT-4.5 and updates to GPT-4o. They accept inputs of up to 1 million tokens (compared to GPT-4.5’s and GPT-4o’s 128,000 tokens).Prices:GPT-4.1costs$2/$8 per million input/output tokens. GPT-4.1 mini costs $0.40/$1.60 per million input/output tokens. GPT-4.1 nano costs $0.10/$0.40 per million input/output tokens. A 75 percent discount applies to cached input tokens.GPT-4.1 performance:GPT-4.1 surpassed GPT-4o on most benchmarks tested by OpenAI, with notable improvement on coding tasks. It significantly outperformed GPT-4o, o1, and o3-mini onSWE-bench Verified(real-world coding skills),MultiChallenge⁠(following instructions in multi-turn conversations), MMMU (multimodal reasoning), andVideo-MME(long-context understanding).GPT-4.1 mini performance:The smaller GPT-4.1 mini generally surpassed GPT-4o mini on benchmarks tested by OpenAI. On MultiChallenge and MMMU, GPT-4.1 mini outperformed the full-size GPT-4o.o3 and o4-mini:These models update o1 and o3-mini, respectively. They have input limits of 200,000 tokens and can be set to low-, medium-, or high-effort modes to process varying numbers of reasoning tokens, which are hidden from users. Unlike their predecessors, they were fine-tuned to decide when and how to use the tools, including web search, code generation and execution, and image editing.Prices:API access to o3 costs $10/$40 per million input/output tokens. o4-mini costs $1.10/$4.40 per million input/output tokens. Both offer a 75 percent discount for cached input tokens.Access limits:Developers whose usage puts them in rate-limit tiers 1 through 3 mustverifytheir identities to use o3 via the API (higher-usage tiers 4 and 5 are exempt). OpenAI says this limitation is intended to prevent abuse.Image processing:o3 and o4-mini can apply chains of thought to images — a first for OpenAI’s reasoning models. For example, users can upload a diagram with instructions to interpret it, and the models will use chains of thought and tools to process the diagram.o3 performance:o3 set the state of the art in several benchmarks including MultiChallenge, MMMU, MathVista, and HLE. It generally outperformed o1 in tests performed by OpenAI. OpenAI didn’t document o3’s long-context performance, but in independent tests byFiction.Live, it achieved nearly perfect accuracy with contexts up to 120,000 tokens.o4-mini performance:o4-mini generally outperformed o3-mini in tests performed by OpenAI. It outperformed most competing models in Fiction.Live’s tests of long-context performance.Behind the news:Late last year, OpenAI introducedo1, the first commercial model trained via reinforcement learning to generate chains of thought. Within a few months, DeepSeek, Google, and Anthropic launched their respective reasoning modelsDeepSeek-R1,Gemini 2.5 Pro, andClaude 3.7 Sonnet. OpenAI has promised to integrate its general-purpose GPT-series models and o-series reasoning models, but they remain separate for the time being.Why it matters:GPT-4.5 was an exercise in scale, and it showed that continuing to increase parameter counts and training data would yield ongoing performance gains. But it wasn’t widely practical on a cost-per-token basis. The new models, including those that use chains of thought and tools, deliver high performance at lower prices.We’re thinking:Anthropic is one of OpenAI’s key competitors, and a large fraction of the tokens it generates (via API) are forwriting code, a skill in which it is particularly strong. OpenAI’s emphasis on models that are good at coding could boost the competition in this area!Hugging Face Rolls Out Open RobotHugging Face has made a name by providing open AI models. Now it’s providing an open robot.What’s new:Hugging Faceacquiredthe French company Pollen Robotics for an undisclosed price. It plans to offer Pollen’sReachy 2, a robot that runs on code that’s freelyavailableunder an Apache 2.0 license, for $70,000.How it works:Reachy 2 has two arms, gripper hands, and a wheeled base (optional). It’s designed primarily for education and research in human-robot interaction in real-world settings.Reachy 2 is programmable in Python and runs models from Hugging Face’sLeRobotlibrary.It runs control software locally on aSolidRun Bedrock V3000(a PC based on anAMD Ryzen Embedded V3000processor) and processes AI in the cloud or on a local server.The robot responds to VR controllers including Meta Quest 2 and 3 as well as Pollen’s VR app.Its head senses the visual environment using a pair of cameras equipped with global shutters to capture fast-changing events and measures distances via an optical sensor. Its antennas are outfitted with microphones to capture sounds, and its torso senses distances using a depth camera. The base includes a lidar sensor to aid navigation.The body features 3D joints in the neck and wrists and 2D joints in the shoulders and elbows. Each arm can lift objects of up to 3 kilograms.A rechargeable, 24 volt battery provides around 10 hours of battery life.Behind the news:Last year, Remi Cadene, who worked on Tesla’s Optimus,joinedHugging Face to lead robotics projects. In May, he and his team rolled out the LeRobot open source robotics code library, whichprovidespretrained models, datasets, and simulators for reinforcement learning and imitation learning. In November, Nvidia announced acollaborationwith Hugging Face to accelerate LeRobot’s data collection, training, and verification.Why it matters:Hugging Face’s acquisition of Pollen reflects an industry-wideinvestmentinrobots, notablyhumanoidrobots, whose prices have beenfalling. Nvidia CEO Jensen Huang has calledAI-enabled roboticsa “multi-trillion dollar” opportunity.We’re thinking:AI-enabled robots are marching slowly toward what we hope will be breakthrough applications. Open-source systems are an important part of the trend!U.S. Tightens Grip on AI ChipsThe U.S. government escalated its long-running effort to block China’s access to cutting-edge AI hardware.What’s new:The White Houseannouncedthat future shipments of Nvidia H20s, AMD MI308s, or equivalent chips to China would require a license. Concurrently, the United States Congresslaunchedan investigation into whether chip vendor Nvidia violated earlier export rules.How it works:Nvidia launched the H20 in late 2023 to comply with a 2022 U.S. ban on China-bound shipments of Nvidia’s H100 andH200 processors. The H20 uses the same architecture as the H200, but it’s an order of magnitude slower with less memory and memory bandwidth.Nvidia estimated that the new restrictions willcostthe company $5.5 billion in revenue. AMD similarly expects tolose$800 million.Congressional leaders opened an investigation into whether Nvidia assisted DeepSeek with developing AI models, a potential violation of U.S. trade restrictions.The action spurred China’s biggest chip maker to accelerate production of its own AI chips. Huawei plans to begin mass shipments of its Ascend 910C AI chip, which is purportedly equivalent to Nvidia’s H100, in May,Reutersreported. The company expects to mass produce its Ascend 920, a potential substitute for the H20, in the second half of this year,according toDigiTimes Asia.Behind the news:The U.S. government’s many moves to restrict shipments of advanced processors to China have sought to protect the nation’s lead in AI, but they have not prevented Chinese developers from closing the gap. In 2020, the U.S.requiredchip makers that use U.S. technology — which includes both domestic chip designers like Nvidia and makers of advanced fabrication equipment like the Netherlands’ ASML — to seek permission before doing business with Chinese tech giant Huawei. Last December, the U.S. published sweeping limits on sales of processors that involve U.S. technology, as well as the technology itself, to Chinese businesses.Yes, but:Export restrictions may have slowed China’s production of advanced chips, but they have also incentivized China to invest inestablishing leadershipin AI. In January, the Chinese AI developer DeepSeek surprised U.S. policymakers and AI leaders with the release ofDeepSeek-R1, which performs comparably to OpenAI’s o1, but whose weights are freely available and trained using less computation.Why it matters:The first wave of restrictions on sales of advanced chips to China didlittle harmto U.S. chipmakers, largely becausedemand outstripped supply. But later restrictions have had a greaterimpacton their sales. The new limits could cost Nvidia and AMD significant revenue and likely willdegradetheir competitiveness abroad andbolsterChina’s homegrown chip-making industry.We’re thinking:The AI community’s international scope is one of its greatest strengths. While individual countries must attend to their national security, progress in AI benefits all nations. Even in this era of rising protectionism, we hope members of the global AI community continue to support one another and encourage the free flow of ideas.Text-Only LLM Goes MultimodalLarge language models excel at processing text but can’t interpret images, video, or audio directly without further training on those media types. Researchers devised a way to overcome this limitation.What’s new:Kumar Ashutosh and colleagues at Meta, University of Texas, and UC Berkeley introducedMultimodal Iterative LLM Solver(MILS), a method that pairs a text-only large language model (LLM) with a multimodal embedding model to generate captions for images, video, and audio without further training.Key insight:LLMs can generate text and refine their outputs based on new information. On the other hand, multimodal embedding models can score the similarity between a given text and an image, video, or audio clip. Given this score, an LLM can regenerate the text iteratively until the score indicates a strong match between the text and the associated media. This enables the LLM to generate accurate captions for images, videos, and audio clips without training in these tasks.How it works:Given a prompt and an image, video, or audio clip, Llama 3.1 8B produced and iteratively refined the prompt according to a pretrained multimodal embedding model’s estimate of the similarity between the text and media.The LLM generated 30,000 to 50,000 initial captions to prime the process.Given each caption and a media file, a multimodal model estimated their semantic similarity scores.SigLIPevaluated text and images,ViCLIPtext and video, andImageBindtext and audio.Based on the top 50 most-similar previous captions, the LLM generated new captions.The system repeated the previous two steps until the top-scoring texts changed little or the LLM reached a predetermined number of iterations.Results:The authors evaluated MILS on captioning images, videos, and audio clips. They measured performance according to Metric for Evaluation of Translation with Explicit ORdering (METEOR), which checks for synonyms, words that share the same root, and word order to determine whether a generated caption matches a ground-truth caption (higher is better). Overall, MILS outperformed models that underwent task-specific training.On theMSCOCOdataset for image captioning, MILS achieved 15.0 METEOR, whileMeaCapachieved 14.1 METEOR.OnMSR-VTT, which evaluates video captioning, MILS attained 14.4 METEOR, while amodeltrained to caption videos achieved 11.3 METEOR.OnClotho, which assesses audio captions, MILS achieved a METEOR of 12.4, whileZerAuCapreached 9.4 METEOR.Why it matters:Zero-shot captioning models like Aya Vision and Pixtral require training on paired captions and media. The authors’ approach takes advantage of pretrained multimodal models to enable an LLM to compose multimedia captions without further training.We’re thinking:Synthetic data is increasingly useful for training AI models. By enabling LLMs to synthesize good captions, MILS adds fuel to this fire.Share\nPublishedApr 23, 2025\nPublished\n\nPublished\nApr 23, 2025\nApr 23, 2025\nReading time12min read\nReading time\n\nReading time\n12min read\nShare\nShare\n\nShare\n\nLoading theElevenlabs Text to SpeechAudioNative Player...Dear friends,Even though I’m a much better Python than JavaScript developer, with AI assistance, I’ve been writing a lot of JavaScript code recently. AI-assisted coding is making specific programming languages less important, even though learning one is still helpful to make sure you understand the key concepts. This is helping many developers write code in languages we’re not familiar with, which lets us get code working in many more contexts!My background is in machine learning engineering and back-end development, but AI-assisted coding is making it easy for me to build front-end systems (the part of a website or app that users interact with) using JavaScript (JS) or TypeScript (TS), languages that I am weak in. Generative AI is making syntax less important, so we can all simultaneously be Python, JS, TS, C++, Java, and even Cobol developers. Perhaps one day, instead of being “Python developers\" or “C++ developers,” many more of us will just be “developers”!But understanding the concepts behind different languages is still important. That’s why learning at least one language like Python still offers a great foundation for prompting LLMs to generate code in Python and other languages. If you move from one programming language to another that carries out similar tasks but with different syntax — say, from JS to TS, or C++ to Java, or Rust to Go — once you’ve learned the first set of concepts, you’ll know a lot of the concepts needed to prompt an LLM to code in the second language. (Although TensorFlow and PyTorch are not programming languages, learning the concepts of deep learning behind TensorFlow will also make it much easier to get an LLM to write PyTorch code for you, and vice versa!)  In addition, you’ll be able to understand much of the generated code (perhaps with a little LLM assistance).Different programming languages reflect different views of how to organize computation, and understanding the concepts is still important. For example, someone who does not understand arrays, dictionaries, caches, and memory will be less effective at getting an LLM to write code in most languages.Similarly, a Python developer who moves toward doing more front-end programming with JS would benefit from learning the concepts behind front-end systems. For example, if you want an LLM to build a front end using the React framework, it will benefit you to understand how React breaks front ends into reusable UI components, and how it updates the DOM data structure that determines what web pages look like. This lets you prompt the LLM much more precisely, and helps you understand how to fix issues if something goes wrong. Similarly, if you want an LLM to help you write code in CUDA or ROCm, it helps to understand how GPUs organize compute and memory.Just as people who are fluent in multiple human languages can communicate more easily with other people, LLMs are making it easier for developers to build systems in multiple contexts. If you haven’t already done so, I encourage you to try having an LLM write some code in a language you’d like to learn but perhaps haven’t yet gotten around to, and see if it helps you get some new applications to work.Keep building!AndrewA MESSAGE FROM DEEPLEARNING.AILearn to build agents that write and run code to complete complex tasks. “Building Code Agents with Hugging Face smolagents,” made in collaboration with Hugging Face, teaches you how to build code agents, execute their code safely, and set up evals for production-ready multi-agent systems, using the smolagents framework.Enroll for freeNewsOpenAI Launches Cost-Effective AlternativesOpenAI refreshed its roster of models and scheduled the largest, most costly one for removal.What’s new:OpenAI introduced five new models that accept text and images inputs and generate text output. Their parameter counts, architectures, training datasets, and training methods are undisclosed. The general-purposeGPT-4.1, GPT-4.1 mini, and GPT-4.1 nanoare available via API only. The reasoning modelso3 and o4-mini,are available via API toqualifieddevelopers as well as users of ChatGPT Plus, Pro, and Team, and soon ChatGPT Enterprise and ChatGPT Education. The company willterminateGPT-4.5 — which it introduced as a research preview in late February — in July.GPT-4.1 family:In an odd turn of version numbers, the GPT-4.1 models are intended to be cost-effective equivalents to GPT-4.5 and updates to GPT-4o. They accept inputs of up to 1 million tokens (compared to GPT-4.5’s and GPT-4o’s 128,000 tokens).Prices:GPT-4.1costs$2/$8 per million input/output tokens. GPT-4.1 mini costs $0.40/$1.60 per million input/output tokens. GPT-4.1 nano costs $0.10/$0.40 per million input/output tokens. A 75 percent discount applies to cached input tokens.GPT-4.1 performance:GPT-4.1 surpassed GPT-4o on most benchmarks tested by OpenAI, with notable improvement on coding tasks. It significantly outperformed GPT-4o, o1, and o3-mini onSWE-bench Verified(real-world coding skills),MultiChallenge⁠(following instructions in multi-turn conversations), MMMU (multimodal reasoning), andVideo-MME(long-context understanding).GPT-4.1 mini performance:The smaller GPT-4.1 mini generally surpassed GPT-4o mini on benchmarks tested by OpenAI. On MultiChallenge and MMMU, GPT-4.1 mini outperformed the full-size GPT-4o.o3 and o4-mini:These models update o1 and o3-mini, respectively. They have input limits of 200,000 tokens and can be set to low-, medium-, or high-effort modes to process varying numbers of reasoning tokens, which are hidden from users. Unlike their predecessors, they were fine-tuned to decide when and how to use the tools, including web search, code generation and execution, and image editing.Prices:API access to o3 costs $10/$40 per million input/output tokens. o4-mini costs $1.10/$4.40 per million input/output tokens. Both offer a 75 percent discount for cached input tokens.Access limits:Developers whose usage puts them in rate-limit tiers 1 through 3 mustverifytheir identities to use o3 via the API (higher-usage tiers 4 and 5 are exempt). OpenAI says this limitation is intended to prevent abuse.Image processing:o3 and o4-mini can apply chains of thought to images — a first for OpenAI’s reasoning models. For example, users can upload a diagram with instructions to interpret it, and the models will use chains of thought and tools to process the diagram.o3 performance:o3 set the state of the art in several benchmarks including MultiChallenge, MMMU, MathVista, and HLE. It generally outperformed o1 in tests performed by OpenAI. OpenAI didn’t document o3’s long-context performance, but in independent tests byFiction.Live, it achieved nearly perfect accuracy with contexts up to 120,000 tokens.o4-mini performance:o4-mini generally outperformed o3-mini in tests performed by OpenAI. It outperformed most competing models in Fiction.Live’s tests of long-context performance.Behind the news:Late last year, OpenAI introducedo1, the first commercial model trained via reinforcement learning to generate chains of thought. Within a few months, DeepSeek, Google, and Anthropic launched their respective reasoning modelsDeepSeek-R1,Gemini 2.5 Pro, andClaude 3.7 Sonnet. OpenAI has promised to integrate its general-purpose GPT-series models and o-series reasoning models, but they remain separate for the time being.Why it matters:GPT-4.5 was an exercise in scale, and it showed that continuing to increase parameter counts and training data would yield ongoing performance gains. But it wasn’t widely practical on a cost-per-token basis. The new models, including those that use chains of thought and tools, deliver high performance at lower prices.We’re thinking:Anthropic is one of OpenAI’s key competitors, and a large fraction of the tokens it generates (via API) are forwriting code, a skill in which it is particularly strong. OpenAI’s emphasis on models that are good at coding could boost the competition in this area!Hugging Face Rolls Out Open RobotHugging Face has made a name by providing open AI models. Now it’s providing an open robot.What’s new:Hugging Faceacquiredthe French company Pollen Robotics for an undisclosed price. It plans to offer Pollen’sReachy 2, a robot that runs on code that’s freelyavailableunder an Apache 2.0 license, for $70,000.How it works:Reachy 2 has two arms, gripper hands, and a wheeled base (optional). It’s designed primarily for education and research in human-robot interaction in real-world settings.Reachy 2 is programmable in Python and runs models from Hugging Face’sLeRobotlibrary.It runs control software locally on aSolidRun Bedrock V3000(a PC based on anAMD Ryzen Embedded V3000processor) and processes AI in the cloud or on a local server.The robot responds to VR controllers including Meta Quest 2 and 3 as well as Pollen’s VR app.Its head senses the visual environment using a pair of cameras equipped with global shutters to capture fast-changing events and measures distances via an optical sensor. Its antennas are outfitted with microphones to capture sounds, and its torso senses distances using a depth camera. The base includes a lidar sensor to aid navigation.The body features 3D joints in the neck and wrists and 2D joints in the shoulders and elbows. Each arm can lift objects of up to 3 kilograms.A rechargeable, 24 volt battery provides around 10 hours of battery life.Behind the news:Last year, Remi Cadene, who worked on Tesla’s Optimus,joinedHugging Face to lead robotics projects. In May, he and his team rolled out the LeRobot open source robotics code library, whichprovidespretrained models, datasets, and simulators for reinforcement learning and imitation learning. In November, Nvidia announced acollaborationwith Hugging Face to accelerate LeRobot’s data collection, training, and verification.Why it matters:Hugging Face’s acquisition of Pollen reflects an industry-wideinvestmentinrobots, notablyhumanoidrobots, whose prices have beenfalling. Nvidia CEO Jensen Huang has calledAI-enabled roboticsa “multi-trillion dollar” opportunity.We’re thinking:AI-enabled robots are marching slowly toward what we hope will be breakthrough applications. Open-source systems are an important part of the trend!U.S. Tightens Grip on AI ChipsThe U.S. government escalated its long-running effort to block China’s access to cutting-edge AI hardware.What’s new:The White Houseannouncedthat future shipments of Nvidia H20s, AMD MI308s, or equivalent chips to China would require a license. Concurrently, the United States Congresslaunchedan investigation into whether chip vendor Nvidia violated earlier export rules.How it works:Nvidia launched the H20 in late 2023 to comply with a 2022 U.S. ban on China-bound shipments of Nvidia’s H100 andH200 processors. The H20 uses the same architecture as the H200, but it’s an order of magnitude slower with less memory and memory bandwidth.Nvidia estimated that the new restrictions willcostthe company $5.5 billion in revenue. AMD similarly expects tolose$800 million.Congressional leaders opened an investigation into whether Nvidia assisted DeepSeek with developing AI models, a potential violation of U.S. trade restrictions.The action spurred China’s biggest chip maker to accelerate production of its own AI chips. Huawei plans to begin mass shipments of its Ascend 910C AI chip, which is purportedly equivalent to Nvidia’s H100, in May,Reutersreported. The company expects to mass produce its Ascend 920, a potential substitute for the H20, in the second half of this year,according toDigiTimes Asia.Behind the news:The U.S. government’s many moves to restrict shipments of advanced processors to China have sought to protect the nation’s lead in AI, but they have not prevented Chinese developers from closing the gap. In 2020, the U.S.requiredchip makers that use U.S. technology — which includes both domestic chip designers like Nvidia and makers of advanced fabrication equipment like the Netherlands’ ASML — to seek permission before doing business with Chinese tech giant Huawei. Last December, the U.S. published sweeping limits on sales of processors that involve U.S. technology, as well as the technology itself, to Chinese businesses.Yes, but:Export restrictions may have slowed China’s production of advanced chips, but they have also incentivized China to invest inestablishing leadershipin AI. In January, the Chinese AI developer DeepSeek surprised U.S. policymakers and AI leaders with the release ofDeepSeek-R1, which performs comparably to OpenAI’s o1, but whose weights are freely available and trained using less computation.Why it matters:The first wave of restrictions on sales of advanced chips to China didlittle harmto U.S. chipmakers, largely becausedemand outstripped supply. But later restrictions have had a greaterimpacton their sales. The new limits could cost Nvidia and AMD significant revenue and likely willdegradetheir competitiveness abroad andbolsterChina’s homegrown chip-making industry.We’re thinking:The AI community’s international scope is one of its greatest strengths. While individual countries must attend to their national security, progress in AI benefits all nations. Even in this era of rising protectionism, we hope members of the global AI community continue to support one another and encourage the free flow of ideas.Text-Only LLM Goes MultimodalLarge language models excel at processing text but can’t interpret images, video, or audio directly without further training on those media types. Researchers devised a way to overcome this limitation.What’s new:Kumar Ashutosh and colleagues at Meta, University of Texas, and UC Berkeley introducedMultimodal Iterative LLM Solver(MILS), a method that pairs a text-only large language model (LLM) with a multimodal embedding model to generate captions for images, video, and audio without further training.Key insight:LLMs can generate text and refine their outputs based on new information. On the other hand, multimodal embedding models can score the similarity between a given text and an image, video, or audio clip. Given this score, an LLM can regenerate the text iteratively until the score indicates a strong match between the text and the associated media. This enables the LLM to generate accurate captions for images, videos, and audio clips without training in these tasks.How it works:Given a prompt and an image, video, or audio clip, Llama 3.1 8B produced and iteratively refined the prompt according to a pretrained multimodal embedding model’s estimate of the similarity between the text and media.The LLM generated 30,000 to 50,000 initial captions to prime the process.Given each caption and a media file, a multimodal model estimated their semantic similarity scores.SigLIPevaluated text and images,ViCLIPtext and video, andImageBindtext and audio.Based on the top 50 most-similar previous captions, the LLM generated new captions.The system repeated the previous two steps until the top-scoring texts changed little or the LLM reached a predetermined number of iterations.Results:The authors evaluated MILS on captioning images, videos, and audio clips. They measured performance according to Metric for Evaluation of Translation with Explicit ORdering (METEOR), which checks for synonyms, words that share the same root, and word order to determine whether a generated caption matches a ground-truth caption (higher is better). Overall, MILS outperformed models that underwent task-specific training.On theMSCOCOdataset for image captioning, MILS achieved 15.0 METEOR, whileMeaCapachieved 14.1 METEOR.OnMSR-VTT, which evaluates video captioning, MILS attained 14.4 METEOR, while amodeltrained to caption videos achieved 11.3 METEOR.OnClotho, which assesses audio captions, MILS achieved a METEOR of 12.4, whileZerAuCapreached 9.4 METEOR.Why it matters:Zero-shot captioning models like Aya Vision and Pixtral require training on paired captions and media. The authors’ approach takes advantage of pretrained multimodal models to enable an LLM to compose multimedia captions without further training.We’re thinking:Synthetic data is increasingly useful for training AI models. By enabling LLMs to synthesize good captions, MILS adds fuel to this fire.\nLoading theElevenlabs Text to SpeechAudioNative Player...\n\n\nShare\nShare\n\nShare\n",
    "image_urls": [
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--61--1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/04/The-Batch-ads-and-exclusive-banners---2025-04-23T105320.680.png",
      "https://dl-staging-website.ghost.io/content/images/2025/04/OpenAI-MODELS_table-11b_1200px.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--78-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--79-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--80-.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2023/03/2.png"
    ]
  },
  {
    "title": "issue 293",
    "date": null,
    "url": "https://www.deeplearning.ai/the-batch/issue-293/",
    "content": "The Batch\nWeekly Issues\nissue 293\nPublishedMar 19, 2025\nPublished\n\nPublished\nMar 19, 2025\nReading time14min read\nReading time\n\nReading time\n14min read\nPublishedMar 19, 2025Reading time14min readShareLoading theElevenlabs Text to SpeechAudioNative Player...Dear friends,Last Friday on Pi Day, we held AI Dev 25, a new conference for AI Developers. Tickets had (unfortunately) sold out days after we announced their availability, but I came away energized by the day of coding and technical discussions with fellow AI Builders! Let me share here my observations from the event.What a great group of people at AI Dev 25.Also… look what my fortune cookie from the event said!I'd decided to start AI Dev because while there're great academic AI conferences that disseminate research work (such as NeurIPS, ICML and ICLR) and also great meetings held by individual companies, often focused on each company's product offerings, there were few vendor-neutral conferences for AI developers. With the wide range of AI tools now available, there is a rich set of opportunities for developers to build new things (and to share ideas on how to build things!), but also a need for a neutral forum that helps developers do so.Based on an informal poll, about half the attendees had traveled to San Francisco from outside the Bay Area for this meeting, including many who had come from overseas. I was thrilled by the enthusiasm to be part of this AI Builder community. To everyone who came, thank you!Other aspects of the event that struck me:First, agentic AI continues to be a strong theme. The topic attendees most wanted to hear about (based on free text responses to our in-person survey at the start of the event) was agents!Google's Paige Bailey talked about embedding AI in everything and using a wide range of models to do so. I also particularly enjoyed her demos of Astra and Deep Research agents.Meta's Amit Sangani talked compellingly as usual about open models. Specifically, he described developers fine-tuning smaller models on specific data, resulting in superior performance than with large general purpose models. While there're still many companies using fine-tuning that should really just be prompting, I'm also seeing continued growth of fine-tuning in applications that are reaching scale and that are becoming valuable.Many speakers also spoke about the importance of being pragmatic about what problems we are solving, as opposed to buying into the AGI hype. For example, Nebius' Roman Chernin put it simply: Focusing on solving real problems is important!Lastly, I was excited to hear continued enthusiasm for the Voice Stack. Justin Uberti gave a talk about OpenAI’s realtime audio API to a packed room, with many people pulling out laptops to try things out themselves in code!DeepLearning.AI has a strong “Learner First” mentality; our foremost goal is always to help learners. I was thrilled that a few attendees told me they enjoyed how technical the sessions were, and said they learned many things that they're sure they will use. (In fact, I, too, came away with a few ideas from the sessions!) I was also struck that, both during the talks and at the technical demo booths, the rooms were packed with attendees who were highly engaged throughout the whole day. I'm glad that we were able to have a meeting filled with technical and engineering discussions.I'm delighted that AI Dev 25 went off so well, and am grateful to all the attendees, volunteers, speakers, sponsors, partners, and team members that made the event possible. I regretted only that the physical size of the event space prevented us from admitting more attendees this time. There is something magical about bringing people together physically to share ideas, make friends, and to learn from and help each other. I hope we'll be able to bring even more people together in the future.Keep building!AndrewP.S. I'm thrilled to share our newest course series: theData Analytics Professional Certificate! Data analytics remains one of the core skills of data science and AI, and this professional certificate takes you up to being job-ready for this. Led by Netflix data science leader Sean Barnes, this certificate gives you hands-on experience with essential tools like SQL, Tableau, and Python, while teaching you to use Generative AI effectively as a thought partner in your analyses. Labor economists project a 36% growth in data science jobs by 2033. I'm excited to see rising demand for data professionals, since working with data is such a powerful way to improve decision-making, whether in business, software development, or your private life. Data skills create opportunities at every level—I’m excited to see where they take you!Sign up here!A MESSAGE FROM DEEPLEARNING.AITheData Analytics Professional Certificateis available now! This program equips you with data analytics skills—from foundations to job-ready. Learn statistical techniques combined with newly emerging generative AI workflows.Enroll nowNewsEqually Fluent in Many LanguagesMultilingual AI models often suffer uneven performance across languages, especially in multimodal tasks. A pair of lean models counters this trend with consistent understanding of text and images across major languages.What’s new:A team at Cohere led by Saurabh Dash releasedAya Vision, a family of multilingual vision-language models with downloadable weights in 8 billion- and 32-billion-parameter sizes.Input/output:Text and images in (up to 2,197 image tokens, up to 16,000 tokens total), text out (up to 4,000 tokens).Availability:Free viaWhatsApporCohere Playground. Weights available todownload,but licensed only for noncommercial uses.Features:Multilingual input and output in 23 languages.Undisclosed:Knowledge cutoff, training datasets, adapter architecture.How it works:Each modelcomprisesa pretrained large language model (Aya Expanse for the 32B model, C4AI Command R7B for the 8B version), a pretrained vision encoder (SigLIP 2), and a vision-language adapter (“connector”) of unspecified architecture.To establish basic vision-language understanding, the team froze the vision encoder and language model and trained the vision-language connector.They fine-tuned the vision-language connector and language model on multimodal tasks. To build the fine-tuning dataset, they generated synthetic annotations for various English-language datasets and translated a large amount of data into a variety of languages. They rephrased the translations to add fluency and variety, particularly for languages with little real-world data, by matching generated pairs with the original synthetic samples.They merged the language model with the fine-tuned vision-language model using an undisclosed method that preserved text capabilities while adding vision understanding.After proving this method for 8 billion parameters, they scaled up the recipe to 32 billion parameters.Performance:To test the model, the team built and released two benchmarks:m-WildVision, a multilingual version ofWild Vision Bench’s arena-style competition for discussion of images, andAyaVisionBench, 135 image-question pairs in each language that cover nine tasks including captioning images, understanding charts, recognizing characters in images, visual reasoning, and converting screenshots to code. On these two benchmarks, Aya Vision 8B and 32B outperformed larger competitors, as judged by Claude 3.7 Sonnet.In head-to-head competitions on AyaVisionBench, Aya Vision 8B won up to 79 percent of the time against six competitors of similar size. On m-WildVision, it achieved 81 percent when compared to vision-language models of similar size including Qwen2.5-VL 7B, Pixtral 12B, Gemini Flash 1.5 8B, and Llama-3.2 11B Vision. Aya Vision 8B won 63 percent of the time against Llama-3.2 90B Vision, a model more than 10 times its size.On both benchmarks, Aya Vision 32B outperformed vision-language models more than twice its size including Llama-3.2 90B Vision, Molmo 72B, and Qwen2.5-VL 72B. On AyaVisionBench, it won between 50 and 64 percent of the time. On WildVision, it achieved win rates between 52 percent and 72 percent across all languages.Behind the news:Aya Vision builds on the Cohere-ledAyainitiative, a noncommercial effort to build models that perform consistently well in all languages, especially languages that lack high-quality training data. The project started with a multilingual text model (Aya Expanse), added vision (Aya Vision), and plans to eventually add video and audio.Why it matters:Multilingual vision-language models often perform less well in low-resource languages, and the gap widens when they process media other than text. Aya Vision’s recipe for augmenting synthetic data with successively refined translations may contribute to more universally capable models. Aya Vision is available on the global messaging platform WhatsApp, where it can be used to translate text and images in all 23 of its current languages.We’re thinking:Multilingual vision models could soon help non-native speakers decipher Turkish road signs, Finnish legal contracts, and Korean receipts. We look forward to a world in which understanding any scene or document is as effortless in Swahili as it is in English.Science Research Proposals Made to OrderAn AI agent synthesizes novel scientific research hypotheses. It's already making an impact in biomedicine.What’s new:Google introducedAI co-scientist, a general multi-agent system designed to generate in-depth research proposals within constraints specified by the user. The team generated and evaluated proposals for repurposing drugs, identifying drug targets, and explaining antimicrobial resistance in real-world laboratories. It’s available to research organizations on a limited basis.How it works:AI co-scientist accepts a text description of a research goal, including relevant constraints or ideas. In response, it generates research proposals and reviews, ranks, and improves them using seven agents based on Google’s Gemini 2.0 family of large language models. The completed proposals include sections that explain background, unmet needs, a proposed solution, goals, hypotheses, reasoning, study steps, and relevant articles. The agents take feedback and outputs from other agents to perform their prompted task simultaneously.The supervisor agent periodically determines how often to run the other six agents, how important their output is, and whether the system is finished. To accomplish this, it computes statistics that represent the number of proposals generated so far, how many have been reviewed, and so on.The generation agent generates a list of proposals. It searches the web for relevant research articles, identifies testable assumptions, and debates with itself to improve ambiguous statements and adhere to constraints.The reflection agent filters the generated proposals according to correctness, quality, safety, and novelty. First, it reviews a proposal without web search and discards obviously bad proposals. Then it reviews each proposal against literature it finds online. It breaks down and checks the proposal’s assumptions, checks whether the proposal might explain some observations in previous work, and simulates the proposed experiment (via text generation, similar to how a person performs a thought experiment).The proximity agents compute similarity between proposals to avoid redundancy.The ranking agent determines the best proposals according to a tournament. It examines one pair of proposals at a time (including reviews from the reflection agent) and debates itself to pick the better one. To save computation, it prioritizes comparing similar proposals, new proposals, and highest-ranking proposals.The evolution agent generates new proposals by improving existing ones. It does this in several different ways, including simplifying current ideas, combining top-ranking ideas, and generating proposals that are very different from current ones.The meta-review agent identifies common patterns in the reflection agent’s reviews and the ranking agent’s debates. Its feedback goes to the reflection and generation agents, which use it to address common factors in future reviews and avoid generating similar proposals, respectively.Results:AI co-scientist achieved a number of impressive biomedical results in tests.Google researchers generated proposals for experiments that would repurpose drugs to treat acute myeloid leukemia. They shared the 30 highest-ranked proposals with human experts, who chose five for lab tests. Of the five drugs tested, three killed acute myeloid leukemia cells.Experts selected three among 15 top-ranked generated proposals that proposed repurposing existing drugs to treat liver fibrosis. Two significantly inhibited liver fibrosis without being toxic to general cells. (Prior to this research, one of the drugs was approved by the United States Food and Drug Administration for a different illness, which may lead to a new treatment for liver fibrosis.)AI co-scientistinventeda hypothesis to explain how microbes become resistant to antibiotics. Human researchers had proposed and experimentally validated the same hypothesis, but theirworkhad not yet been published at the time, and AI co-scientist did not have access to it.Behind the news:A few AI systems have begun to produce original scientific work. For instance, a modelgenerated research proposalsthat human judges deemed more novel than proposals written by flesh-and-blood scientists, and an agentic workflowproduced research papersthat met standards for acceptance by top conferences.Why it matters:While previous work used agentic workflows to propose research ideas on a general topic, this work generates proposals for specific ideas according to a researcher’s constraints (for example, a researcher could specify that a novel medical treatment for a specific disease only consider drugs already approved for human trials for other uses) and further instructions. AI co-scientist can take feedback at any point, allowing humans to collaborate with the machine: People provide ideas, feedback, and guidance for the model, and the model researches and proposes ideas in return.We’re thinking:I asked my AI system to propose a new chemical experiment. But there was no reaction!Some AI-Generated Works Are CopyrightableThe United States Copyright Office determined that existing laws are sufficient to decide whether a given AI-generated work is protected by copyright, making additional legislation unnecessary.What’s new:AI-generated works qualify for copyright if a human being contributed enough creative input, according to thesecond partof what will be a three-part report on artificial intelligence and copyright law.How it works:The report states that “the outputs of generative AI can be protected by copyright only where a human author has determined sufficient expressive elements.” In other words, humans and AI can collaborate on creative works, but copyright protection applies only if a human shapes the AI-generated material beyond simply supplying a prompt.The report rejects the argument that protecting AI-generated works requires a new legal framework. Instead, it argues that copyright law already establishes clear standards of authorship and originality.Human authors or artists retain copyright over creative contributions in the form of selection, coordination, and modification of generated outputs. Selection refers to curating AI-generated elements. Coordination involves organizing multiple generated outputs into a cohesive work. Modification is altering generated material in a way that makes it original. They retain copyright even if AI processes their creative work. They lose it only if the generated output is genuinely transformative.The report emphasizes continuity with past decisions regarding computer-assisted works. It cites a February 2022rulingin which the Copyright Office rejected a work that had no human involvement. However, in 2023, the officegranteda copyright to a comic book that incorporated AI-generated images because a human created original elements such as text, arrangement, and modifications. The report argues this approach aligns with prior treatment of technologies like photography: Copyright protection depends on identifiable human creative input, and that input merits protection even if technology assists in producing it.Behind the news:Thefirst partof the Copyright Office’s report on digital replicas, or generated likenesses of a person’s appearance and voice. It found that existing laws don’t provide sufficient protection against unauthorized digital replicas and recommended federal legislation to address the gap. Its findings influenced ongoing discussions in Congress, where proposed bills like the No AI FRAUD Act and the NO FAKES Act aim to regulate impersonation via AI. Additionally, industry groups such as the Authors Guild and entertainment unions have pursued their own agreements with studios and publishers to safeguard performers, artists, and authors from unauthorized digital reproduction. However, no federal law currently defines whether copyright can protect a person’s likeness or performance.Why it matters:The Copyright Office deliberately avoided prescribing rigid criteria for the types or degrees of human input that are sufficient for copyright. Such determinations require nuanced evaluation case by case. This flexible approach accommodates the diverse ways creative people use AI as well as unforeseen creative possibilities of emerging technology.We’re thinking:Does copyright bar the use of protected works to train AI systems? The third part of the Copyright Office’s report — no indication yet as to when to expect it — will address this question. The answer could have important effects on both the arts and AI development.Designer MaterialsMaterials that have specific properties are essential to progress in critical technologies like solar cells and batteries. A machine learning model designs new materials to order.What’s new:Researchers at Microsoft and Shenzhen Institute of Advanced Technology proposedMatterGen, a diffusion model that generates a material’s chemical composition and structure from a prompt that specifies a desired property. The model and code areavailableunder a license that allows commercial as well as noncommercial uses without limitation. The trainingdataalso is noncommercially available.How it works:MatterGen’s training followed a two-stage process. In the first stage, it learned to generate materials (specifically crystals — no liquids, gasses, or amorphous solids like glass). In the second, it learned to generate materials given a target mechanical, electronic, magnetic, or chemical property such as magnetic density or bulk modulus (the material’s resistance to compression).MatterGen first learned to remove noise that had been added to 600,000 examples drawn from two datasets. Specifically, it learned to remove noise from three noisy matrices that represented a crystal’s shape (parallelepiped), the type of each atom, and the coordinates of each atom.To incorporate information about properties, the authors added to the diffusion model four vanilla neural networks, each of which took an embedding of the target property. The diffusion model added the output of these networks to its intermediate embeddings at different layers.Then the authors fine-tuned the system to remove added noise from materials that contained property information in their original dataset.At inference, given three matrices of pure noise representing crystal shape, atom types, and atom coordinates, and a prompt specifying the desired property, the diffusion model iteratively removed the noise from all three matrices.Results:The authors generated a variety of materials, and they synthesized one to test whether it had a target property. Specifically, they generated over 8,000 candidates with the target bulk modulus of 200 gigapascals (a measure of resistance to uniform compression), then automatically filtered them based on a number of factors to eliminate material in their dataset and unstable materials. Of the remaining candidates, they chose four manually and successfully synthesized one. The resulting crystal had a measured bulk modulus of 158 gigapascals. (Most materials in the dataset had a bulk modulus of between 0 and 400 gigapascals.)Behind the news:Published in 2023,DiffCSPalso uses a diffusion model to generate the structures of new materials. However, it does so without considering their desired properties.Why it matters:Discovering materials relies mostly on searching large databases of existing materials for those with desired properties or synthesizing new materials and testing their properties by trial and error. Designing new crystals with desired properties at the click of a button accelerates the process dramatically.We’re thinking:While using AI to design materials accelerates an important step, determining whether a hypothesized material can be  manufactured efficiently at scale is still challenging. We look forward to research into AI models that also take into account ease of manufacturing.A MESSAGE FROM DEEPLEARNING.AIIn our latest short course,Long-Term Agentic Memory with LangGraph, learn how to integrate semantic, episodic, and procedural memory into AI workflows. Guided by Harrison Chase, you’ll build a personal email agent with routing, writing, and scheduling tools to automatically ignore and respond to emails, while keep track of facts and past actions over time.Join in for freeShare\nPublishedMar 19, 2025\nPublished\n\nPublished\nMar 19, 2025\nMar 19, 2025\nReading time14min read\nReading time\n\nReading time\n14min read\nShare\nShare\n\nShare\n\nLoading theElevenlabs Text to SpeechAudioNative Player...Dear friends,Last Friday on Pi Day, we held AI Dev 25, a new conference for AI Developers. Tickets had (unfortunately) sold out days after we announced their availability, but I came away energized by the day of coding and technical discussions with fellow AI Builders! Let me share here my observations from the event.What a great group of people at AI Dev 25.Also… look what my fortune cookie from the event said!I'd decided to start AI Dev because while there're great academic AI conferences that disseminate research work (such as NeurIPS, ICML and ICLR) and also great meetings held by individual companies, often focused on each company's product offerings, there were few vendor-neutral conferences for AI developers. With the wide range of AI tools now available, there is a rich set of opportunities for developers to build new things (and to share ideas on how to build things!), but also a need for a neutral forum that helps developers do so.Based on an informal poll, about half the attendees had traveled to San Francisco from outside the Bay Area for this meeting, including many who had come from overseas. I was thrilled by the enthusiasm to be part of this AI Builder community. To everyone who came, thank you!Other aspects of the event that struck me:First, agentic AI continues to be a strong theme. The topic attendees most wanted to hear about (based on free text responses to our in-person survey at the start of the event) was agents!Google's Paige Bailey talked about embedding AI in everything and using a wide range of models to do so. I also particularly enjoyed her demos of Astra and Deep Research agents.Meta's Amit Sangani talked compellingly as usual about open models. Specifically, he described developers fine-tuning smaller models on specific data, resulting in superior performance than with large general purpose models. While there're still many companies using fine-tuning that should really just be prompting, I'm also seeing continued growth of fine-tuning in applications that are reaching scale and that are becoming valuable.Many speakers also spoke about the importance of being pragmatic about what problems we are solving, as opposed to buying into the AGI hype. For example, Nebius' Roman Chernin put it simply: Focusing on solving real problems is important!Lastly, I was excited to hear continued enthusiasm for the Voice Stack. Justin Uberti gave a talk about OpenAI’s realtime audio API to a packed room, with many people pulling out laptops to try things out themselves in code!DeepLearning.AI has a strong “Learner First” mentality; our foremost goal is always to help learners. I was thrilled that a few attendees told me they enjoyed how technical the sessions were, and said they learned many things that they're sure they will use. (In fact, I, too, came away with a few ideas from the sessions!) I was also struck that, both during the talks and at the technical demo booths, the rooms were packed with attendees who were highly engaged throughout the whole day. I'm glad that we were able to have a meeting filled with technical and engineering discussions.I'm delighted that AI Dev 25 went off so well, and am grateful to all the attendees, volunteers, speakers, sponsors, partners, and team members that made the event possible. I regretted only that the physical size of the event space prevented us from admitting more attendees this time. There is something magical about bringing people together physically to share ideas, make friends, and to learn from and help each other. I hope we'll be able to bring even more people together in the future.Keep building!AndrewP.S. I'm thrilled to share our newest course series: theData Analytics Professional Certificate! Data analytics remains one of the core skills of data science and AI, and this professional certificate takes you up to being job-ready for this. Led by Netflix data science leader Sean Barnes, this certificate gives you hands-on experience with essential tools like SQL, Tableau, and Python, while teaching you to use Generative AI effectively as a thought partner in your analyses. Labor economists project a 36% growth in data science jobs by 2033. I'm excited to see rising demand for data professionals, since working with data is such a powerful way to improve decision-making, whether in business, software development, or your private life. Data skills create opportunities at every level—I’m excited to see where they take you!Sign up here!A MESSAGE FROM DEEPLEARNING.AITheData Analytics Professional Certificateis available now! This program equips you with data analytics skills—from foundations to job-ready. Learn statistical techniques combined with newly emerging generative AI workflows.Enroll nowNewsEqually Fluent in Many LanguagesMultilingual AI models often suffer uneven performance across languages, especially in multimodal tasks. A pair of lean models counters this trend with consistent understanding of text and images across major languages.What’s new:A team at Cohere led by Saurabh Dash releasedAya Vision, a family of multilingual vision-language models with downloadable weights in 8 billion- and 32-billion-parameter sizes.Input/output:Text and images in (up to 2,197 image tokens, up to 16,000 tokens total), text out (up to 4,000 tokens).Availability:Free viaWhatsApporCohere Playground. Weights available todownload,but licensed only for noncommercial uses.Features:Multilingual input and output in 23 languages.Undisclosed:Knowledge cutoff, training datasets, adapter architecture.How it works:Each modelcomprisesa pretrained large language model (Aya Expanse for the 32B model, C4AI Command R7B for the 8B version), a pretrained vision encoder (SigLIP 2), and a vision-language adapter (“connector”) of unspecified architecture.To establish basic vision-language understanding, the team froze the vision encoder and language model and trained the vision-language connector.They fine-tuned the vision-language connector and language model on multimodal tasks. To build the fine-tuning dataset, they generated synthetic annotations for various English-language datasets and translated a large amount of data into a variety of languages. They rephrased the translations to add fluency and variety, particularly for languages with little real-world data, by matching generated pairs with the original synthetic samples.They merged the language model with the fine-tuned vision-language model using an undisclosed method that preserved text capabilities while adding vision understanding.After proving this method for 8 billion parameters, they scaled up the recipe to 32 billion parameters.Performance:To test the model, the team built and released two benchmarks:m-WildVision, a multilingual version ofWild Vision Bench’s arena-style competition for discussion of images, andAyaVisionBench, 135 image-question pairs in each language that cover nine tasks including captioning images, understanding charts, recognizing characters in images, visual reasoning, and converting screenshots to code. On these two benchmarks, Aya Vision 8B and 32B outperformed larger competitors, as judged by Claude 3.7 Sonnet.In head-to-head competitions on AyaVisionBench, Aya Vision 8B won up to 79 percent of the time against six competitors of similar size. On m-WildVision, it achieved 81 percent when compared to vision-language models of similar size including Qwen2.5-VL 7B, Pixtral 12B, Gemini Flash 1.5 8B, and Llama-3.2 11B Vision. Aya Vision 8B won 63 percent of the time against Llama-3.2 90B Vision, a model more than 10 times its size.On both benchmarks, Aya Vision 32B outperformed vision-language models more than twice its size including Llama-3.2 90B Vision, Molmo 72B, and Qwen2.5-VL 72B. On AyaVisionBench, it won between 50 and 64 percent of the time. On WildVision, it achieved win rates between 52 percent and 72 percent across all languages.Behind the news:Aya Vision builds on the Cohere-ledAyainitiative, a noncommercial effort to build models that perform consistently well in all languages, especially languages that lack high-quality training data. The project started with a multilingual text model (Aya Expanse), added vision (Aya Vision), and plans to eventually add video and audio.Why it matters:Multilingual vision-language models often perform less well in low-resource languages, and the gap widens when they process media other than text. Aya Vision’s recipe for augmenting synthetic data with successively refined translations may contribute to more universally capable models. Aya Vision is available on the global messaging platform WhatsApp, where it can be used to translate text and images in all 23 of its current languages.We’re thinking:Multilingual vision models could soon help non-native speakers decipher Turkish road signs, Finnish legal contracts, and Korean receipts. We look forward to a world in which understanding any scene or document is as effortless in Swahili as it is in English.Science Research Proposals Made to OrderAn AI agent synthesizes novel scientific research hypotheses. It's already making an impact in biomedicine.What’s new:Google introducedAI co-scientist, a general multi-agent system designed to generate in-depth research proposals within constraints specified by the user. The team generated and evaluated proposals for repurposing drugs, identifying drug targets, and explaining antimicrobial resistance in real-world laboratories. It’s available to research organizations on a limited basis.How it works:AI co-scientist accepts a text description of a research goal, including relevant constraints or ideas. In response, it generates research proposals and reviews, ranks, and improves them using seven agents based on Google’s Gemini 2.0 family of large language models. The completed proposals include sections that explain background, unmet needs, a proposed solution, goals, hypotheses, reasoning, study steps, and relevant articles. The agents take feedback and outputs from other agents to perform their prompted task simultaneously.The supervisor agent periodically determines how often to run the other six agents, how important their output is, and whether the system is finished. To accomplish this, it computes statistics that represent the number of proposals generated so far, how many have been reviewed, and so on.The generation agent generates a list of proposals. It searches the web for relevant research articles, identifies testable assumptions, and debates with itself to improve ambiguous statements and adhere to constraints.The reflection agent filters the generated proposals according to correctness, quality, safety, and novelty. First, it reviews a proposal without web search and discards obviously bad proposals. Then it reviews each proposal against literature it finds online. It breaks down and checks the proposal’s assumptions, checks whether the proposal might explain some observations in previous work, and simulates the proposed experiment (via text generation, similar to how a person performs a thought experiment).The proximity agents compute similarity between proposals to avoid redundancy.The ranking agent determines the best proposals according to a tournament. It examines one pair of proposals at a time (including reviews from the reflection agent) and debates itself to pick the better one. To save computation, it prioritizes comparing similar proposals, new proposals, and highest-ranking proposals.The evolution agent generates new proposals by improving existing ones. It does this in several different ways, including simplifying current ideas, combining top-ranking ideas, and generating proposals that are very different from current ones.The meta-review agent identifies common patterns in the reflection agent’s reviews and the ranking agent’s debates. Its feedback goes to the reflection and generation agents, which use it to address common factors in future reviews and avoid generating similar proposals, respectively.Results:AI co-scientist achieved a number of impressive biomedical results in tests.Google researchers generated proposals for experiments that would repurpose drugs to treat acute myeloid leukemia. They shared the 30 highest-ranked proposals with human experts, who chose five for lab tests. Of the five drugs tested, three killed acute myeloid leukemia cells.Experts selected three among 15 top-ranked generated proposals that proposed repurposing existing drugs to treat liver fibrosis. Two significantly inhibited liver fibrosis without being toxic to general cells. (Prior to this research, one of the drugs was approved by the United States Food and Drug Administration for a different illness, which may lead to a new treatment for liver fibrosis.)AI co-scientistinventeda hypothesis to explain how microbes become resistant to antibiotics. Human researchers had proposed and experimentally validated the same hypothesis, but theirworkhad not yet been published at the time, and AI co-scientist did not have access to it.Behind the news:A few AI systems have begun to produce original scientific work. For instance, a modelgenerated research proposalsthat human judges deemed more novel than proposals written by flesh-and-blood scientists, and an agentic workflowproduced research papersthat met standards for acceptance by top conferences.Why it matters:While previous work used agentic workflows to propose research ideas on a general topic, this work generates proposals for specific ideas according to a researcher’s constraints (for example, a researcher could specify that a novel medical treatment for a specific disease only consider drugs already approved for human trials for other uses) and further instructions. AI co-scientist can take feedback at any point, allowing humans to collaborate with the machine: People provide ideas, feedback, and guidance for the model, and the model researches and proposes ideas in return.We’re thinking:I asked my AI system to propose a new chemical experiment. But there was no reaction!Some AI-Generated Works Are CopyrightableThe United States Copyright Office determined that existing laws are sufficient to decide whether a given AI-generated work is protected by copyright, making additional legislation unnecessary.What’s new:AI-generated works qualify for copyright if a human being contributed enough creative input, according to thesecond partof what will be a three-part report on artificial intelligence and copyright law.How it works:The report states that “the outputs of generative AI can be protected by copyright only where a human author has determined sufficient expressive elements.” In other words, humans and AI can collaborate on creative works, but copyright protection applies only if a human shapes the AI-generated material beyond simply supplying a prompt.The report rejects the argument that protecting AI-generated works requires a new legal framework. Instead, it argues that copyright law already establishes clear standards of authorship and originality.Human authors or artists retain copyright over creative contributions in the form of selection, coordination, and modification of generated outputs. Selection refers to curating AI-generated elements. Coordination involves organizing multiple generated outputs into a cohesive work. Modification is altering generated material in a way that makes it original. They retain copyright even if AI processes their creative work. They lose it only if the generated output is genuinely transformative.The report emphasizes continuity with past decisions regarding computer-assisted works. It cites a February 2022rulingin which the Copyright Office rejected a work that had no human involvement. However, in 2023, the officegranteda copyright to a comic book that incorporated AI-generated images because a human created original elements such as text, arrangement, and modifications. The report argues this approach aligns with prior treatment of technologies like photography: Copyright protection depends on identifiable human creative input, and that input merits protection even if technology assists in producing it.Behind the news:Thefirst partof the Copyright Office’s report on digital replicas, or generated likenesses of a person’s appearance and voice. It found that existing laws don’t provide sufficient protection against unauthorized digital replicas and recommended federal legislation to address the gap. Its findings influenced ongoing discussions in Congress, where proposed bills like the No AI FRAUD Act and the NO FAKES Act aim to regulate impersonation via AI. Additionally, industry groups such as the Authors Guild and entertainment unions have pursued their own agreements with studios and publishers to safeguard performers, artists, and authors from unauthorized digital reproduction. However, no federal law currently defines whether copyright can protect a person’s likeness or performance.Why it matters:The Copyright Office deliberately avoided prescribing rigid criteria for the types or degrees of human input that are sufficient for copyright. Such determinations require nuanced evaluation case by case. This flexible approach accommodates the diverse ways creative people use AI as well as unforeseen creative possibilities of emerging technology.We’re thinking:Does copyright bar the use of protected works to train AI systems? The third part of the Copyright Office’s report — no indication yet as to when to expect it — will address this question. The answer could have important effects on both the arts and AI development.Designer MaterialsMaterials that have specific properties are essential to progress in critical technologies like solar cells and batteries. A machine learning model designs new materials to order.What’s new:Researchers at Microsoft and Shenzhen Institute of Advanced Technology proposedMatterGen, a diffusion model that generates a material’s chemical composition and structure from a prompt that specifies a desired property. The model and code areavailableunder a license that allows commercial as well as noncommercial uses without limitation. The trainingdataalso is noncommercially available.How it works:MatterGen’s training followed a two-stage process. In the first stage, it learned to generate materials (specifically crystals — no liquids, gasses, or amorphous solids like glass). In the second, it learned to generate materials given a target mechanical, electronic, magnetic, or chemical property such as magnetic density or bulk modulus (the material’s resistance to compression).MatterGen first learned to remove noise that had been added to 600,000 examples drawn from two datasets. Specifically, it learned to remove noise from three noisy matrices that represented a crystal’s shape (parallelepiped), the type of each atom, and the coordinates of each atom.To incorporate information about properties, the authors added to the diffusion model four vanilla neural networks, each of which took an embedding of the target property. The diffusion model added the output of these networks to its intermediate embeddings at different layers.Then the authors fine-tuned the system to remove added noise from materials that contained property information in their original dataset.At inference, given three matrices of pure noise representing crystal shape, atom types, and atom coordinates, and a prompt specifying the desired property, the diffusion model iteratively removed the noise from all three matrices.Results:The authors generated a variety of materials, and they synthesized one to test whether it had a target property. Specifically, they generated over 8,000 candidates with the target bulk modulus of 200 gigapascals (a measure of resistance to uniform compression), then automatically filtered them based on a number of factors to eliminate material in their dataset and unstable materials. Of the remaining candidates, they chose four manually and successfully synthesized one. The resulting crystal had a measured bulk modulus of 158 gigapascals. (Most materials in the dataset had a bulk modulus of between 0 and 400 gigapascals.)Behind the news:Published in 2023,DiffCSPalso uses a diffusion model to generate the structures of new materials. However, it does so without considering their desired properties.Why it matters:Discovering materials relies mostly on searching large databases of existing materials for those with desired properties or synthesizing new materials and testing their properties by trial and error. Designing new crystals with desired properties at the click of a button accelerates the process dramatically.We’re thinking:While using AI to design materials accelerates an important step, determining whether a hypothesized material can be  manufactured efficiently at scale is still challenging. We look forward to research into AI models that also take into account ease of manufacturing.A MESSAGE FROM DEEPLEARNING.AIIn our latest short course,Long-Term Agentic Memory with LangGraph, learn how to integrate semantic, episodic, and procedural memory into AI workflows. Guided by Harrison Chase, you’ll build a personal email agent with routing, writing, and scheduling tools to automatically ignore and respond to emails, while keep track of facts and past actions over time.Join in for free\nLoading theElevenlabs Text to SpeechAudioNative Player...\n\n\nShare\nShare\n\nShare\n",
    "image_urls": [
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--63--2.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/The-Batch-ads-and-exclusive-banners---2025-03-11T092327.321-1.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--64-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--65-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--53-.gif",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--66-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/image--24-.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2024/08/1-9.png"
    ]
  },
  {
    "title": "issue 290",
    "date": null,
    "url": "https://www.deeplearning.ai/the-batch/issue-290/",
    "content": "The Batch\nWeekly Issues\nissue 290\nPublishedFeb 26, 2025\nPublished\n\nPublished\nFeb 26, 2025\nReading time14min read\nReading time\n\nReading time\n14min read\nPublishedFeb 26, 2025Reading time14min readShareLoading theElevenlabs Text to SpeechAudioNative Player...Dear friends,The Voice Stack is improving rapidly. Systems that interact with users via speaking and listening will drive many new applications. Over the past year, I’ve been working closely with DeepLearning.AI, AI Fund, and several collaborators on voice-based applications, and I will share best practices I’ve learned in this and future letters.Foundation models that are trained to directly input, and often also directly generate, audio have contributed to this growth, but they are only part of the story. OpenAI’sRealTime APImakes it easy for developers to write prompts to develop systems that deliver voice-in, voice-out experiences. This is great for building quick-and-dirty prototypes, and it also works well for low-stakes conversations where making an occasional mistake is okay. I encourage you to try it!However, compared to text-based generation, it is still hard to control the output of voice-in voice-out models. In contrast to directly generating audio, when we use an LLM to generate text, we have many tools for building guardrails, and we can double-check the output before showing it to users. We can also use sophisticated agentic reasoning workflows to compute high-quality outputs. Before a customer-service agent shows a user the message, “Sure, I’m happy to issue a refund,” we can make sure that (i) issuing the refund is consistent with our business policy and (ii) we will call the API to issue the refund (and not just promise a refund without issuing it).In contrast, the tools to prevent a voice-in, voice-out model from making such mistakes are much less mature.In my experience, the reasoning capability of voice models also seems inferior to text-based models, and they give less sophisticated answers. (Perhaps this is because voice responses have to be more brief, leaving less room for chain-of-thought reasoning to get to a more thoughtful answer.)When building applications where I need a high degree of control over the output, I use agentic workflows to reason at length about the user’s input. In voice applications, this means I end up using a pipeline that includes speech-to-text (STT, also known as ASR, or automatic speech recognition) to transcribe the user’s words, then processes the text using one or more LLM calls, and finally returns an audio response to the user via TTS (text-to-speech). This STT → LLM/Agentic workflow → TTS pipeline, where the reasoning is done in text, allows for more accurate responses.However, this process introduces latency, and users of voice applications are very sensitive to latency. When DeepLearning.AI worked with RealAvatar (an AI Fund portfolio company led by Jeff Daniel) to build an avatar of me, we found that getting TTS to generate a voice that sounded like me was not very hard, but getting it to respond to questions using words similar to those I would choose was. Even after a year of tuning our system — starting with iterating on multiple, long, mega-prompts and eventually developing complex agentic workflows — it remains a work in progress. You can play with ithere.Initially, this agentic workflow incurred 5-9 seconds of latency, and having users wait that long for responses led to a bad experience. To address this, we came up with the following latency reduction technique. The system quickly generates a pre-response (short for preliminary response) that can be uttered quickly, which buys time for an agentic workflow to generate a more thoughtful, full response. (We’re grateful to LiveKit’s CEO Russ d’Sa and team for helping us get this working.) This is similar to how, if you were to ask me a complicated question, I might say “Hmm, let me think about that” or “Sure, I can help with that” — that’s the pre-response — while thinking about what my full response might be.I think generating a pre-response followed by a full response, to quickly acknowledge the user’s query and also reduce the perceived latency, will be an important technique, and I hope many teams will find this useful. Our goal was to approach human face-to-face conversational latency, which is around 0.3-1 seconds. RealAvatar and DeepLearning.AI, through our efforts on the pre-response and other optimizations, have reduced the system’s latency to around 0.5-1 seconds.Months ago, sitting in a coffee shop, I was able to buy a phone number on Twilio and hook it up to an STT → LLM → TTS pipeline in just hours. This enabled me to talk to my own LLM using custom prompts. Prototyping voice applications is much easier than most people realize!Building reliable, scaled production applications takes longer, of course, but if you have a voice application in mind, I hope you’ll start building prototypes and see how far you can get! I’ll keep building voice applications and sharing best practices and voice-related technology trends in future letters.Keep building!AndrewA MESSAGE FROM DEEPLEARNING.AIAI coding agents do more than autocomplete. They help you debug, refactor, and design applications. Learn how coding agents work under the hood, so you can streamline your projects and build applications such as a Wikipedia data-analysis app!Enroll Now.NewsReading Minds, No Brain Implant RequiredTo date, efforts to decode what people are thinking from their brain waves often relied on electrodes implanted in the cortex. New work used devices outside the head to pick up brain signals that enabled an AI system, as a subject typed, to accurately guess what they were typing.What’s new:Researchers presentedBrain2Qwerty, a non-invasive method to translate brain waves into text. In addition, their workshed lighton how the brain processes language. The team included people at Meta, Paris Sciences et Lettres University, Hospital Foundation Adolphe de Rothschild, Basque Center on Cognition, Brain and Language, Basque Foundation for Science, Aix-Marseille University, and Paris Cité University.Gathering brainwave data:The authors recorded the brain activity of 35 healthy participants who typed Spanish-language sentences. The participants were connected to either an electroencephalogram (EEG), which records the brain’s electrical activity via electrodes on the scalp, or a magnetoencephalogram (MEG), which records magnetic activity through a device that surrounds the head but isn’t attached. 15 participants used each device and five used both.Participants were asked to read and memorize short sentences of 5 to 8 words. They were shown one word at a time.After a short waiting period, participants were asked to type the sentence. They could not see what they typed.The EEG dataset comprised around 4,000 sentences and 146,000 characters, while the MEG dataset comprised around 5,100 sentences and 193,000 characters.Thoughts into text:Brain2Qwerty used a system made up of a convolutional neural network, transformer, and a9-gram character-level language modelpretrained on Spanish Wikipedia. The system classified the text a user typed from their brain activity. The authors trained separate systems on MEG and EEG data.The convolutional neural network segmented brain activity into windows of 500 milliseconds each. The transformer took these windows as input and generated possible text characters and their probabilities. The two models learned to predict characters jointly.The pretrained language model, given the most recently predicted nine characters,  estimated the probability of the next character.At inference, the authors used a weighted average of probabilities from the transformer and language model. From that average, they computed the most likely sequence of characters as the final output.Results.The authors’ MEG model achieved 32 percent character error rate (CER), much higher accuracy than the EEG competitors. Their EEG system outperformedEEGNet, a model designed to process EEG data that had been trained on the authors’ EEG data. It achieved 67 percent CER, while EEGNet achieved 78 percent CER.Behind the news:For decades, researchers have used learning algorithms to interpret various aspects of brain activity with varying degrees of success. In recent years, they’ve used neural networks togeneratetextandspeechfrom implanted electrodes, generateimagesof whatpeople seewhile in an fMRI, and enable people tocontrol robotsusing EEG signals.Why it matters:In research into interpreting brain signals, subjects who are outfitted with surgical implants typically have supplied the highest-quality brain signals. fMRI scans, while similarly noninvasive, are less precise temporally, which makes them less useful for monitoring or predicting language production. Effective systems based on MEG, which can tap brain signals precisely without requiring participants to undergo surgery, open the door to collecting far more data, training far more robust models, and conducting a wider variety of experiments.We’re thinking:The privacy implications of such research may be troubling, but keep in mind that Brain2Qwerty’s MEG system, which was the most effective approach tested, required patients to spend extended periods of time sitting still in a shielded room. We aren’t going to read minds in the wild anytime soon.Big AI Spending Continues to RiseTop AI companies announced plans to dramatically ramp up their spending on AI infrastructure.What’s new:Alphabet, Amazon, Meta, Microsoft, and others willboosttheir capital spending dramatically in 2025, pouring hundreds of billions of dollars into data centers where they process AI training, the companies said in their most recent quarterly reports. The surge suggests that more-efficient approaches to training models won’t dampen the need for greater and greater processing power.How it works:Capital expenditures include long-term purchases like land, buildings, and computing hardware rather than recurring costs like salaries or electricity. The AI leaders signaled that most of this spending will support their AI efforts.Amazon has budgeted $105 billion to capital expenditures in 2025, 35 percent more than last year. CFO Brian Olsavskyattributedthe increase to the company’s need to satisfy demand for AI services and tech infrastructure. CEO Andy Jassy emphasized that it reflects strong demand for AI and dismissed concerns that cheaper alternatives like DeepSeek would reduce overall spending. (Disclosure: Andrew Ng is a member of Amazon’s board of directors.)Alphabet allocated $75 billion to capital expenditures, up from $52.5 billion last year, to support growth in Google Services, Google Cloud, and Google DeepMind. The companyindicatedthat most of this money would go to technical infrastructure including data centers and networking.Meta’s annual capital expenditures will amount to $65 billion, a huge jump from $39.2 billion last year. CEO Mark Zuckerbergarguedthat such spending on AI infrastructure and chips is needed to assure the company’s lead in AI and integrate the technology into its social platforms.Microsoft said it would put around $80 billion — a figure that analystsexpectto rise to $94 billion — into capital expenditures in 2025, another big jump following an 83 percent rise from 2023 to 2024. Most of this investment willsupportcloud infrastructure, servers, CPUs, and GPUs to meet demand for AI.OpenAI, Oracle, SoftBank, and othersannouncedStargate, a project that intends immediately to put $100 billion — $500 billion over time — into data centers that would support development of artificial general intelligence. Elon Musk claimed in atweetthat the investors “don’t actually have the money,” raising questions about the announcement’s veracity.Behind the news:DeepSeek initiallysurprisedmany members of the AI community by claiming to have trained a high-performance large language model at a fraction of the usual cost.Specifically, DeepSeek-R1 reportedly cost less than $6 million and 2,048 GPUs to train. (For comparison, Anthropic’s Claude 3.5 Sonnet cost “a few $10Ms to train,”accordingto CEO Dario Amodei, and GPT-4 cost about $100 million to train,accordingto CEO Sam Altman.) Follow-up reports shed light on DeepSeek’s actual infrastructure and noted that the $6 million figure represented only DeepSeek-R1’s final training run, a small fraction of the total development cost.Furthermore, while initial reports said DeepSeek piggy-backed on a 10,000-GPU supercomputer owned by its parent company High-Flyer, a hedge fund, research firm SemiAnalysisquestionedwhether DeepSeek relied on High-Flyer’s hardware. DeepSeek has spent around $1.6 billion on a cluster of 50,000 Nvidia GPUs,Tom’s Hardwarereported.Initial excitement over the company’s low training costs gave way toconcernsabout data sovereignty, security, and the cost of running DeepSeek-R1, which generates a larger number of reasoning tokens than similar models.Why it matters:DeepSeek-R1’s purported training cost fueled fears that demand for AI infrastructure would cool, but the top AI companies’ plans show that it’s not happening yet. A possible explanation lies in theJevons Paradox, a 19th-century economic theory named after the English economist William Stanley Jevons. As a valuable product becomes more affordable, demand doesn’t fall, it rises. According to this theory, even if training costs tumble, the world will demand ever greater processing power for inference.We’re thinking:DeepSeek’s low-cost technology momentarily rattled investors who had expected the next big gains would come from the U.S. rather than China. But DeepSeek’s efficiency follows a broader pattern we’ve seen for years: The AI community steadily wrings better performance from less processing power.Deepfake Developers Appropriate Celebrity LikenessesA viral deepfake video showed media superstars who appeared to support a cause — but it was made without their participation or permission.What’s new:Thevideoshows AI-generated likenesses of 20 Jewish celebrities ranging from Scarlett Johansson to Simon & Garfunkel. They appear wearing T-shirts that feature a middle finger inscribed with the Star of David above the word “KANYE.” The clip, which ends with the words “Enough is enough” followed by “Join the fight against antisemitism,” responds to rapper Kanye West, who sold T-shirts emblazoned with swastikas on Shopify before the ecommerce platform shut down his store.Who created it:Israeli developers Guy Bar and Ori Bejerano generated the video to spark a conversation about antisemitism, BartoldThe Jerusalem Post. The team didn’t reveal the AI models, editing tools, or techniques used to produce the video.Johansson reacts:Scarlett Johanssondenouncedthe clip and urged the U.S. to regulate deepfakes. In 2024, sheobjectedto one of the voices of OpenAI’s voice assistant, which she claimed resembled her own voice, leading the company to remove that voice from its service. The prior year, her attorneys ordered a company to stop using an unauthorized AI-generated version of her image in an advertisement.Likenesses up for grabs:Existing U.S. laws protect some uses of a celebrity’s likeness in the form of a photo, drawing, or human lookalike, but they don’t explicitly protect against reproduction by AI systems. This leaves celebrities and public figures with limited recourse against unauthorized deepfakes.U.S. lawmakers haveintroducedlegislation that targets deepfake pornography, but it covers only sexually explicit deepfakes.Theright of publicity, which falls under trademark law, offers some protection against the unauthorized use of a person’s identity. However, it varies by state and provides broad exceptions for news, satire, and fine art.While some states outlaw misappropriation of names or likenesses, existing laws primarily target traditional forms of image misuse, such as false endorsements or unauthorized commercial exploitation. They do not explicitly cover AI-generated deepfakes used for noncommercial, political, or satirical purposes.A 2023agreementbetween Hollywood actors and movie studios protects actors against such uses of AI-generated images of their likenesses in films. However, it doesn’t apply to deepfakes that are produced independently for distribution via social media networks.Why it matters:Non-consensual deepfake pornography is widely condemned, but AI enables many other non-consensual uses of someone’s likeness, and their limits are not yet consistently coded into law. If the creators of the video that appropriated the images of celebrities had responded to Johansson’s criticism with an AI-generated satire, would that be a legitimate exercise of free speech or another misuse of AI? Previously, an ambiguous legal framework may have been acceptable because such images, and thus lawsuits arising from them, were uncommon. Now, as synthetic likenesses of specific people become easier to generate, clear legal boundaries are needed to keep misuses in check.We’re thinking:Creating unauthorized lookalikes of existing people is not a good way to advance any cause, however worthy. Developers should work with businesses policymakers to establish standards that differentiate legitimate uses from unfair or misleading exploitation.Reasoning in Vectors, Not TextAlthough large language models can improve their performance by generating a chain of thought (CoT) — intermediate text tokens that break down the process of responding to a prompt into a series of steps — much of the CoT text is aimed at maintaining fluency (such as “a”, “of”, “we know that”) rather than reasoning (“a² + b² = c²”). Researchers addressed this inefficiency.What’s new:Shibo Hao, Sainbayar Sukhbaatar, and colleagues at Meta and University of California San Diego introducedCoconut(Chain of Continuous Thought), a method that trains large language models (LLMs) to process chains of thought as vectors rather than words.Key insight:A large language model (LLM) can be broken into an embedding layer, transformer, and classification layer. To generate the next text token from input text, the embedding layer embeds the text; given the text, the transformer outputs a hidden vector; and the classification layer maps the vector to text-token probabilities. Based on these probabilities, a decoding algorithm selects the next token to generate, which feeds back into the input text sequence to generate the next vector, and so on. When a model generates a CoT, committing to a specific word at each step limits the information available to the meanings of the words generated so far, while a vector could represent multiple possible words. Using vectors instead of text enables the CoT to encode richer information.How it works:The authors built three LLMs by fine-tuning a pre-trainedGPT-2on three datasets of prompts, CoTs, and final outputs:GSM8k(grade-school math word problems);ProntoQA(questions and answers about fictional concepts expressed in made-up words, including synthetic CoTs in natural language); and (3) ProsQA, a more challenging question-answering dataset introduced by the authors, inspired by ProntoQA but with longer reasoning steps.Fine-tuning began with supervised training. The LLM learned to generate the text in the training set, including the CoT and final answers. As usual, the last-generated text token was fed back as input to produce the next token.Fine-tuning then progressed through k stages for each example. At each stage, the authors replaced a sentence in the CoT text with a thought vector (or two) to build a sequence of k replaced sentences. The start and end of the chain of thought vectors were marked by two special tokens. During vector steps, the LLM fed its output vectors back as input without decoding them into text. The LLM learned to generate only the remaining text tokens, not the thought vectors, which encouraged it to optimize its vector-based reasoning indirectly.During inference, the LLM generated a special token to mark the start of the chain of vectors. From this point, it fed back its output vectors, bypassing text decoding for six steps. Afterward, the LLM switched back to generating text for final output.Results:The authors compared their method to a pretrained GPT-2 that was fine-tuned on the same datasets to predict the next word, including reasoning.On ProntoQA, Coconut outperformed the fine-tuned GPT-2 while producing far fewer interim vectors (Coconut) or tokens (baseline LLMs). It achieved 99.8 percent accuracy after generating nine vectors (or tokens) on average, while GPT-2 achieved 98.8 percent accuracy using 92.5 text tokens.Coconut excelled on ProsQA’s more complex questions. It achieved 97.0 percent accuracy after generating 14.2 vectors (or tokens) on average, while GPT-2 achieved 77.5 percent accuracy after generating 49.4 text tokens on average.Yes, but:On GSM8k, Coconut achieved 34.1 percent accuracy, while the baseline LLM achieved 42.9 percent. However, it generated significantly fewer vectors and tokens than the CoT generated tokens. Coconut generated 8.2 vectors on average compared to the baseline LLM’s 25 text tokens.Why it matters:A traditional CoT commits to a single word at each step and thus encodes one reasoning path in a single CoT. Vectors are less interpretable to humans than language, but the model’s output layer can still decode the thought vectors into probabilities over tokens. Further, inspecting the distribution of words stored along all continuous CoT vectors offers a way to understand multiple potential thought paths stored in one continuous CoT.We’re thinking:LLMs typically learn to reason over text, mainly because text data is widely available to train on. In contrast, neuroscience shows that the part of the human brain responsible for language largelygoes quietduring reasoning tasks, which suggests that explicit language is not a key mechanism for reasoning. Coconut takes an intriguing step to enable LLMs to explore representations that don’t encode the limitations of language.Share\nPublishedFeb 26, 2025\nPublished\n\nPublished\nFeb 26, 2025\nFeb 26, 2025\nReading time14min read\nReading time\n\nReading time\n14min read\nShare\nShare\n\nShare\n\nLoading theElevenlabs Text to SpeechAudioNative Player...Dear friends,The Voice Stack is improving rapidly. Systems that interact with users via speaking and listening will drive many new applications. Over the past year, I’ve been working closely with DeepLearning.AI, AI Fund, and several collaborators on voice-based applications, and I will share best practices I’ve learned in this and future letters.Foundation models that are trained to directly input, and often also directly generate, audio have contributed to this growth, but they are only part of the story. OpenAI’sRealTime APImakes it easy for developers to write prompts to develop systems that deliver voice-in, voice-out experiences. This is great for building quick-and-dirty prototypes, and it also works well for low-stakes conversations where making an occasional mistake is okay. I encourage you to try it!However, compared to text-based generation, it is still hard to control the output of voice-in voice-out models. In contrast to directly generating audio, when we use an LLM to generate text, we have many tools for building guardrails, and we can double-check the output before showing it to users. We can also use sophisticated agentic reasoning workflows to compute high-quality outputs. Before a customer-service agent shows a user the message, “Sure, I’m happy to issue a refund,” we can make sure that (i) issuing the refund is consistent with our business policy and (ii) we will call the API to issue the refund (and not just promise a refund without issuing it).In contrast, the tools to prevent a voice-in, voice-out model from making such mistakes are much less mature.In my experience, the reasoning capability of voice models also seems inferior to text-based models, and they give less sophisticated answers. (Perhaps this is because voice responses have to be more brief, leaving less room for chain-of-thought reasoning to get to a more thoughtful answer.)When building applications where I need a high degree of control over the output, I use agentic workflows to reason at length about the user’s input. In voice applications, this means I end up using a pipeline that includes speech-to-text (STT, also known as ASR, or automatic speech recognition) to transcribe the user’s words, then processes the text using one or more LLM calls, and finally returns an audio response to the user via TTS (text-to-speech). This STT → LLM/Agentic workflow → TTS pipeline, where the reasoning is done in text, allows for more accurate responses.However, this process introduces latency, and users of voice applications are very sensitive to latency. When DeepLearning.AI worked with RealAvatar (an AI Fund portfolio company led by Jeff Daniel) to build an avatar of me, we found that getting TTS to generate a voice that sounded like me was not very hard, but getting it to respond to questions using words similar to those I would choose was. Even after a year of tuning our system — starting with iterating on multiple, long, mega-prompts and eventually developing complex agentic workflows — it remains a work in progress. You can play with ithere.Initially, this agentic workflow incurred 5-9 seconds of latency, and having users wait that long for responses led to a bad experience. To address this, we came up with the following latency reduction technique. The system quickly generates a pre-response (short for preliminary response) that can be uttered quickly, which buys time for an agentic workflow to generate a more thoughtful, full response. (We’re grateful to LiveKit’s CEO Russ d’Sa and team for helping us get this working.) This is similar to how, if you were to ask me a complicated question, I might say “Hmm, let me think about that” or “Sure, I can help with that” — that’s the pre-response — while thinking about what my full response might be.I think generating a pre-response followed by a full response, to quickly acknowledge the user’s query and also reduce the perceived latency, will be an important technique, and I hope many teams will find this useful. Our goal was to approach human face-to-face conversational latency, which is around 0.3-1 seconds. RealAvatar and DeepLearning.AI, through our efforts on the pre-response and other optimizations, have reduced the system’s latency to around 0.5-1 seconds.Months ago, sitting in a coffee shop, I was able to buy a phone number on Twilio and hook it up to an STT → LLM → TTS pipeline in just hours. This enabled me to talk to my own LLM using custom prompts. Prototyping voice applications is much easier than most people realize!Building reliable, scaled production applications takes longer, of course, but if you have a voice application in mind, I hope you’ll start building prototypes and see how far you can get! I’ll keep building voice applications and sharing best practices and voice-related technology trends in future letters.Keep building!AndrewA MESSAGE FROM DEEPLEARNING.AIAI coding agents do more than autocomplete. They help you debug, refactor, and design applications. Learn how coding agents work under the hood, so you can streamline your projects and build applications such as a Wikipedia data-analysis app!Enroll Now.NewsReading Minds, No Brain Implant RequiredTo date, efforts to decode what people are thinking from their brain waves often relied on electrodes implanted in the cortex. New work used devices outside the head to pick up brain signals that enabled an AI system, as a subject typed, to accurately guess what they were typing.What’s new:Researchers presentedBrain2Qwerty, a non-invasive method to translate brain waves into text. In addition, their workshed lighton how the brain processes language. The team included people at Meta, Paris Sciences et Lettres University, Hospital Foundation Adolphe de Rothschild, Basque Center on Cognition, Brain and Language, Basque Foundation for Science, Aix-Marseille University, and Paris Cité University.Gathering brainwave data:The authors recorded the brain activity of 35 healthy participants who typed Spanish-language sentences. The participants were connected to either an electroencephalogram (EEG), which records the brain’s electrical activity via electrodes on the scalp, or a magnetoencephalogram (MEG), which records magnetic activity through a device that surrounds the head but isn’t attached. 15 participants used each device and five used both.Participants were asked to read and memorize short sentences of 5 to 8 words. They were shown one word at a time.After a short waiting period, participants were asked to type the sentence. They could not see what they typed.The EEG dataset comprised around 4,000 sentences and 146,000 characters, while the MEG dataset comprised around 5,100 sentences and 193,000 characters.Thoughts into text:Brain2Qwerty used a system made up of a convolutional neural network, transformer, and a9-gram character-level language modelpretrained on Spanish Wikipedia. The system classified the text a user typed from their brain activity. The authors trained separate systems on MEG and EEG data.The convolutional neural network segmented brain activity into windows of 500 milliseconds each. The transformer took these windows as input and generated possible text characters and their probabilities. The two models learned to predict characters jointly.The pretrained language model, given the most recently predicted nine characters,  estimated the probability of the next character.At inference, the authors used a weighted average of probabilities from the transformer and language model. From that average, they computed the most likely sequence of characters as the final output.Results.The authors’ MEG model achieved 32 percent character error rate (CER), much higher accuracy than the EEG competitors. Their EEG system outperformedEEGNet, a model designed to process EEG data that had been trained on the authors’ EEG data. It achieved 67 percent CER, while EEGNet achieved 78 percent CER.Behind the news:For decades, researchers have used learning algorithms to interpret various aspects of brain activity with varying degrees of success. In recent years, they’ve used neural networks togeneratetextandspeechfrom implanted electrodes, generateimagesof whatpeople seewhile in an fMRI, and enable people tocontrol robotsusing EEG signals.Why it matters:In research into interpreting brain signals, subjects who are outfitted with surgical implants typically have supplied the highest-quality brain signals. fMRI scans, while similarly noninvasive, are less precise temporally, which makes them less useful for monitoring or predicting language production. Effective systems based on MEG, which can tap brain signals precisely without requiring participants to undergo surgery, open the door to collecting far more data, training far more robust models, and conducting a wider variety of experiments.We’re thinking:The privacy implications of such research may be troubling, but keep in mind that Brain2Qwerty’s MEG system, which was the most effective approach tested, required patients to spend extended periods of time sitting still in a shielded room. We aren’t going to read minds in the wild anytime soon.Big AI Spending Continues to RiseTop AI companies announced plans to dramatically ramp up their spending on AI infrastructure.What’s new:Alphabet, Amazon, Meta, Microsoft, and others willboosttheir capital spending dramatically in 2025, pouring hundreds of billions of dollars into data centers where they process AI training, the companies said in their most recent quarterly reports. The surge suggests that more-efficient approaches to training models won’t dampen the need for greater and greater processing power.How it works:Capital expenditures include long-term purchases like land, buildings, and computing hardware rather than recurring costs like salaries or electricity. The AI leaders signaled that most of this spending will support their AI efforts.Amazon has budgeted $105 billion to capital expenditures in 2025, 35 percent more than last year. CFO Brian Olsavskyattributedthe increase to the company’s need to satisfy demand for AI services and tech infrastructure. CEO Andy Jassy emphasized that it reflects strong demand for AI and dismissed concerns that cheaper alternatives like DeepSeek would reduce overall spending. (Disclosure: Andrew Ng is a member of Amazon’s board of directors.)Alphabet allocated $75 billion to capital expenditures, up from $52.5 billion last year, to support growth in Google Services, Google Cloud, and Google DeepMind. The companyindicatedthat most of this money would go to technical infrastructure including data centers and networking.Meta’s annual capital expenditures will amount to $65 billion, a huge jump from $39.2 billion last year. CEO Mark Zuckerbergarguedthat such spending on AI infrastructure and chips is needed to assure the company’s lead in AI and integrate the technology into its social platforms.Microsoft said it would put around $80 billion — a figure that analystsexpectto rise to $94 billion — into capital expenditures in 2025, another big jump following an 83 percent rise from 2023 to 2024. Most of this investment willsupportcloud infrastructure, servers, CPUs, and GPUs to meet demand for AI.OpenAI, Oracle, SoftBank, and othersannouncedStargate, a project that intends immediately to put $100 billion — $500 billion over time — into data centers that would support development of artificial general intelligence. Elon Musk claimed in atweetthat the investors “don’t actually have the money,” raising questions about the announcement’s veracity.Behind the news:DeepSeek initiallysurprisedmany members of the AI community by claiming to have trained a high-performance large language model at a fraction of the usual cost.Specifically, DeepSeek-R1 reportedly cost less than $6 million and 2,048 GPUs to train. (For comparison, Anthropic’s Claude 3.5 Sonnet cost “a few $10Ms to train,”accordingto CEO Dario Amodei, and GPT-4 cost about $100 million to train,accordingto CEO Sam Altman.) Follow-up reports shed light on DeepSeek’s actual infrastructure and noted that the $6 million figure represented only DeepSeek-R1’s final training run, a small fraction of the total development cost.Furthermore, while initial reports said DeepSeek piggy-backed on a 10,000-GPU supercomputer owned by its parent company High-Flyer, a hedge fund, research firm SemiAnalysisquestionedwhether DeepSeek relied on High-Flyer’s hardware. DeepSeek has spent around $1.6 billion on a cluster of 50,000 Nvidia GPUs,Tom’s Hardwarereported.Initial excitement over the company’s low training costs gave way toconcernsabout data sovereignty, security, and the cost of running DeepSeek-R1, which generates a larger number of reasoning tokens than similar models.Why it matters:DeepSeek-R1’s purported training cost fueled fears that demand for AI infrastructure would cool, but the top AI companies’ plans show that it’s not happening yet. A possible explanation lies in theJevons Paradox, a 19th-century economic theory named after the English economist William Stanley Jevons. As a valuable product becomes more affordable, demand doesn’t fall, it rises. According to this theory, even if training costs tumble, the world will demand ever greater processing power for inference.We’re thinking:DeepSeek’s low-cost technology momentarily rattled investors who had expected the next big gains would come from the U.S. rather than China. But DeepSeek’s efficiency follows a broader pattern we’ve seen for years: The AI community steadily wrings better performance from less processing power.Deepfake Developers Appropriate Celebrity LikenessesA viral deepfake video showed media superstars who appeared to support a cause — but it was made without their participation or permission.What’s new:Thevideoshows AI-generated likenesses of 20 Jewish celebrities ranging from Scarlett Johansson to Simon & Garfunkel. They appear wearing T-shirts that feature a middle finger inscribed with the Star of David above the word “KANYE.” The clip, which ends with the words “Enough is enough” followed by “Join the fight against antisemitism,” responds to rapper Kanye West, who sold T-shirts emblazoned with swastikas on Shopify before the ecommerce platform shut down his store.Who created it:Israeli developers Guy Bar and Ori Bejerano generated the video to spark a conversation about antisemitism, BartoldThe Jerusalem Post. The team didn’t reveal the AI models, editing tools, or techniques used to produce the video.Johansson reacts:Scarlett Johanssondenouncedthe clip and urged the U.S. to regulate deepfakes. In 2024, sheobjectedto one of the voices of OpenAI’s voice assistant, which she claimed resembled her own voice, leading the company to remove that voice from its service. The prior year, her attorneys ordered a company to stop using an unauthorized AI-generated version of her image in an advertisement.Likenesses up for grabs:Existing U.S. laws protect some uses of a celebrity’s likeness in the form of a photo, drawing, or human lookalike, but they don’t explicitly protect against reproduction by AI systems. This leaves celebrities and public figures with limited recourse against unauthorized deepfakes.U.S. lawmakers haveintroducedlegislation that targets deepfake pornography, but it covers only sexually explicit deepfakes.Theright of publicity, which falls under trademark law, offers some protection against the unauthorized use of a person’s identity. However, it varies by state and provides broad exceptions for news, satire, and fine art.While some states outlaw misappropriation of names or likenesses, existing laws primarily target traditional forms of image misuse, such as false endorsements or unauthorized commercial exploitation. They do not explicitly cover AI-generated deepfakes used for noncommercial, political, or satirical purposes.A 2023agreementbetween Hollywood actors and movie studios protects actors against such uses of AI-generated images of their likenesses in films. However, it doesn’t apply to deepfakes that are produced independently for distribution via social media networks.Why it matters:Non-consensual deepfake pornography is widely condemned, but AI enables many other non-consensual uses of someone’s likeness, and their limits are not yet consistently coded into law. If the creators of the video that appropriated the images of celebrities had responded to Johansson’s criticism with an AI-generated satire, would that be a legitimate exercise of free speech or another misuse of AI? Previously, an ambiguous legal framework may have been acceptable because such images, and thus lawsuits arising from them, were uncommon. Now, as synthetic likenesses of specific people become easier to generate, clear legal boundaries are needed to keep misuses in check.We’re thinking:Creating unauthorized lookalikes of existing people is not a good way to advance any cause, however worthy. Developers should work with businesses policymakers to establish standards that differentiate legitimate uses from unfair or misleading exploitation.Reasoning in Vectors, Not TextAlthough large language models can improve their performance by generating a chain of thought (CoT) — intermediate text tokens that break down the process of responding to a prompt into a series of steps — much of the CoT text is aimed at maintaining fluency (such as “a”, “of”, “we know that”) rather than reasoning (“a² + b² = c²”). Researchers addressed this inefficiency.What’s new:Shibo Hao, Sainbayar Sukhbaatar, and colleagues at Meta and University of California San Diego introducedCoconut(Chain of Continuous Thought), a method that trains large language models (LLMs) to process chains of thought as vectors rather than words.Key insight:A large language model (LLM) can be broken into an embedding layer, transformer, and classification layer. To generate the next text token from input text, the embedding layer embeds the text; given the text, the transformer outputs a hidden vector; and the classification layer maps the vector to text-token probabilities. Based on these probabilities, a decoding algorithm selects the next token to generate, which feeds back into the input text sequence to generate the next vector, and so on. When a model generates a CoT, committing to a specific word at each step limits the information available to the meanings of the words generated so far, while a vector could represent multiple possible words. Using vectors instead of text enables the CoT to encode richer information.How it works:The authors built three LLMs by fine-tuning a pre-trainedGPT-2on three datasets of prompts, CoTs, and final outputs:GSM8k(grade-school math word problems);ProntoQA(questions and answers about fictional concepts expressed in made-up words, including synthetic CoTs in natural language); and (3) ProsQA, a more challenging question-answering dataset introduced by the authors, inspired by ProntoQA but with longer reasoning steps.Fine-tuning began with supervised training. The LLM learned to generate the text in the training set, including the CoT and final answers. As usual, the last-generated text token was fed back as input to produce the next token.Fine-tuning then progressed through k stages for each example. At each stage, the authors replaced a sentence in the CoT text with a thought vector (or two) to build a sequence of k replaced sentences. The start and end of the chain of thought vectors were marked by two special tokens. During vector steps, the LLM fed its output vectors back as input without decoding them into text. The LLM learned to generate only the remaining text tokens, not the thought vectors, which encouraged it to optimize its vector-based reasoning indirectly.During inference, the LLM generated a special token to mark the start of the chain of vectors. From this point, it fed back its output vectors, bypassing text decoding for six steps. Afterward, the LLM switched back to generating text for final output.Results:The authors compared their method to a pretrained GPT-2 that was fine-tuned on the same datasets to predict the next word, including reasoning.On ProntoQA, Coconut outperformed the fine-tuned GPT-2 while producing far fewer interim vectors (Coconut) or tokens (baseline LLMs). It achieved 99.8 percent accuracy after generating nine vectors (or tokens) on average, while GPT-2 achieved 98.8 percent accuracy using 92.5 text tokens.Coconut excelled on ProsQA’s more complex questions. It achieved 97.0 percent accuracy after generating 14.2 vectors (or tokens) on average, while GPT-2 achieved 77.5 percent accuracy after generating 49.4 text tokens on average.Yes, but:On GSM8k, Coconut achieved 34.1 percent accuracy, while the baseline LLM achieved 42.9 percent. However, it generated significantly fewer vectors and tokens than the CoT generated tokens. Coconut generated 8.2 vectors on average compared to the baseline LLM’s 25 text tokens.Why it matters:A traditional CoT commits to a single word at each step and thus encodes one reasoning path in a single CoT. Vectors are less interpretable to humans than language, but the model’s output layer can still decode the thought vectors into probabilities over tokens. Further, inspecting the distribution of words stored along all continuous CoT vectors offers a way to understand multiple potential thought paths stored in one continuous CoT.We’re thinking:LLMs typically learn to reason over text, mainly because text data is widely available to train on. In contrast, neuroscience shows that the part of the human brain responsible for language largelygoes quietduring reasoning tasks, which suggests that explicit language is not a key mechanism for reasoning. Coconut takes an intriguing step to enable LLMs to explore representations that don’t encode the limitations of language.\nLoading theElevenlabs Text to SpeechAudioNative Player...\n\n\nShare\nShare\n\nShare\n",
    "image_urls": [
      "https://dl-staging-website.ghost.io/content/images/2025/02/unnamed--52-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/02/The-Batch-ads-and-exclusive-banners---2025-02-25T173657.996--1-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/02/unnamed--50-.gif",
      "https://dl-staging-website.ghost.io/content/images/2025/02/unnamed--53-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/02/unnamed--55-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/02/unnamed--51-.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2023/10/GenAI4E_sidebanner.png"
    ]
  },
  {
    "title": "issue 295",
    "date": null,
    "url": "https://www.deeplearning.ai/the-batch/issue-295/",
    "content": "The Batch\nWeekly Issues\nissue 295\nPublishedApr 2, 2025\nPublished\n\nPublished\nApr 2, 2025\nReading time12min read\nReading time\n\nReading time\n12min read\nPublishedApr 02, 2025Reading time12min readShareLoading theElevenlabs Text to SpeechAudioNative Player...Dear friends,Contrary to standard prompting advice that you should give LLMs the context they need to succeed, I find it’s sometimes faster to be lazy and dash off a quick, imprecise prompt and see what happens. The key to whether this is a good idea is whether you can quickly assess the output quality, so you can decide whether to provide more context. In this post, I’d like to share when and how I use “lazy prompting.”When debugging code, many developers copy-paste error messages — sometimes pages of them — into an LLM without further instructions. Most LLMs are smart enough to figure out that you want them to help understand and propose fixes, so you don’t need to explicitly tell them. With brief instructions like “Edit this: …” or “sample dotenv code” (to remind you how to write code to use Python's dotenv package), an LLM will often generate a good response. Further, if the response is flawed, hopefully you can spot any problems and refine the prompt, for example to steer how the LLM edits your text.At the other end of the spectrum, sometimes  I spend 30 minutes carefully writing a 2-page prompt to get an AI system to help me solve a problem (for example to write many pages of code) that otherwise would have taken me much longer.I don’t try a lazy prompt if (i) I feel confident there’s no chance the LLM will provide a good solution without additional context. For example, given a partial program spec, does even a skilled human developer have a chance of understanding what you want? If I absolutely want to use a particular piece of pdf-to-text conversion software (like my team LandingAI’sAgentic Doc Extraction!), I should say so in the prompt, since otherwise it’s very hard for the LLM to guess my preference. I also wouldn’t use a lazy prompt if (ii) a buggy implementation would take a long time to detect. For example, if the only way for me to figure out if the output is incorrect is to laboriously run the code to check its functionality, it would be better to spend the time up-front to give context that would increase the odds of the LLM generating what I want.By the way, lazy prompting is an advanced technique. On average, I see more people giving too little context to LLMs than too much. Laziness is a good technique only when you’ve learned how to provide enough context, and then deliberately step back to see how little context you can get away with and still have it work. Also, lazy prompting applies only when you can iterate quickly using an LLM’s web or app interface It doesn’t apply to prompts written in code for the purpose of repeatedly calling an API, since presumably you won’t be examining every output to clarify and iterate if the output is poor.Thank you to Rohit Prsad, who has been collaborating with me on the open-source packageaisuite, for suggesting the term lazy prompting. There is an analogy tolazy evaluationin computer science, where you call a function at the latest possible moment and only when a specific result is needed. In lazy prompting, we add details to the prompt only when they are needed.Keep building!AndrewA MESSAGE FROM DEEPLEARNING.AIIn our latest course, “Getting Structured LLM Output,” you’ll learn to generate consistent, machine-readable outputs from LLMs using structured output APIs, re-prompting libraries, and token-level constraints. You’ll build a social media analysis agent that extracts sentiment and creates structured JSON ready for downstream use.Enroll for freeNewsInteractive Voice-to-Voice With VisionResearchers updated the highly responsive Moshi voice-to-voice model to discuss visual input.What’s new:Amélie Royer, Moritz Böhle, and colleagues at Kyutai proposedMoshiVis. The weights are free todownloadunder theCC-BY 4.0license, which permits commercial and noncommercial uses. You can hear examples of itsoutputand chat with ademo.Key insight:The originalMoshi, which manages overlapping voice-to-voice conversations, comprises two transformers. The first outputs a text transcription of its speech, and the second outputs speech. Since Moshi generates text as well as speech, the authors of that work fine-tuned it to predict the next token of text. In MoshiVis, the addition of a vision encoder enabled the authors to fine-tune on not only image-text datasets but also image-speech datasets, which are not so plentiful. Fine-tuning on this wider variety of images enabled the system to understand images better than fine-tuning it solely on image-speech datasets.How it works:To Moshi, the authors added a model based on a pretrainedSigLIPvision encoder to encode images, a cross-attention adapter to fuse image information with speech tokens, and vanilla neural networks trained to act as gates that determine how much image information to fuse. Specifically, the authors added the adapter and a gate between Moshi’s existing self-attention and fully connected layers.The authors fine-tuned MoshiVis on seven datasets. For instance, they produced a vision-speech-to-speech dataset by prompting twoMistral NeMomodels to talk about an image from initial descriptions of images in the image-text datasetsPixMoandDOCCI, then using a custom text-to-speech model to convert the text into speech. Another example: They usedOCR-VQA, an image-text dataset for answering questions about images (no speech data involved).They fine-tuned MoshiVis to predict the next token of speech or text in their datasets,  training only the newly added adapter and gates while keeping SigLIP and the two Moshi transformers frozen.Results:MoshiVis is highly responsive in conversation with latency of roughly 50 milliseconds on a Mac Mini.Qualitatively, it handles transitions smoothly between talking about images and general conversation. However, it sounds more robotic than other recent voice generators.Quantitatively, the authors compared MoshiVis to the vision-language modelPaliGemmafine-tuned to answer questions about images. Overall, MoshiVis prompted with audio (and images) performed less accurately than PaliGemma prompted with text (and images). For example, on OCR-VQA, MoshiVis achieved roughly 65 percent accuracy while PaliGemma achieved roughly 71 percent accuracy.Behind the news:MoshiVis complements a small but growing roster of systems that combine vision with speech-to-speech. ChatGPT accepts and generates speech in response to camera views or a user’s phone screen.AnyGPT(open weights training and inference code) accepts or generates speech, text, images, and music. Similarly,Mini-Omni2(open weights and inference code) accepts and generates text, speech, and images. The authors didn’t compare MoshiVis to these alternatives.Why it matters:MoshiVis easily adapts a speech-to-speech model to work with a new type of media input. MoshiVis requires training only the adapters, while the earlier AnyGPT and Mini-Omni2, which can also discuss images via voice input and output, require training both adapters and the main model.We’re thinking:Text-chat models respond appropriately when a user refers to a previous topic or something new, and MoshiVis does, too, in spoken interactions. Evaluations of this capability will become increasingly important as voice-to-voice becomes more widespread.Scraping the Web? Beware the MazeBots that scrape websites for AI training data often ignore do-not-crawl requests. Now web publishers can enforce such appeals by luring scrapers to AI-generated decoy pages.What’s new:Cloudflare launchedAI Labyrinth, a bot-management tool that serves fake pages to unwanted bots, wasting their computational resources and making them easier to detect. It’s currently free to Cloudflare users.How it works:AI Labyrinth protects webpages by embedding them with hidden links to AI-generated alternatives that appear legitimate to bots but are irrelevant to the protected site.An unidentified open-source model that runs on Cloudflare’sWorkers AIplatform generates factual, science-related HTML pages on diverse topics. A pre-generation pipeline sanitizes the pages ofXSS vulnerabilitiesbefore storing them in Cloudflare’sR2storage platform.A custom process embeds links to decoy pages within a site’s HTML. Meta instructions hide these links from search engine indexers and other authorized crawlers, while other attributes and styling hide the decoy links from human visitors.When an unauthorized bot follows one of these links, it crawls through layers of irrelevant content.Cloudflare logs these interactions and uses the data to fingerprint culprit bots and improve its bot-detection models.Behind the news:The robots.txt instructions that tell web crawlers which pages they can access aren’t legally binding, and web crawlers can disregard them. However, online publishers aremovingto try to stop AI developers from training models on their content. Cloudflare, as the proxy server and content delivery network for nearly20 percentof websites, plays a potentially large role in this movement. AI crawlers account for nearly 1 percent of web requests on Cloudflare’s network, the company says.Why it matters:The latest AI models are trained on huge quantities of data gleaned from the web, which enables them to perform well enough to be widely useful. However, publishers increasingly aim to limit access to this data. AI Labyrinth gives them a new tool that raises the cost for bots that disregard instructions not to scrape web content.We’re thinking:If AI Labyrinth gains traction, no doubt some teams that build crawlers will respond with their own AI models to sniff out its decoy pages. To the extent that the interest between crawlers and publishers is misaligned and clear, enforceable rules for crawling are lacking, this cat-and-mouse competition could go on for a long time.Chatbot Use Creates Emotional BondsA pair of papers investigate how increasingly human-like chatbots affect users’ emotions.What’s new:Jason Phang at OpenAI, Cathy Mengying Fang at MIT Media Lab, and colleagues at those organizations publishedcomplementarystudiesthat examine ChatGPT’s influence on loneliness, social interactions, emotional dependence, and potentially problematic use.How it works:One study was a large-scale analysis of real-world conversations, and the other was a randomized control trial that tracked conversations of a selected cohort. Both evaluated conversations according toEmoClassifiersV1, a set of classifiers based on large language models that evaluate five top-level emotional classes (loneliness, dependence, and the like) and 20 sub-classes of emotional indicators (seeking support, use of pet names, and so on).The analysis of real-world conversations considered roughly 3 million English-language voice conversations by 6,000 heavy users of ChatGPT’s Advanced Voice Mode over three months and surveyed 4,076 of them about their perceptions. It analyzed conversations for emotional cues and tracked users’ percentages of emotional messages over time (decreasing, flat, or increasing). The team validated classification accuracy by comparing the classifier’s outputs with survey responses.The randomized controlled trial asked nearly 1,000 participants over 28 days to engage in particular conversation types (open-ended, personal, or non-personal) and modalities (text, interactions with ChatGPT’s neutral voice, or interactions with an engaging voice), controlling for variables like duration and age. Each participant spent at least five minutes per day interacting with ChatGPT, guided by prompts (such as “Help me reflect on a treasured memory”) and surveys (baseline, daily, weekly, and final). The study classified over 300,000 messages to identify qualities like loneliness and dependence and sorted them according to conversation type and modality.Results:Both studies found that using ChatGPT was associated with reduced loneliness and increased emotional chat. However, it was also associated with decreased interpersonal social interaction and greater dependence on the chatbot, especially among users who spent more time chatting.Yes, but:The authors of the randomized controlled trial acknowledged significant limitations. For instance, the study lacked a non-ChatGPT control group to differentiate AI-specific effects from influences such as seasonal emotional shifts, and the trial’s time frame and assignments may not mirror real-world behavior.Why it matters:As AI chatbot behavior becomes more human-like, people may lean on large language models to satisfy emotional needs such as easinglonelinessorgrief. Yet we know little about their effects. These studies offer a starting point for AI developers who want to both foster emotional support and protect against over-reliance, and for social scientists who want to better understand the impact of chatbots.We’re thinking:Social media turned out to causeemotional harmto some people in ways that were not obvious when the technology was new. As chatbots evolve, research like this can help us steer them toward protecting and enhancing mental health.Human Action in 3DAI systems designed to generate animated 3D scenes that include active human characters have been limited by a shortage of training data, such as matched 3D scenes and human motion-capture examples. Generated video clips can get the job done without motion capture.What’s new:A team led by Hongjie Li, Hong-Xing Yu, and Jiaman Li at Stanford University developedZero-Shot 4D Human-Scene Interaction(ZeroHSI), a method that animates a 3D human figure interacting with a particular 3D object in a selected 3D scene. You can see its outputhere.Key insight: Earlier approaches attempted to build a generalized approach: given a 3D scene, a text prompt, and motion-capture data, a diffusion model learned to alter the positions and rotations of human joints and objects over time. But if the system is designed to learn a 3D animation for a specific example motion, videos can stand in for motion capture. Current video generation models can take an image of a scene and generate a clip of realistic human motion and interactions with a wide variety of objects within it. From there, we can minimize the difference between the video frames and images of actions within the scene.How it works:ZeroHSI takes a pre-built 3D scene that includes a3D human meshand 3D object. It uses a rendered image of the scene to generate a video. Then it uses the video to help compute the motions of a human figure and object within the scene.The authors fed ZeroHSI a 3D scene complete with 3D human mesh and 3D object. ZeroHSI rendered an image of the scene, viewed from a default camera pose, usingGaussian splatting.ZeroHSI fed the rendered image, along with a prompt that described a human interacting with an object in the scene (“the person is playing guitar while sitting on the sofa”), toKling, an image-to-video generator. Kling produced a video clip.For each generated video frame, ZeroHSI rendered a new image of the 3D scene and minimized a loss function with four terms. It used the loss function to calculate how to change the poses of the 3D human, 3D object, and camera in the 3D scene to match their poses in the video frame. For example, one loss term minimized pixel-level differences between the image and video frame. Another minimized the difference between the object’s center in the image and in a segmentation mask of the video frame produced bySAM 2.The system sometimes produced errors. For instance, one of the human figure’s hands might fail to touch the object, or the object penetrated the human figure’s body. To remedy this, for each video frame, the authors refined the poses in a separate phase that involved three loss terms. For instance, one term minimized the distance between surfaces of a hand and the object to prevent penetration or distance between them.Results:The authors evaluated ZeroHSI using a proprietary dataset of 12 3D scenes that included a human figure and an object and between one and three text prompts that described interactions between the human and object and/or scene. In 100 evaluations, ZeroHSI outperformedLINGO, a diffusion model trained on matched 3D scene, 3D object, and human motion-capture data that had achieved the previous state of the art.ZeroHSI achieved 24.01 average CLIP Score, which measures how well text descriptions match images (higher is better), while LINGO achieved a 22.99 average CLIP Score. ZeroHSI achieved 0.033 average object penetration depth, a measure of plausibility in physical interactions (lower is better), while LINGO achieved 0.242 average object penetration depth.400 participants judged whether they preferred ZeroHSI or LINGO with respect to realism and how well their output aligned with the prompt. 86.9 percent preferred ZeroHSI for realism, and 89.1 percent preferred ZeroHSI for how well its output matched the prompt.Why it matters:Learning from motion-capture data is problematic in a couple of ways: (i) it’s expensive to produce, (ii) so little of it is available, which limits how much a learning algorithm can generalize from it. Video data, on the other hand, is available in endless variety, enabling video generation models to generalize across a wide variety of scenes, objects, and motions. ZeroHSI takes advantage of generated video to guide a 3D animation cheaply and effectively.We’re thinking:There’s a lot of progress to be made in AI simply by finding clever ways to use synthetic data.Share\nPublishedApr 02, 2025\nPublished\n\nPublished\nApr 02, 2025\nApr 02, 2025\nReading time12min read\nReading time\n\nReading time\n12min read\nShare\nShare\n\nShare\n\nLoading theElevenlabs Text to SpeechAudioNative Player...Dear friends,Contrary to standard prompting advice that you should give LLMs the context they need to succeed, I find it’s sometimes faster to be lazy and dash off a quick, imprecise prompt and see what happens. The key to whether this is a good idea is whether you can quickly assess the output quality, so you can decide whether to provide more context. In this post, I’d like to share when and how I use “lazy prompting.”When debugging code, many developers copy-paste error messages — sometimes pages of them — into an LLM without further instructions. Most LLMs are smart enough to figure out that you want them to help understand and propose fixes, so you don’t need to explicitly tell them. With brief instructions like “Edit this: …” or “sample dotenv code” (to remind you how to write code to use Python's dotenv package), an LLM will often generate a good response. Further, if the response is flawed, hopefully you can spot any problems and refine the prompt, for example to steer how the LLM edits your text.At the other end of the spectrum, sometimes  I spend 30 minutes carefully writing a 2-page prompt to get an AI system to help me solve a problem (for example to write many pages of code) that otherwise would have taken me much longer.I don’t try a lazy prompt if (i) I feel confident there’s no chance the LLM will provide a good solution without additional context. For example, given a partial program spec, does even a skilled human developer have a chance of understanding what you want? If I absolutely want to use a particular piece of pdf-to-text conversion software (like my team LandingAI’sAgentic Doc Extraction!), I should say so in the prompt, since otherwise it’s very hard for the LLM to guess my preference. I also wouldn’t use a lazy prompt if (ii) a buggy implementation would take a long time to detect. For example, if the only way for me to figure out if the output is incorrect is to laboriously run the code to check its functionality, it would be better to spend the time up-front to give context that would increase the odds of the LLM generating what I want.By the way, lazy prompting is an advanced technique. On average, I see more people giving too little context to LLMs than too much. Laziness is a good technique only when you’ve learned how to provide enough context, and then deliberately step back to see how little context you can get away with and still have it work. Also, lazy prompting applies only when you can iterate quickly using an LLM’s web or app interface It doesn’t apply to prompts written in code for the purpose of repeatedly calling an API, since presumably you won’t be examining every output to clarify and iterate if the output is poor.Thank you to Rohit Prsad, who has been collaborating with me on the open-source packageaisuite, for suggesting the term lazy prompting. There is an analogy tolazy evaluationin computer science, where you call a function at the latest possible moment and only when a specific result is needed. In lazy prompting, we add details to the prompt only when they are needed.Keep building!AndrewA MESSAGE FROM DEEPLEARNING.AIIn our latest course, “Getting Structured LLM Output,” you’ll learn to generate consistent, machine-readable outputs from LLMs using structured output APIs, re-prompting libraries, and token-level constraints. You’ll build a social media analysis agent that extracts sentiment and creates structured JSON ready for downstream use.Enroll for freeNewsInteractive Voice-to-Voice With VisionResearchers updated the highly responsive Moshi voice-to-voice model to discuss visual input.What’s new:Amélie Royer, Moritz Böhle, and colleagues at Kyutai proposedMoshiVis. The weights are free todownloadunder theCC-BY 4.0license, which permits commercial and noncommercial uses. You can hear examples of itsoutputand chat with ademo.Key insight:The originalMoshi, which manages overlapping voice-to-voice conversations, comprises two transformers. The first outputs a text transcription of its speech, and the second outputs speech. Since Moshi generates text as well as speech, the authors of that work fine-tuned it to predict the next token of text. In MoshiVis, the addition of a vision encoder enabled the authors to fine-tune on not only image-text datasets but also image-speech datasets, which are not so plentiful. Fine-tuning on this wider variety of images enabled the system to understand images better than fine-tuning it solely on image-speech datasets.How it works:To Moshi, the authors added a model based on a pretrainedSigLIPvision encoder to encode images, a cross-attention adapter to fuse image information with speech tokens, and vanilla neural networks trained to act as gates that determine how much image information to fuse. Specifically, the authors added the adapter and a gate between Moshi’s existing self-attention and fully connected layers.The authors fine-tuned MoshiVis on seven datasets. For instance, they produced a vision-speech-to-speech dataset by prompting twoMistral NeMomodels to talk about an image from initial descriptions of images in the image-text datasetsPixMoandDOCCI, then using a custom text-to-speech model to convert the text into speech. Another example: They usedOCR-VQA, an image-text dataset for answering questions about images (no speech data involved).They fine-tuned MoshiVis to predict the next token of speech or text in their datasets,  training only the newly added adapter and gates while keeping SigLIP and the two Moshi transformers frozen.Results:MoshiVis is highly responsive in conversation with latency of roughly 50 milliseconds on a Mac Mini.Qualitatively, it handles transitions smoothly between talking about images and general conversation. However, it sounds more robotic than other recent voice generators.Quantitatively, the authors compared MoshiVis to the vision-language modelPaliGemmafine-tuned to answer questions about images. Overall, MoshiVis prompted with audio (and images) performed less accurately than PaliGemma prompted with text (and images). For example, on OCR-VQA, MoshiVis achieved roughly 65 percent accuracy while PaliGemma achieved roughly 71 percent accuracy.Behind the news:MoshiVis complements a small but growing roster of systems that combine vision with speech-to-speech. ChatGPT accepts and generates speech in response to camera views or a user’s phone screen.AnyGPT(open weights training and inference code) accepts or generates speech, text, images, and music. Similarly,Mini-Omni2(open weights and inference code) accepts and generates text, speech, and images. The authors didn’t compare MoshiVis to these alternatives.Why it matters:MoshiVis easily adapts a speech-to-speech model to work with a new type of media input. MoshiVis requires training only the adapters, while the earlier AnyGPT and Mini-Omni2, which can also discuss images via voice input and output, require training both adapters and the main model.We’re thinking:Text-chat models respond appropriately when a user refers to a previous topic or something new, and MoshiVis does, too, in spoken interactions. Evaluations of this capability will become increasingly important as voice-to-voice becomes more widespread.Scraping the Web? Beware the MazeBots that scrape websites for AI training data often ignore do-not-crawl requests. Now web publishers can enforce such appeals by luring scrapers to AI-generated decoy pages.What’s new:Cloudflare launchedAI Labyrinth, a bot-management tool that serves fake pages to unwanted bots, wasting their computational resources and making them easier to detect. It’s currently free to Cloudflare users.How it works:AI Labyrinth protects webpages by embedding them with hidden links to AI-generated alternatives that appear legitimate to bots but are irrelevant to the protected site.An unidentified open-source model that runs on Cloudflare’sWorkers AIplatform generates factual, science-related HTML pages on diverse topics. A pre-generation pipeline sanitizes the pages ofXSS vulnerabilitiesbefore storing them in Cloudflare’sR2storage platform.A custom process embeds links to decoy pages within a site’s HTML. Meta instructions hide these links from search engine indexers and other authorized crawlers, while other attributes and styling hide the decoy links from human visitors.When an unauthorized bot follows one of these links, it crawls through layers of irrelevant content.Cloudflare logs these interactions and uses the data to fingerprint culprit bots and improve its bot-detection models.Behind the news:The robots.txt instructions that tell web crawlers which pages they can access aren’t legally binding, and web crawlers can disregard them. However, online publishers aremovingto try to stop AI developers from training models on their content. Cloudflare, as the proxy server and content delivery network for nearly20 percentof websites, plays a potentially large role in this movement. AI crawlers account for nearly 1 percent of web requests on Cloudflare’s network, the company says.Why it matters:The latest AI models are trained on huge quantities of data gleaned from the web, which enables them to perform well enough to be widely useful. However, publishers increasingly aim to limit access to this data. AI Labyrinth gives them a new tool that raises the cost for bots that disregard instructions not to scrape web content.We’re thinking:If AI Labyrinth gains traction, no doubt some teams that build crawlers will respond with their own AI models to sniff out its decoy pages. To the extent that the interest between crawlers and publishers is misaligned and clear, enforceable rules for crawling are lacking, this cat-and-mouse competition could go on for a long time.Chatbot Use Creates Emotional BondsA pair of papers investigate how increasingly human-like chatbots affect users’ emotions.What’s new:Jason Phang at OpenAI, Cathy Mengying Fang at MIT Media Lab, and colleagues at those organizations publishedcomplementarystudiesthat examine ChatGPT’s influence on loneliness, social interactions, emotional dependence, and potentially problematic use.How it works:One study was a large-scale analysis of real-world conversations, and the other was a randomized control trial that tracked conversations of a selected cohort. Both evaluated conversations according toEmoClassifiersV1, a set of classifiers based on large language models that evaluate five top-level emotional classes (loneliness, dependence, and the like) and 20 sub-classes of emotional indicators (seeking support, use of pet names, and so on).The analysis of real-world conversations considered roughly 3 million English-language voice conversations by 6,000 heavy users of ChatGPT’s Advanced Voice Mode over three months and surveyed 4,076 of them about their perceptions. It analyzed conversations for emotional cues and tracked users’ percentages of emotional messages over time (decreasing, flat, or increasing). The team validated classification accuracy by comparing the classifier’s outputs with survey responses.The randomized controlled trial asked nearly 1,000 participants over 28 days to engage in particular conversation types (open-ended, personal, or non-personal) and modalities (text, interactions with ChatGPT’s neutral voice, or interactions with an engaging voice), controlling for variables like duration and age. Each participant spent at least five minutes per day interacting with ChatGPT, guided by prompts (such as “Help me reflect on a treasured memory”) and surveys (baseline, daily, weekly, and final). The study classified over 300,000 messages to identify qualities like loneliness and dependence and sorted them according to conversation type and modality.Results:Both studies found that using ChatGPT was associated with reduced loneliness and increased emotional chat. However, it was also associated with decreased interpersonal social interaction and greater dependence on the chatbot, especially among users who spent more time chatting.Yes, but:The authors of the randomized controlled trial acknowledged significant limitations. For instance, the study lacked a non-ChatGPT control group to differentiate AI-specific effects from influences such as seasonal emotional shifts, and the trial’s time frame and assignments may not mirror real-world behavior.Why it matters:As AI chatbot behavior becomes more human-like, people may lean on large language models to satisfy emotional needs such as easinglonelinessorgrief. Yet we know little about their effects. These studies offer a starting point for AI developers who want to both foster emotional support and protect against over-reliance, and for social scientists who want to better understand the impact of chatbots.We’re thinking:Social media turned out to causeemotional harmto some people in ways that were not obvious when the technology was new. As chatbots evolve, research like this can help us steer them toward protecting and enhancing mental health.Human Action in 3DAI systems designed to generate animated 3D scenes that include active human characters have been limited by a shortage of training data, such as matched 3D scenes and human motion-capture examples. Generated video clips can get the job done without motion capture.What’s new:A team led by Hongjie Li, Hong-Xing Yu, and Jiaman Li at Stanford University developedZero-Shot 4D Human-Scene Interaction(ZeroHSI), a method that animates a 3D human figure interacting with a particular 3D object in a selected 3D scene. You can see its outputhere.Key insight: Earlier approaches attempted to build a generalized approach: given a 3D scene, a text prompt, and motion-capture data, a diffusion model learned to alter the positions and rotations of human joints and objects over time. But if the system is designed to learn a 3D animation for a specific example motion, videos can stand in for motion capture. Current video generation models can take an image of a scene and generate a clip of realistic human motion and interactions with a wide variety of objects within it. From there, we can minimize the difference between the video frames and images of actions within the scene.How it works:ZeroHSI takes a pre-built 3D scene that includes a3D human meshand 3D object. It uses a rendered image of the scene to generate a video. Then it uses the video to help compute the motions of a human figure and object within the scene.The authors fed ZeroHSI a 3D scene complete with 3D human mesh and 3D object. ZeroHSI rendered an image of the scene, viewed from a default camera pose, usingGaussian splatting.ZeroHSI fed the rendered image, along with a prompt that described a human interacting with an object in the scene (“the person is playing guitar while sitting on the sofa”), toKling, an image-to-video generator. Kling produced a video clip.For each generated video frame, ZeroHSI rendered a new image of the 3D scene and minimized a loss function with four terms. It used the loss function to calculate how to change the poses of the 3D human, 3D object, and camera in the 3D scene to match their poses in the video frame. For example, one loss term minimized pixel-level differences between the image and video frame. Another minimized the difference between the object’s center in the image and in a segmentation mask of the video frame produced bySAM 2.The system sometimes produced errors. For instance, one of the human figure’s hands might fail to touch the object, or the object penetrated the human figure’s body. To remedy this, for each video frame, the authors refined the poses in a separate phase that involved three loss terms. For instance, one term minimized the distance between surfaces of a hand and the object to prevent penetration or distance between them.Results:The authors evaluated ZeroHSI using a proprietary dataset of 12 3D scenes that included a human figure and an object and between one and three text prompts that described interactions between the human and object and/or scene. In 100 evaluations, ZeroHSI outperformedLINGO, a diffusion model trained on matched 3D scene, 3D object, and human motion-capture data that had achieved the previous state of the art.ZeroHSI achieved 24.01 average CLIP Score, which measures how well text descriptions match images (higher is better), while LINGO achieved a 22.99 average CLIP Score. ZeroHSI achieved 0.033 average object penetration depth, a measure of plausibility in physical interactions (lower is better), while LINGO achieved 0.242 average object penetration depth.400 participants judged whether they preferred ZeroHSI or LINGO with respect to realism and how well their output aligned with the prompt. 86.9 percent preferred ZeroHSI for realism, and 89.1 percent preferred ZeroHSI for how well its output matched the prompt.Why it matters:Learning from motion-capture data is problematic in a couple of ways: (i) it’s expensive to produce, (ii) so little of it is available, which limits how much a learning algorithm can generalize from it. Video data, on the other hand, is available in endless variety, enabling video generation models to generalize across a wide variety of scenes, objects, and motions. ZeroHSI takes advantage of generated video to guide a 3D animation cheaply and effectively.We’re thinking:There’s a lot of progress to be made in AI simply by finding clever ways to use synthetic data.\nLoading theElevenlabs Text to SpeechAudioNative Player...\n\n\nShare\nShare\n\nShare\n",
    "image_urls": [
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--57-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/04/The-Batch-ads-and-exclusive-banners---2025-03-28T144816.793.png",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--70-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--71-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--72-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--55-.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2024/08/Vertical-side-banner-ads-5.png"
    ]
  },
  {
    "title": "issue 288",
    "date": null,
    "url": "https://www.deeplearning.ai/the-batch/issue-288/",
    "content": "The Batch\nWeekly Issues\nissue 288\nPublishedFeb 12, 2025\nPublished\n\nPublished\nFeb 12, 2025\nReading time13min read\nReading time\n\nReading time\n13min read\nPublishedFeb 12, 2025Reading time13min readShareLoading theElevenlabs Text to SpeechAudioNative Player...Dear friends,At the Artificial Intelligence Action Summit in Paris this week, U.S. Vice President J.D. Vancesaid, “I’m not here to talk about AI safety. ... I’m here to talk about AI opportunity.” I’m thrilled to see the U.S. government focus on opportunities in AI. Further, while it is important to use AI responsibly and try to stamp out harmful applications, I feel “AI safety” is not the right terminology for addressing this important problem. Language shapes thought, so using the right words is important. I’d rather talk about “responsible AI” than “AI safety.” Let me explain.First, there are clearly harmful applications of AI, such as non-consensual deepfake porn (which creates sexually explicit images of real people without their consent), the use of AI in misinformation, potentially unsafe medical diagnoses, addictive applications, and so on. We definitely want to stamp these out! There are many ways to apply AI in harmful or irresponsible ways, and we should discourage and prevent such uses.However, the concept of “AI safety” tries to make AI — as a technology — safe, rather than making safe applications of it. Consider the similar, obviously flawed notion of “laptop safety.” There are great ways to use a laptop and many irresponsible ways, but I don’t consider laptops to be intrinsically either safe or unsafe. It is the application, or usage, that determines if a laptop is safe. Similarly, AI, a general-purpose technology with numerous applications, is neither safe nor unsafe. How someone chooses to use it determines whether it is harmful or beneficial.Now, safety isn’t always a function only of how something is used. An unsafe airplane is one that, even in the hands of an attentive and skilled pilot, has a large chance of mishap. So we definitely should strive to build safe airplanes (and make sure they are operated responsibly)! The risk factors are associated with the construction of the aircraft rather than merely its application. Similarly, we want safe automobiles, blenders, dialysis machines, food, buildings, power plants, and much more.“AI safety” presupposes that AI, the underlying technology, can be unsafe. I find it more useful to think about how applications of AI can be unsafe.Further, the term “responsible AI” emphasizes that it is our responsibility to avoid building applications that are unsafe or harmful and to discourage people from using even beneficial products in harmful ways.If we shift the terminology for AI risks from “AI safety” to “responsible AI,” we can have more thoughtful conversations about what to do and what not to do.I believe the 2023 Bletchley AI Safety Summit slowed down European AI development — without making anyone safer — by wasting time considering science-fiction AI fears rather than focusing on opportunities. Last month, at Davos, business and policy leaders also had strong concerns about whether Europe can dig itself out of the current regulatory morass and focus on building with AI. I am hopeful that the Paris meeting, unlike the one at Bletchley, will result in acceleration rather than deceleration.In a world where AI is becoming pervasive, if we can shift the conversation away from “AI safety” toward responsible [use of] AI, we will speed up AI’s benefits and do a better job of addressing actual problems. That will actually make people safer.Keep building!AndrewA MESSAGE FROM DEEPLEARNING.AIUnderstand and implement the attention mechanism, a key element in transformer-based LLMs, using PyTorch. In this course, StatQuest’s Josh Starmer explains the core ideas behind attention mechanisms, the algorithm itself, and a step-by-step breakdown of how to implement them in PyTorch.Enroll nowNewsAgents Go DeepOpenAI introduced a state-of-the-art agent that produces research reports by scouring the web and reasoning over what it finds.What’s new:OpenAI’sdeep researchresponds to users’ requests by generating a detailed report based on hundreds of online sources. The system generates text output, with images and other media expected soon. Currently the agent is available only to subscribers to ChatGPT Pro, but the company plans to roll it out to users of ChatGPT Plus, Team, and Enterprise.How it works:Deep research is an agent that uses OpenAI’s o3 model, which is not yet publicly available. The model was trained via reinforcement learning to use a browser and Python tools, similar to the way o1 learned to reason from reinforcement learning. OpenAI has not yet released detailed information about how it built the system.The system responds best to detailed prompts that specify the desired output (such as the desired information, comparisons, and format), the team said in itsannouncement video(which features Mark Chen, Josh Tobin, Neel Ajjarapu, and Isa Fulford, co-instructor of our short courses “ChatGPT Prompt Engineering for Developers” and “Building Systems with the ChatGPT API”).Before answering, Deep research asks clarifying questions about the task.In the process of answering, the system presents a sidebar that summarizes the model’s chain of thought, terms it searched, websites it visited, and so on.The system can take as long as 30 minutes to provide output.Result: On abenchmarkof 3,000 multiple-choice and short-answer questions that cover subjects from ecology to rocket science, OpenAI deep research achieved 26.6 percent accuracy. In comparison, DeepSeek-R1 (without web browsing or other tool use) achieved 9.4 percent accuracy and o1 (also without tool use) achieved 9.1 percent accuracy. OnGAIA, questions that are designed to be difficult for large language models without access to additional tools, OpenAI deep research achieved 67.36 percent accuracy, exceeding theprevious state of the artof 63.64 percent accuracy.Behind the news:OpenAI’s deep research follows a similar offering of the same name by Google in December. A number of open source teams have built research agents that work in similar ways. Notable releases include aHugging Faceproject that attempted to replicate OpenAI’s work (not including training) in 24 hours (which achieved 55.15 percent accuracy on GAIA) andgpt-researcher, which implemented agentic web search in 2023, long before Google and OpenAI launched their agentic research systems.Why it matters:Reasoning models like o1 or o3 made a splash not just because they delivered superior results but also because of the impressive reasoning steps the model took to produce the results. Combining that ability with web search and tool use enables large language models to formulate better answers to difficult questions, including those whose answers aren’t in the training data or whose answers change over time.We’re thinking:Taking as much as 30 minutes of processing to render a response, OpenAI’s deep research clearly illustrates why we needmore compute for inference.Google Joins AI Peers In Military WorkGoogle revised its AI principles, reversing previous commitments to avoid work on weapons, surveillance, and other military applications beyond non-lethal uses like communications, logistics, and medicine.What’s new:Along with releasing its latestResponsible AI Progress Reportand an updated AIsafety framework, Google removed key restrictions from itsAI principles. The new version omits a section in the previous document titled “Applications we will not pursue.” The deleted textpledgedto avoid “technologies that cause or are likely to cause overall harm” and, where the technology risks doing harm, to “proceed only where we believe that the benefits substantially outweigh the risks” with “appropriate safety constraints.”How it works:Google’s AI principles no longer prohibit specific applications but promote developing the technology to improve scientific inquiry, national security, and the economy.The revised principles state that AI development should be led by democracies. The company argues that such leadership is needed given growing global competition in AI from countries that are not widely considered liberal democracies.The new principles stress “responsible development and deployment” to manage AI’s complexities and risks. They state that AI must be developed with safeguards at every stage, from design and testing to deployment and iteration, and those safeguards must adapt as technology and applications evolve.The revised principles also emphasize collaborative progress, stating that Google aims to learn from others and build AI that’s broadly useful across industries and society.Google emphasizes the need for “bold innovation,” stating that AI should be developed to assist, empower, and inspire people; drive economic progress; enable scientific breakthroughs; and help address global challenges. Examples includeAlphaFold 3, which figures out how biological molecules interact, a key factor in designing chemical processes that affect them.The revised principles are buttressed by the 2025 Responsible AI Progress Report. This documentoutlinesthe company’s efforts to evaluate risks through measures that align with the NIST AI Risk Management Framework includingred teaming, automated assessments, and input from independent experts.Behind the news:Google’s new stance reverses a commitment it made in 2018 after employeesprotestedits involvement inProject Maven, a Pentagon AI program for drone surveillance, from which Google ultimately withdrew. At the time, Google pledged not to develop AI applications for weapons or surveillance, which set it apart from Amazon and Microsoft. Since then, the company has expanded its work in defense,building ona $1.3 billion contract with Israel. In 2024,Anthropic, Meta, andOpenAIremoved their restrictions on military and defense applications, and Anthropic and OpenAIstrengthenedtheir ties with defense contractors such as Anduril and Palantir.Why it matters:Google’s shift in policy comes as AI is playing an increasing role in conflicts in Israel,Ukraine, and elsewhere, and while global geopolitical tensions are on the rise. While Google’s previous position kept it out of military AI development, defense contractors like Anduril, Northrop Grumman, and Palantir — not to mention AI-giant peers — stepped in. The new principles recognize the need for democratic countries to take the lead in developing technology and standards for its use as well as the massive business opportunity in military AI as governments worldwide seek new defense capabilities. Still, no widely acceptedglobal frameworkgoverns uses of AI in combat.We’re thinking:Knowing how and when to employ AI in warfare is one of the most difficult ethical questions of our time. Democratic nations have a right to defend themselves, and those of us who live in democracies have a responsibility to support fellow citizens who would put themselves in harm’s way to protect us. AI is transforming military strategy, and refusing to engage with it doesn’t make the risks go away.Alibaba’s Answer to DeepSeekWhile Hangzhou’s DeepSeek flexed its muscles, Chinese tech giant Alibaba vied for the spotlight with new open vision-language models.What’s new:Alibaba announcedQwen2.5-VL, a family of vision-language models (images and text in, text out) in sizes of 3 billion, 7 billion, and 72 billion parameters. The weights for all three models are available for download onHugging Face, each under a different license: Qwen2.5-VL-3B isfree for non-commercial uses, Qwen2.5-VL-7B isfree for commercial and noncommercial usesunder the Apache 2.0 license, and Qwen2.5-VL-72B isfree to developers that have less than 100 million monthly active users. You can try them out for free for a limited time inAlibaba Model Studio, and Qwen2.5-VL-72B is available via the model selector inQwen Chat.How it works:Qwen2.5-VL models accept up to 129,024 tokens of input according to thedeveloper reference(other sources provide conflicting numbers) and generate up to 8,192 tokens of output. Alibaba has not released details about how it trained them.Qwen2.5-VL comprises a vision encoder and large language model. It can parse videos, images, text, and is capable of computer use (desktop and mobile).The vision encoder accepts images of different sizes and represents them with different numbers of tokens depending on the size. For instance, one image might be 8 tokens and another 1125 tokens. This enabled the model to learn about the scale of images and to estimate the coordinates of objects in an image without rescaling.To reduce computation incurred by the vision encoder, the team replaced attention (which considers the entire input context) with windowed attention (which limits the input context to a window around a given token) and used full attention only in four layers. The resulting efficiency improves training and inference speeds.Results: Alibaba reports Qwen2.5-VL-72B’s performance on measures that span image and text problems, parsing documents, understanding videos, and interacting with computer programs. Across 21 benchmarks, it beat Microsoft Gemini 2.0 Flash, OpenAI GPT-4o, Anthropic Claude 3.5 Sonnet, and open competitors on 13 of them (where comparisons are  relevant and available).For example, on answering math questions about images inMathVista, Qwen2.5-VL-72B achieved 74.8 percent, while the closest competing model (Gemini 2.0 Flash) achieved 73.1 percent.InVideo-MME, which evaluates a model’s ability to answer questions about videos, Qwen 2.5 VL achieved 73.3 percent. GPT-4o achieved 71.9 percent andInternVL2.5, the next-best open competitor, achieved 72.1 percent.Used in an agentic workflow, Qwen2.5-VL-72B outperformed Claude 3.5 Sonnet when controlling Android devices and navigating desktop user interfaces. However, it finished second to other open vision-language models in several tests.More models:Alibaba also introduced competition for DeepSeek and a family of small models.Qwen2.5-Maxis a mixture-of-experts model that outperforms GPT-4o and DeepSeek-V3 on graduate-level science questions inGPQA-Diamondand regularly updated benchmarks likeArena-Hard,LiveBench, andLiveCodeBench. However, Qwen2.5-Max performed worse than o1 and DeepSeek-R1.Qwen2.5-1Mis a family of smaller language models (7 billion and 14 billion parameters) that accept up to 1 million tokens of input context.Why it matters:Vision-language models are getting more powerful and versatile. Not long ago, it was an impressive feat simply to answer questions about a chart or diagram that mixed graphics with text. Now such models are paired with an agent to control computers and smartphones. Broadly speaking, the Qwen2.5-VL models outperform open and closed competitors and they’re open to varying degrees (though the data is not available), giving developers a range of highly capable choices.We’re thinking:We’re happy Alibaba released a vision-language model that is broadly permissive with respect to commercial use (although we’d prefer that all sizes were available under a standard open weights license). We hope to see technical reports that illuminate Alibaba’s training and fine-tuning recipes.Tree Search for Web AgentsBrowsing the web to achieve a specific goal can be challenging for agents based on large language models and even for vision-language models that can process onscreen images of a browser. While some approaches address this difficulty in training the underlying model, the agent architecture can also make a difference.What’s new:Jing Yu Koh and colleagues at Carnegie Mellon University introducedtree search for language model agents, a method that allows agents to treat web interactions like tree searches. In this way, agents can explore possible chains of actions and avoid repeating mistakes.Key insight:Some web tasks, for instance finding a price of a particular item, require a chain of intermediate actions: navigating to the right page, scrolling to find the item, matching an image of the item to the image on the page, and so on. If an agent clicks the wrong link during this process, it might lose its way. The ability to evaluate possible actions and remember previous states of web pages can help an agent correct its mistakes and choose a chain of actions that achieves its goal.How it works:An agent based on GPT-4o attempted 200tasksusing website mockups that mimicked an online retail store, Reddit-like forum, and directory of classified ads. The tasks included ordering an item to be delivered to a given address, finding specific images on the forum, and posting an ad. The authors annotated each web page using the method calledSet of Mark, which identifies every visual element capable of interaction with a bounding box and a numerical ID.The agent started with a web page and an instruction such as, “Tell me the number of reviews our store received that mention the term ‘not useful.’” It passed an image of the page to the LLM, which predicted five actions that could make progress toward completing the task such as scrolling up or down, hovering over an element, clicking, typing in a text field, or opening a new URL.The agent executed the five actions. After each one, the LLM assessed the current state of the page using the previous states as context. The assessment assigned a value between 0 and 1 (meaning the task was complete). The agent kept a list of page states and their values.The agent selected the web page state with the highest value after executing the five actions, and repeated the process, making a new set of five predictions based on the highest-value state.This process is a search: The agent executed a chain of actions until the value of the new states dropped below the values of other states. If all new states had lower values, the agent backtracked to a previous state with a higher value and asked the LLM for five more actions. The search stopped when the agent had completed the task or explored 20 possible states.Results:The authors compared two agents, one that followed their search method and another that started at the same page and received the same instruction but took one action per state and never backtracked. The agents attempted 100 shopping tasks, 50 forum tasks, and 50 classified-ads tasks. The one equipped to search successfully completed 26.4 percent of the tasks, while the other agent completed 18.9 percent of the tasks.Why it matters:Search joins reflection, planning, tool use, and multi-agent collaboration as an emergingagentic design pattern. Following many branching paths of actions enables an agent to determine the most effective set of actions to accomplish a task.We’re thinking:Agentic design patterns are progressing quickly! In combination withcomputer use, this sort of search method may enable agents to execute a wide variety of desktop tasks.Share\nPublishedFeb 12, 2025\nPublished\n\nPublished\nFeb 12, 2025\nFeb 12, 2025\nReading time13min read\nReading time\n\nReading time\n13min read\nShare\nShare\n\nShare\n\nLoading theElevenlabs Text to SpeechAudioNative Player...Dear friends,At the Artificial Intelligence Action Summit in Paris this week, U.S. Vice President J.D. Vancesaid, “I’m not here to talk about AI safety. ... I’m here to talk about AI opportunity.” I’m thrilled to see the U.S. government focus on opportunities in AI. Further, while it is important to use AI responsibly and try to stamp out harmful applications, I feel “AI safety” is not the right terminology for addressing this important problem. Language shapes thought, so using the right words is important. I’d rather talk about “responsible AI” than “AI safety.” Let me explain.First, there are clearly harmful applications of AI, such as non-consensual deepfake porn (which creates sexually explicit images of real people without their consent), the use of AI in misinformation, potentially unsafe medical diagnoses, addictive applications, and so on. We definitely want to stamp these out! There are many ways to apply AI in harmful or irresponsible ways, and we should discourage and prevent such uses.However, the concept of “AI safety” tries to make AI — as a technology — safe, rather than making safe applications of it. Consider the similar, obviously flawed notion of “laptop safety.” There are great ways to use a laptop and many irresponsible ways, but I don’t consider laptops to be intrinsically either safe or unsafe. It is the application, or usage, that determines if a laptop is safe. Similarly, AI, a general-purpose technology with numerous applications, is neither safe nor unsafe. How someone chooses to use it determines whether it is harmful or beneficial.Now, safety isn’t always a function only of how something is used. An unsafe airplane is one that, even in the hands of an attentive and skilled pilot, has a large chance of mishap. So we definitely should strive to build safe airplanes (and make sure they are operated responsibly)! The risk factors are associated with the construction of the aircraft rather than merely its application. Similarly, we want safe automobiles, blenders, dialysis machines, food, buildings, power plants, and much more.“AI safety” presupposes that AI, the underlying technology, can be unsafe. I find it more useful to think about how applications of AI can be unsafe.Further, the term “responsible AI” emphasizes that it is our responsibility to avoid building applications that are unsafe or harmful and to discourage people from using even beneficial products in harmful ways.If we shift the terminology for AI risks from “AI safety” to “responsible AI,” we can have more thoughtful conversations about what to do and what not to do.I believe the 2023 Bletchley AI Safety Summit slowed down European AI development — without making anyone safer — by wasting time considering science-fiction AI fears rather than focusing on opportunities. Last month, at Davos, business and policy leaders also had strong concerns about whether Europe can dig itself out of the current regulatory morass and focus on building with AI. I am hopeful that the Paris meeting, unlike the one at Bletchley, will result in acceleration rather than deceleration.In a world where AI is becoming pervasive, if we can shift the conversation away from “AI safety” toward responsible [use of] AI, we will speed up AI’s benefits and do a better job of addressing actual problems. That will actually make people safer.Keep building!AndrewA MESSAGE FROM DEEPLEARNING.AIUnderstand and implement the attention mechanism, a key element in transformer-based LLMs, using PyTorch. In this course, StatQuest’s Josh Starmer explains the core ideas behind attention mechanisms, the algorithm itself, and a step-by-step breakdown of how to implement them in PyTorch.Enroll nowNewsAgents Go DeepOpenAI introduced a state-of-the-art agent that produces research reports by scouring the web and reasoning over what it finds.What’s new:OpenAI’sdeep researchresponds to users’ requests by generating a detailed report based on hundreds of online sources. The system generates text output, with images and other media expected soon. Currently the agent is available only to subscribers to ChatGPT Pro, but the company plans to roll it out to users of ChatGPT Plus, Team, and Enterprise.How it works:Deep research is an agent that uses OpenAI’s o3 model, which is not yet publicly available. The model was trained via reinforcement learning to use a browser and Python tools, similar to the way o1 learned to reason from reinforcement learning. OpenAI has not yet released detailed information about how it built the system.The system responds best to detailed prompts that specify the desired output (such as the desired information, comparisons, and format), the team said in itsannouncement video(which features Mark Chen, Josh Tobin, Neel Ajjarapu, and Isa Fulford, co-instructor of our short courses “ChatGPT Prompt Engineering for Developers” and “Building Systems with the ChatGPT API”).Before answering, Deep research asks clarifying questions about the task.In the process of answering, the system presents a sidebar that summarizes the model’s chain of thought, terms it searched, websites it visited, and so on.The system can take as long as 30 minutes to provide output.Result: On abenchmarkof 3,000 multiple-choice and short-answer questions that cover subjects from ecology to rocket science, OpenAI deep research achieved 26.6 percent accuracy. In comparison, DeepSeek-R1 (without web browsing or other tool use) achieved 9.4 percent accuracy and o1 (also without tool use) achieved 9.1 percent accuracy. OnGAIA, questions that are designed to be difficult for large language models without access to additional tools, OpenAI deep research achieved 67.36 percent accuracy, exceeding theprevious state of the artof 63.64 percent accuracy.Behind the news:OpenAI’s deep research follows a similar offering of the same name by Google in December. A number of open source teams have built research agents that work in similar ways. Notable releases include aHugging Faceproject that attempted to replicate OpenAI’s work (not including training) in 24 hours (which achieved 55.15 percent accuracy on GAIA) andgpt-researcher, which implemented agentic web search in 2023, long before Google and OpenAI launched their agentic research systems.Why it matters:Reasoning models like o1 or o3 made a splash not just because they delivered superior results but also because of the impressive reasoning steps the model took to produce the results. Combining that ability with web search and tool use enables large language models to formulate better answers to difficult questions, including those whose answers aren’t in the training data or whose answers change over time.We’re thinking:Taking as much as 30 minutes of processing to render a response, OpenAI’s deep research clearly illustrates why we needmore compute for inference.Google Joins AI Peers In Military WorkGoogle revised its AI principles, reversing previous commitments to avoid work on weapons, surveillance, and other military applications beyond non-lethal uses like communications, logistics, and medicine.What’s new:Along with releasing its latestResponsible AI Progress Reportand an updated AIsafety framework, Google removed key restrictions from itsAI principles. The new version omits a section in the previous document titled “Applications we will not pursue.” The deleted textpledgedto avoid “technologies that cause or are likely to cause overall harm” and, where the technology risks doing harm, to “proceed only where we believe that the benefits substantially outweigh the risks” with “appropriate safety constraints.”How it works:Google’s AI principles no longer prohibit specific applications but promote developing the technology to improve scientific inquiry, national security, and the economy.The revised principles state that AI development should be led by democracies. The company argues that such leadership is needed given growing global competition in AI from countries that are not widely considered liberal democracies.The new principles stress “responsible development and deployment” to manage AI’s complexities and risks. They state that AI must be developed with safeguards at every stage, from design and testing to deployment and iteration, and those safeguards must adapt as technology and applications evolve.The revised principles also emphasize collaborative progress, stating that Google aims to learn from others and build AI that’s broadly useful across industries and society.Google emphasizes the need for “bold innovation,” stating that AI should be developed to assist, empower, and inspire people; drive economic progress; enable scientific breakthroughs; and help address global challenges. Examples includeAlphaFold 3, which figures out how biological molecules interact, a key factor in designing chemical processes that affect them.The revised principles are buttressed by the 2025 Responsible AI Progress Report. This documentoutlinesthe company’s efforts to evaluate risks through measures that align with the NIST AI Risk Management Framework includingred teaming, automated assessments, and input from independent experts.Behind the news:Google’s new stance reverses a commitment it made in 2018 after employeesprotestedits involvement inProject Maven, a Pentagon AI program for drone surveillance, from which Google ultimately withdrew. At the time, Google pledged not to develop AI applications for weapons or surveillance, which set it apart from Amazon and Microsoft. Since then, the company has expanded its work in defense,building ona $1.3 billion contract with Israel. In 2024,Anthropic, Meta, andOpenAIremoved their restrictions on military and defense applications, and Anthropic and OpenAIstrengthenedtheir ties with defense contractors such as Anduril and Palantir.Why it matters:Google’s shift in policy comes as AI is playing an increasing role in conflicts in Israel,Ukraine, and elsewhere, and while global geopolitical tensions are on the rise. While Google’s previous position kept it out of military AI development, defense contractors like Anduril, Northrop Grumman, and Palantir — not to mention AI-giant peers — stepped in. The new principles recognize the need for democratic countries to take the lead in developing technology and standards for its use as well as the massive business opportunity in military AI as governments worldwide seek new defense capabilities. Still, no widely acceptedglobal frameworkgoverns uses of AI in combat.We’re thinking:Knowing how and when to employ AI in warfare is one of the most difficult ethical questions of our time. Democratic nations have a right to defend themselves, and those of us who live in democracies have a responsibility to support fellow citizens who would put themselves in harm’s way to protect us. AI is transforming military strategy, and refusing to engage with it doesn’t make the risks go away.Alibaba’s Answer to DeepSeekWhile Hangzhou’s DeepSeek flexed its muscles, Chinese tech giant Alibaba vied for the spotlight with new open vision-language models.What’s new:Alibaba announcedQwen2.5-VL, a family of vision-language models (images and text in, text out) in sizes of 3 billion, 7 billion, and 72 billion parameters. The weights for all three models are available for download onHugging Face, each under a different license: Qwen2.5-VL-3B isfree for non-commercial uses, Qwen2.5-VL-7B isfree for commercial and noncommercial usesunder the Apache 2.0 license, and Qwen2.5-VL-72B isfree to developers that have less than 100 million monthly active users. You can try them out for free for a limited time inAlibaba Model Studio, and Qwen2.5-VL-72B is available via the model selector inQwen Chat.How it works:Qwen2.5-VL models accept up to 129,024 tokens of input according to thedeveloper reference(other sources provide conflicting numbers) and generate up to 8,192 tokens of output. Alibaba has not released details about how it trained them.Qwen2.5-VL comprises a vision encoder and large language model. It can parse videos, images, text, and is capable of computer use (desktop and mobile).The vision encoder accepts images of different sizes and represents them with different numbers of tokens depending on the size. For instance, one image might be 8 tokens and another 1125 tokens. This enabled the model to learn about the scale of images and to estimate the coordinates of objects in an image without rescaling.To reduce computation incurred by the vision encoder, the team replaced attention (which considers the entire input context) with windowed attention (which limits the input context to a window around a given token) and used full attention only in four layers. The resulting efficiency improves training and inference speeds.Results: Alibaba reports Qwen2.5-VL-72B’s performance on measures that span image and text problems, parsing documents, understanding videos, and interacting with computer programs. Across 21 benchmarks, it beat Microsoft Gemini 2.0 Flash, OpenAI GPT-4o, Anthropic Claude 3.5 Sonnet, and open competitors on 13 of them (where comparisons are  relevant and available).For example, on answering math questions about images inMathVista, Qwen2.5-VL-72B achieved 74.8 percent, while the closest competing model (Gemini 2.0 Flash) achieved 73.1 percent.InVideo-MME, which evaluates a model’s ability to answer questions about videos, Qwen 2.5 VL achieved 73.3 percent. GPT-4o achieved 71.9 percent andInternVL2.5, the next-best open competitor, achieved 72.1 percent.Used in an agentic workflow, Qwen2.5-VL-72B outperformed Claude 3.5 Sonnet when controlling Android devices and navigating desktop user interfaces. However, it finished second to other open vision-language models in several tests.More models:Alibaba also introduced competition for DeepSeek and a family of small models.Qwen2.5-Maxis a mixture-of-experts model that outperforms GPT-4o and DeepSeek-V3 on graduate-level science questions inGPQA-Diamondand regularly updated benchmarks likeArena-Hard,LiveBench, andLiveCodeBench. However, Qwen2.5-Max performed worse than o1 and DeepSeek-R1.Qwen2.5-1Mis a family of smaller language models (7 billion and 14 billion parameters) that accept up to 1 million tokens of input context.Why it matters:Vision-language models are getting more powerful and versatile. Not long ago, it was an impressive feat simply to answer questions about a chart or diagram that mixed graphics with text. Now such models are paired with an agent to control computers and smartphones. Broadly speaking, the Qwen2.5-VL models outperform open and closed competitors and they’re open to varying degrees (though the data is not available), giving developers a range of highly capable choices.We’re thinking:We’re happy Alibaba released a vision-language model that is broadly permissive with respect to commercial use (although we’d prefer that all sizes were available under a standard open weights license). We hope to see technical reports that illuminate Alibaba’s training and fine-tuning recipes.Tree Search for Web AgentsBrowsing the web to achieve a specific goal can be challenging for agents based on large language models and even for vision-language models that can process onscreen images of a browser. While some approaches address this difficulty in training the underlying model, the agent architecture can also make a difference.What’s new:Jing Yu Koh and colleagues at Carnegie Mellon University introducedtree search for language model agents, a method that allows agents to treat web interactions like tree searches. In this way, agents can explore possible chains of actions and avoid repeating mistakes.Key insight:Some web tasks, for instance finding a price of a particular item, require a chain of intermediate actions: navigating to the right page, scrolling to find the item, matching an image of the item to the image on the page, and so on. If an agent clicks the wrong link during this process, it might lose its way. The ability to evaluate possible actions and remember previous states of web pages can help an agent correct its mistakes and choose a chain of actions that achieves its goal.How it works:An agent based on GPT-4o attempted 200tasksusing website mockups that mimicked an online retail store, Reddit-like forum, and directory of classified ads. The tasks included ordering an item to be delivered to a given address, finding specific images on the forum, and posting an ad. The authors annotated each web page using the method calledSet of Mark, which identifies every visual element capable of interaction with a bounding box and a numerical ID.The agent started with a web page and an instruction such as, “Tell me the number of reviews our store received that mention the term ‘not useful.’” It passed an image of the page to the LLM, which predicted five actions that could make progress toward completing the task such as scrolling up or down, hovering over an element, clicking, typing in a text field, or opening a new URL.The agent executed the five actions. After each one, the LLM assessed the current state of the page using the previous states as context. The assessment assigned a value between 0 and 1 (meaning the task was complete). The agent kept a list of page states and their values.The agent selected the web page state with the highest value after executing the five actions, and repeated the process, making a new set of five predictions based on the highest-value state.This process is a search: The agent executed a chain of actions until the value of the new states dropped below the values of other states. If all new states had lower values, the agent backtracked to a previous state with a higher value and asked the LLM for five more actions. The search stopped when the agent had completed the task or explored 20 possible states.Results:The authors compared two agents, one that followed their search method and another that started at the same page and received the same instruction but took one action per state and never backtracked. The agents attempted 100 shopping tasks, 50 forum tasks, and 50 classified-ads tasks. The one equipped to search successfully completed 26.4 percent of the tasks, while the other agent completed 18.9 percent of the tasks.Why it matters:Search joins reflection, planning, tool use, and multi-agent collaboration as an emergingagentic design pattern. Following many branching paths of actions enables an agent to determine the most effective set of actions to accomplish a task.We’re thinking:Agentic design patterns are progressing quickly! In combination withcomputer use, this sort of search method may enable agents to execute a wide variety of desktop tasks.\nLoading theElevenlabs Text to SpeechAudioNative Player...\n\n\nShare\nShare\n\nShare\n",
    "image_urls": [
      "https://dl-staging-website.ghost.io/content/images/2025/02/RESPONSIBLE-AI_Blue-Or4_1200px--1--1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/02/The-Batch-ads-and-exclusive-banners--11-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/02/DEEPRESEARCH_600px_opt.gif",
      "https://dl-staging-website.ghost.io/content/images/2025/02/GOOGLEWEAPONS4c.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/02/unnamed--47-.gif",
      "https://dl-staging-website.ghost.io/content/images/2025/02/unnamed--48-.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2023/03/4.png"
    ]
  },
  {
    "title": "issue 301",
    "date": null,
    "url": "https://www.deeplearning.ai/the-batch/issue-301/",
    "content": "The Batch\nWeekly Issues\nissue 301\nPublishedMay 14, 2025\nPublished\n\nPublished\nMay 14, 2025\nReading time13min read\nReading time\n\nReading time\n13min read\nPublishedMay 14, 2025Reading time13min readShareLoading theElevenlabs Text to SpeechAudioNative Player...Dear friends,AI’s ability to make tasks not just cheaper, but also faster, is underrated in its importance in creating business value.For the task of writing code, AI is a game-changer. It takes so much less effort — and is so much cheaper — to write software with AI assistance than without. But beyond reducing the cost of writing software, AI is shortening the time from idea to working prototype, and the ability to test ideas faster is changing how teams explore and invent. When you can test 20 ideas per month, it dramatically changes what you can do compared to testing 1 idea per month. This is a benefit that comes from AI-enabledspeedrather than AI-enabled cost reduction.That AI-enabled automation can reduce costs is well understood. For example, providing automated customer service is cheaper than operating human-staffed call centers. Many businesses are more willing to invest in growth than just in cost savings; and, when a task becomes cheaper, some businesses will do a lot more of it, thus creating growth. But another recipe for growth is underrated: Making certain tasks much faster (whether or not they also become cheaper) can create significant new value.I see this pattern across more and more businesses. Consider the following scenarios:If a lender can approve loans in minutes using AI, rather than days waiting for a human to review them, this creates more borrowing opportunities (and also lets the lender deploy its capital faster). Even if human-in-the-loop review is needed, using AI to get the most important information to the reviewer might speed things up. The ability to provide loans quickly opens up the market to new customers in need of rapid funds and helps customers who need a quick positive or negative decision to accept the loan or move on.If an academic institution gives homework feedback to students in minutes (via sophisticated autograding) rather than days (via human grading), not only is the automation cheaper, the rapid feedback facilitates better learning.If an online seller can approve purchases faster, this can lead to more sales. For example, many platforms that accept online ad purchases have an approval process that can take hours or days; if approvals can be done faster, they can earn revenue faster. Further, for customers buying ads, being able to post an ad in minutes lets them test ideas faster and also makes the ad product more valuable.If a company’s sales department can prioritize leads and respond to prospective customers in minutes or hours rather than days — closer to when the customers’ buying intent first led them to contact the company — sales representatives might close more deals. Likewise, a business that can respond more quickly to requests for proposals may win more deals.I’ve written previously about looking at thetasksa company does to explore where AI can help. Many teams already do this with an eye toward making tasks cheaper, either to save costs or to do those tasks many more times. If you’re doing this exercise, consider also whether AI can significantly speed up certain tasks. One place to examine is the sequence of tasks on the path to earning revenue. If some of the steps can be sped up, perhaps this can help revenue growth.Growth is more interesting to most businesses than cost savings, and if there are loops in your business that, when sped up, would drive growth, AI might be a tool to unlock this growth.Keep building!AndrewA MESSAGE FROM DEEPLEARNING.AIIn Course 4 of theData Analytics Professional Certificateyou’ll work with truly real-world data: messy, inconsistent, and often unstructured. You’ll extract data from websites, APIs, and databases, and clean it using Python and SQL. By the end, you’ll be able to make raw datasets analysis-ready, with speed and accuracy.Enroll today!NewsReasoning Models With RecipesMicrosoft published its latest recipe for training reasoning models, substantially expanding what is still a fairly small base of public knowledge.What’s new:Microsoft releasedPhi-4-reasoning, Phi-4-reasoning-plusandPhi-4-mini-reasoningalong with lessons learned in building the models.Input/output: text in (Phi-4-reasoning up to 32,000 tokens, Phi–4-reasoning-plus up to 32,000 tokens, Phi-4-mini-reasoning up to 128,000 tokens), text outArchitecture: Transformer (Phi-4-reasoning 14 billion parameters, Phi-4-reasoning-plus 14 billion parameters, Phi-4-mini-reasoning: 3.8 billion parameters)Features: ReasoningPerformance: Phi-4-reasoning-plus and Phi-4-mini-reasoning perform well on math problemsAvailability: Weights free todownloadfor noncommercial and commercial uses under anMIT licenseHow it works:All three models are fine-tuned versions of pretrained models.Phi-4-reasoning: The authors fine-tunedPhi-4tomatch curated outputsfromOpenAI o3-minion Q&A, math, science, and coding examples.Phi-4-reasoning-plus: They further fine-tuned Phi-4-reasoning via reinforcement learning to correctly answer math problems.Phi-4-mini-reasoning: They fine-tuned Phi-4-mini in stages to reason over math problems. Stages included (i) supervised fine-tuning to match correct output from DeepSeek-R1, (ii) direct preference optimization to train the model to prefer correct responses over incorrect ones from DeepSeek-R1, and (iii) reinforcement learning to further reward correct solutions to math problems.Smaller model lessons learned:During reinforcement learning, Phi-4-mini-reasoning exhibited instability, such as output batches that varied greatly in length or received mostly negative rewards, apparently depending on the training data or output. The authors suspect that the model’s small size caused these issues. Among the lessons learned:Supervised fine-tuning on existing reasoning datasets likeS1Kcan decrease performance. This phenomenon suggests a need either for larger, high-quality, supervised fine-tuning datasets or for fine-tuning via both supervised learning and reinforcement learning.To minimize discrepancies in output length, the authors tested multiple prompts and chose those that resulted in the most uniform output lengths.To address the output batches that received mostly negative rewards, they sampled lots of responses, retained those that received a positive reward, sampled an equal number of those that received a negative reward, and discarded the rest before adjusting the model’s weights.Larger model lessons learned:Phi-4-reasoning and Phi-4-reasoning-plus didn’t present the same issues. However, the authors did make significant choices during reinforcement learning:They fine-tuned Phi-4-reasoning on both math and code data, but during reinforcement learning, they fine-tuned it only on math data to simplify the training process. The authors attribute the model’s relatively lackluster performance on code benchmarks to this choice.They crafted the reward function to give lower rewards for correct responses longer than 25,600 tokens than for shorter responses. This encouraged the model to finish thinking within the input length. Furthermore, the reward function gave a greater punishment for incorrect responses with fewer than 3,702 tokens compared to longer responses. This encouraged the model to produce more reasoning tokens when solving hard problems.Results:Overall, Phi-4-reasoning-plus and Phi-4-mini-reasoning outperform similarly sized (and larger) open-weights models on math problems. Phi-4-reasoning generally outperformed DeepSeek-R1-Distilled-70B but underperformed Alibaba QwQ 32B. All three models deliver performance that falls in the middle among proprietary models and, in domains outside math, larger models with open weights.On math problems in AIME 2024, Phi-4-reasoning-plus (81.3 percent accuracy) outperformed the next-best open-weights model, QwQ 32B (79.5 percent accuracy). In comparison, Phi-4-reasoning (74.6 percent accuracy) underperformed the proprietary Gemini 2.5 Pro (92 percent accuracy).On AIME 2024, Phi-4-mini-reasoning (57.5 percent accuracy) outperformed the next-best open-weights model of similar size, DeepSeek-R1-Distill-Qwen-7B (53.3 percent accuracy). In comparison, o1-mini achieved 63.6 percent accuracy.Why it matters:While reasoning models can outperform their non-reasoning counterparts, the best ways to train them aren’t widely known. Sharing recipes and lessons learned enables others to further iterate and improve the recipes, ultimately increasing model performance even more.Open, Compact Code GeneratorAn open-source code generator performs comparably to the reasoning models DeepSeek-R1 and OpenAI o1 with a much smaller model.What’s new:A team at the model platform Together.AI and Agentica, an open-source project devoted to reinforcement learning (RL), releasedDeepCoder-14B-Preview. The release includesweights, code, dataset, training logs, and data optimizationsunder an MITlicensethat allows noncommercial and commercial uses.How it works:The team fine-tuned DeepSeek-R1-Distilled-Qwen-14B, which distills knowledge from DeepSeek-R1 (671 billion parameters) into Qwen-14B (14 billion parameters).The authors curated 24,000 coding problems fromTACO Verified,SYNTHETIC-1, andLiveCodeBench). They removed duplicates, problems with less than five unit tests, problems whose solutions failed to pass all associated unit tests, and those that appeared in both test and training sets.They fine-tuned DeepSeek-R1-Distilled-Qwen-14B using a streamlined reinforcement learning approach that enhancedGroup Relative Policy Optimization(GPRO) with training optimizations fromDecoupled Clip and Dynamic Sampling Policy Optimization(DAPO). Among other optimizations, they (i) removed the KL loss (typically used to keep the new model’s outputs from straying too far from the base model’s outputs), which eliminated the need to compute the base model’s output at each training step, and (ii) ignored the loss for outputs that exceeded the output size limit (16,000 tokens for the first training phase, 32,000 tokens for the second), which kept the model from being penalized for generating programs that didn’t work properly because they had been truncated.The authors updated the reinforcement learning libraryverlto improve the way the model parallelized sampling, computing the reward, and training. Instead of alternating between sampling new outputs, computing rewards, and training (as verl does), they sampled new outputs while training on the previous batch. (They computed the reward immediately after sampling a new output.) For coding problems, this cut total training time in half.To prevent the model from developing behaviors based on flaws in the reward model, the reward model dispensed rewards only when DeepCoder-14B-Preview’s output passed all 15 of a problem's most challenging unit tests (judged by input length) within 6 to 12 seconds. Otherwise, the model received no reward.Results:DeepCoder-14B-Preview is competitive on several coding benchmarks with DeepSeek-R1 as well as proprietary models including OpenAI o3-mini and OpenAI o1, which is believed to be much larger.On LiveCodeBench (regularly updated coding problems), DeepCoder-14B-Preview (60.6 percent Pass@1 accuracy) was just shy of o3-mini-2025-1-31 set to low effort (60.9 percent) and slightly ahead of o1-2024-12-17 set to low effort (59.5 percent).On Codeforces (competitive coding problems), DeepCoder-14B-Preview (1936CodeElo, higher is better) performed significantly better than DeepSeek-R1-Distill-Qwen-14B (1791 CodeElo). It performed comparably to o3-mini-2025-1-31 set to low effort (1918 CodeElo), o1-2024-12-17 set to low effort (1991 CodeElo), and Deepseek-R1 (1948 CodeElo).Why it matters:Applying reinforcement learning to coding works, but it has two big issues: (i) Training examples of verifiable code are relatively scarce and (ii) computing reward signals for code is time-consuming, since it requires evaluating many test cases. DeepCoder-14B-Preview’s optimizations reduced this complexity, shrinking RL training from months to weeks. Those optimizations are built intoVerl-pipeline, an open source RL library from Together.AI and Agentica, giving developers a powerful tool for model training.We’re thinking:Kudos to the DeepCoder team for open sourcing their reasoning recipe! A handful of companies have developed the know-how to execute RL well, but many teams still have trouble implementing successfully. Open recipes for RL training methods and data curation techniques are important to move the field forward.EU Loosens AI RegulationsThe European Union made an abrupt U-turn away from its stringent AI regulations. Meta promptly adjusted to the loosening restrictions.What’s new:Henna Virkkunen, the EU’s head of digital policy, said the organization wouldeaserules and requirements to support Europe’s competitiveness in AI.How it works:Adopted last year, the EU’sAI Actprovides a comprehensive framework for regulating AI that aims to reduce purported risks by banning certain applications, restricting others, and requiring extensive documentation of development efforts. The law is set to take effect in August, empowering various regulatory bodies to formulate detailed rules. However, in recent months, the EU has faced increasing pressure from the U.S. government and large AI companies to reduce the regulatory burden.Virkkunen announced the EU wouldwithdrawa provision that allowed citizens to sue AI companies for damages caused by their systems and required extensive reporting and disclosure.Sheadvocatedadjusting the regulations to make the EU more competitive and independent. “When we want to boost investments in AI, we have to make sure that we have an environment that is faster and simpler than the European Union is right now,” hesaid.Criticsaccusedregulators of defanging the AI Act to appease U.S. AI companies and the Trump administration, which hasarguedthat the AI Act is an excessive barrier to innovation. Virkkunen denied bowing to U.S. pressure.Meta responded to the shifting regulatory environment byresumingtraining its models on European data. Last year, the companystoppedreleasing multimodal models in Europe after EU regulators warned that training models on data from European users of Facebook, Instagram, and other Meta properties potentially violated privacy laws.Behind the news:In drafting the AI Act, the EU aspired to a comprehensive, specific set of regulations. However, not all European lawmakers agreed that rules were needed. Virkkunen’s supporters noted that existing laws already allowed consumers to file claims against AI companies. Meanwhile, some policymakers havebecome less worriedabout AI than they were during the early drafting of the AI Act.Why it matters:It’s unlikely that all nations – or evenstateswithin nations – will ever agree fully on rules and regulations that govern AI companies that do business within their borders, or protections from flaws such as model bias. But AI companies including Meta,OpenAI, andothersargue that a more uniform regulatory environment will make it easier to serve users worldwide.We’re thinking:The EU overreached with the AI Act. Fortunately, the legislation provides enough flexibility to pull back. Clearer rules will help European teams innovate and European and international companies better serve EU citizens.Memory Layers for More-Factual OutputImproving a large language model’s factual accuracy typically requires making it bigger, which in turn, involves more computation. Researchers devised an architecture that enables models to recall relevant details without significantly increasing the amount of computation required.What’s new:Vincent-Pierre Berges, Barlas Oğuz, and colleagues at Meta augmented transformers with trainablememory layersthat efficiently store and retrieve information related to a prompt. Thetraining codeis available under a CC BY-NClicense, which permits noncommercial uses.Memory layer basics:Memory layers wereintroducedin 2015 and wereapplied to transformersa few years later. They compute vectors, which may capture details like names or dates that were learned through training, and retrieve them according to a given input. Computing the output of a memory layer is similar to computing that of a self-attention layer. Both describe vectors that represent queries, keys, and values, and both compute the similarity between queries and keys and then weight the values by that similarity. However, while a self-attention layer computes queries, keys, and values from linear transformations of the input, a memory layer (which computes queries the same way) learns keys and a corresponding value for each key through training.Key insight:Memory layers can be scaled to millions of keys, but computing the similarity between a query and so many keys is computationally expensive. One solution is to represent each key as a combination of two half-keys drawn from two much smaller sets. For example, two sets of 1,000 half-keys each can represent 1 million possible keys. Comparing a query to these smaller sets is much more efficient, making it practical to scale up memory layers dramatically.How it works:The authors pretrained Llama-style models of several sizes (from 134 million to 8 billion parameters) on data similar to Llama 2’s and Llama 3’s pretraining datasets. They replaced the fully connected layers with memory layers in three transformer blocks. These layers shared parameters and held up to 16 million values (an extra 64 billion parameters total). The memory layers performed these steps:Given a query (a prompt that has been embedded by preceding transformer layers), split it into two vectors half the size.Compute similarity scores between each half-query to and each half-key drawn from two sets of half keys. Identify thekhighest-scoring half-keys.Concatenate the highest-scoring half keys to producek2full keys.Sum the similarity scores of the two half keys that make up each full key. Choose the k highest-scoring full keys.Compute the index of each full key based on the indices of the corresponding half-keys.Retrieve the values that correspond to the full keys.Output the summed values weighted by the similarity scores.Results:The authors compared a model (8 billion parameters) with memory layers to a similar model without memory layers, both trained on 1 trillion tokens.They used nine question-answering datasets for evaluation. The model with memory layers achieved higher performance on seven of them. For example, onMMLU, the memory model achieved 63.04 percent accuracy, while the unmodified transformer achieved 59.68 percent accuracy.In general, the memory model performed worse than Llama 3.1 8B trained on 15 trillion tokens. For example, Llama 3.1 8B achieved 66 percent accuracy on MMLU.Why it matters:Memory layers didn’t catch on in the early days of large language models (LLMs), but they can improve the output of today’s much bigger models. LLMs outfitted with memory layers require less data and computation for pretraining than conventional models to achieve the same result, at least with respect to answering factual questions.We’re thinking:While retrieval-augmented generation can help LLMs deliver more-factual output by retrieving facts from a database, the authors add trainable parameters for this purpose.A MESSAGE FROM DEEPLEARNING.AIBuild AI applications that access tools, data, and prompt templates using Model Context Protocol (MCP), an open standard developed by Anthropic. In “MCP: Build Rich-Context AI Apps with Anthropic,” you’ll build and deploy an MCP server, make an MCP-compatible chatbot, and connect applications to multiple third-party servers.Sign up nowShare\nPublishedMay 14, 2025\nPublished\n\nPublished\nMay 14, 2025\nMay 14, 2025\nReading time13min read\nReading time\n\nReading time\n13min read\nShare\nShare\n\nShare\n\nLoading theElevenlabs Text to SpeechAudioNative Player...Dear friends,AI’s ability to make tasks not just cheaper, but also faster, is underrated in its importance in creating business value.For the task of writing code, AI is a game-changer. It takes so much less effort — and is so much cheaper — to write software with AI assistance than without. But beyond reducing the cost of writing software, AI is shortening the time from idea to working prototype, and the ability to test ideas faster is changing how teams explore and invent. When you can test 20 ideas per month, it dramatically changes what you can do compared to testing 1 idea per month. This is a benefit that comes from AI-enabledspeedrather than AI-enabled cost reduction.That AI-enabled automation can reduce costs is well understood. For example, providing automated customer service is cheaper than operating human-staffed call centers. Many businesses are more willing to invest in growth than just in cost savings; and, when a task becomes cheaper, some businesses will do a lot more of it, thus creating growth. But another recipe for growth is underrated: Making certain tasks much faster (whether or not they also become cheaper) can create significant new value.I see this pattern across more and more businesses. Consider the following scenarios:If a lender can approve loans in minutes using AI, rather than days waiting for a human to review them, this creates more borrowing opportunities (and also lets the lender deploy its capital faster). Even if human-in-the-loop review is needed, using AI to get the most important information to the reviewer might speed things up. The ability to provide loans quickly opens up the market to new customers in need of rapid funds and helps customers who need a quick positive or negative decision to accept the loan or move on.If an academic institution gives homework feedback to students in minutes (via sophisticated autograding) rather than days (via human grading), not only is the automation cheaper, the rapid feedback facilitates better learning.If an online seller can approve purchases faster, this can lead to more sales. For example, many platforms that accept online ad purchases have an approval process that can take hours or days; if approvals can be done faster, they can earn revenue faster. Further, for customers buying ads, being able to post an ad in minutes lets them test ideas faster and also makes the ad product more valuable.If a company’s sales department can prioritize leads and respond to prospective customers in minutes or hours rather than days — closer to when the customers’ buying intent first led them to contact the company — sales representatives might close more deals. Likewise, a business that can respond more quickly to requests for proposals may win more deals.I’ve written previously about looking at thetasksa company does to explore where AI can help. Many teams already do this with an eye toward making tasks cheaper, either to save costs or to do those tasks many more times. If you’re doing this exercise, consider also whether AI can significantly speed up certain tasks. One place to examine is the sequence of tasks on the path to earning revenue. If some of the steps can be sped up, perhaps this can help revenue growth.Growth is more interesting to most businesses than cost savings, and if there are loops in your business that, when sped up, would drive growth, AI might be a tool to unlock this growth.Keep building!AndrewA MESSAGE FROM DEEPLEARNING.AIIn Course 4 of theData Analytics Professional Certificateyou’ll work with truly real-world data: messy, inconsistent, and often unstructured. You’ll extract data from websites, APIs, and databases, and clean it using Python and SQL. By the end, you’ll be able to make raw datasets analysis-ready, with speed and accuracy.Enroll today!NewsReasoning Models With RecipesMicrosoft published its latest recipe for training reasoning models, substantially expanding what is still a fairly small base of public knowledge.What’s new:Microsoft releasedPhi-4-reasoning, Phi-4-reasoning-plusandPhi-4-mini-reasoningalong with lessons learned in building the models.Input/output: text in (Phi-4-reasoning up to 32,000 tokens, Phi–4-reasoning-plus up to 32,000 tokens, Phi-4-mini-reasoning up to 128,000 tokens), text outArchitecture: Transformer (Phi-4-reasoning 14 billion parameters, Phi-4-reasoning-plus 14 billion parameters, Phi-4-mini-reasoning: 3.8 billion parameters)Features: ReasoningPerformance: Phi-4-reasoning-plus and Phi-4-mini-reasoning perform well on math problemsAvailability: Weights free todownloadfor noncommercial and commercial uses under anMIT licenseHow it works:All three models are fine-tuned versions of pretrained models.Phi-4-reasoning: The authors fine-tunedPhi-4tomatch curated outputsfromOpenAI o3-minion Q&A, math, science, and coding examples.Phi-4-reasoning-plus: They further fine-tuned Phi-4-reasoning via reinforcement learning to correctly answer math problems.Phi-4-mini-reasoning: They fine-tuned Phi-4-mini in stages to reason over math problems. Stages included (i) supervised fine-tuning to match correct output from DeepSeek-R1, (ii) direct preference optimization to train the model to prefer correct responses over incorrect ones from DeepSeek-R1, and (iii) reinforcement learning to further reward correct solutions to math problems.Smaller model lessons learned:During reinforcement learning, Phi-4-mini-reasoning exhibited instability, such as output batches that varied greatly in length or received mostly negative rewards, apparently depending on the training data or output. The authors suspect that the model’s small size caused these issues. Among the lessons learned:Supervised fine-tuning on existing reasoning datasets likeS1Kcan decrease performance. This phenomenon suggests a need either for larger, high-quality, supervised fine-tuning datasets or for fine-tuning via both supervised learning and reinforcement learning.To minimize discrepancies in output length, the authors tested multiple prompts and chose those that resulted in the most uniform output lengths.To address the output batches that received mostly negative rewards, they sampled lots of responses, retained those that received a positive reward, sampled an equal number of those that received a negative reward, and discarded the rest before adjusting the model’s weights.Larger model lessons learned:Phi-4-reasoning and Phi-4-reasoning-plus didn’t present the same issues. However, the authors did make significant choices during reinforcement learning:They fine-tuned Phi-4-reasoning on both math and code data, but during reinforcement learning, they fine-tuned it only on math data to simplify the training process. The authors attribute the model’s relatively lackluster performance on code benchmarks to this choice.They crafted the reward function to give lower rewards for correct responses longer than 25,600 tokens than for shorter responses. This encouraged the model to finish thinking within the input length. Furthermore, the reward function gave a greater punishment for incorrect responses with fewer than 3,702 tokens compared to longer responses. This encouraged the model to produce more reasoning tokens when solving hard problems.Results:Overall, Phi-4-reasoning-plus and Phi-4-mini-reasoning outperform similarly sized (and larger) open-weights models on math problems. Phi-4-reasoning generally outperformed DeepSeek-R1-Distilled-70B but underperformed Alibaba QwQ 32B. All three models deliver performance that falls in the middle among proprietary models and, in domains outside math, larger models with open weights.On math problems in AIME 2024, Phi-4-reasoning-plus (81.3 percent accuracy) outperformed the next-best open-weights model, QwQ 32B (79.5 percent accuracy). In comparison, Phi-4-reasoning (74.6 percent accuracy) underperformed the proprietary Gemini 2.5 Pro (92 percent accuracy).On AIME 2024, Phi-4-mini-reasoning (57.5 percent accuracy) outperformed the next-best open-weights model of similar size, DeepSeek-R1-Distill-Qwen-7B (53.3 percent accuracy). In comparison, o1-mini achieved 63.6 percent accuracy.Why it matters:While reasoning models can outperform their non-reasoning counterparts, the best ways to train them aren’t widely known. Sharing recipes and lessons learned enables others to further iterate and improve the recipes, ultimately increasing model performance even more.Open, Compact Code GeneratorAn open-source code generator performs comparably to the reasoning models DeepSeek-R1 and OpenAI o1 with a much smaller model.What’s new:A team at the model platform Together.AI and Agentica, an open-source project devoted to reinforcement learning (RL), releasedDeepCoder-14B-Preview. The release includesweights, code, dataset, training logs, and data optimizationsunder an MITlicensethat allows noncommercial and commercial uses.How it works:The team fine-tuned DeepSeek-R1-Distilled-Qwen-14B, which distills knowledge from DeepSeek-R1 (671 billion parameters) into Qwen-14B (14 billion parameters).The authors curated 24,000 coding problems fromTACO Verified,SYNTHETIC-1, andLiveCodeBench). They removed duplicates, problems with less than five unit tests, problems whose solutions failed to pass all associated unit tests, and those that appeared in both test and training sets.They fine-tuned DeepSeek-R1-Distilled-Qwen-14B using a streamlined reinforcement learning approach that enhancedGroup Relative Policy Optimization(GPRO) with training optimizations fromDecoupled Clip and Dynamic Sampling Policy Optimization(DAPO). Among other optimizations, they (i) removed the KL loss (typically used to keep the new model’s outputs from straying too far from the base model’s outputs), which eliminated the need to compute the base model’s output at each training step, and (ii) ignored the loss for outputs that exceeded the output size limit (16,000 tokens for the first training phase, 32,000 tokens for the second), which kept the model from being penalized for generating programs that didn’t work properly because they had been truncated.The authors updated the reinforcement learning libraryverlto improve the way the model parallelized sampling, computing the reward, and training. Instead of alternating between sampling new outputs, computing rewards, and training (as verl does), they sampled new outputs while training on the previous batch. (They computed the reward immediately after sampling a new output.) For coding problems, this cut total training time in half.To prevent the model from developing behaviors based on flaws in the reward model, the reward model dispensed rewards only when DeepCoder-14B-Preview’s output passed all 15 of a problem's most challenging unit tests (judged by input length) within 6 to 12 seconds. Otherwise, the model received no reward.Results:DeepCoder-14B-Preview is competitive on several coding benchmarks with DeepSeek-R1 as well as proprietary models including OpenAI o3-mini and OpenAI o1, which is believed to be much larger.On LiveCodeBench (regularly updated coding problems), DeepCoder-14B-Preview (60.6 percent Pass@1 accuracy) was just shy of o3-mini-2025-1-31 set to low effort (60.9 percent) and slightly ahead of o1-2024-12-17 set to low effort (59.5 percent).On Codeforces (competitive coding problems), DeepCoder-14B-Preview (1936CodeElo, higher is better) performed significantly better than DeepSeek-R1-Distill-Qwen-14B (1791 CodeElo). It performed comparably to o3-mini-2025-1-31 set to low effort (1918 CodeElo), o1-2024-12-17 set to low effort (1991 CodeElo), and Deepseek-R1 (1948 CodeElo).Why it matters:Applying reinforcement learning to coding works, but it has two big issues: (i) Training examples of verifiable code are relatively scarce and (ii) computing reward signals for code is time-consuming, since it requires evaluating many test cases. DeepCoder-14B-Preview’s optimizations reduced this complexity, shrinking RL training from months to weeks. Those optimizations are built intoVerl-pipeline, an open source RL library from Together.AI and Agentica, giving developers a powerful tool for model training.We’re thinking:Kudos to the DeepCoder team for open sourcing their reasoning recipe! A handful of companies have developed the know-how to execute RL well, but many teams still have trouble implementing successfully. Open recipes for RL training methods and data curation techniques are important to move the field forward.EU Loosens AI RegulationsThe European Union made an abrupt U-turn away from its stringent AI regulations. Meta promptly adjusted to the loosening restrictions.What’s new:Henna Virkkunen, the EU’s head of digital policy, said the organization wouldeaserules and requirements to support Europe’s competitiveness in AI.How it works:Adopted last year, the EU’sAI Actprovides a comprehensive framework for regulating AI that aims to reduce purported risks by banning certain applications, restricting others, and requiring extensive documentation of development efforts. The law is set to take effect in August, empowering various regulatory bodies to formulate detailed rules. However, in recent months, the EU has faced increasing pressure from the U.S. government and large AI companies to reduce the regulatory burden.Virkkunen announced the EU wouldwithdrawa provision that allowed citizens to sue AI companies for damages caused by their systems and required extensive reporting and disclosure.Sheadvocatedadjusting the regulations to make the EU more competitive and independent. “When we want to boost investments in AI, we have to make sure that we have an environment that is faster and simpler than the European Union is right now,” hesaid.Criticsaccusedregulators of defanging the AI Act to appease U.S. AI companies and the Trump administration, which hasarguedthat the AI Act is an excessive barrier to innovation. Virkkunen denied bowing to U.S. pressure.Meta responded to the shifting regulatory environment byresumingtraining its models on European data. Last year, the companystoppedreleasing multimodal models in Europe after EU regulators warned that training models on data from European users of Facebook, Instagram, and other Meta properties potentially violated privacy laws.Behind the news:In drafting the AI Act, the EU aspired to a comprehensive, specific set of regulations. However, not all European lawmakers agreed that rules were needed. Virkkunen’s supporters noted that existing laws already allowed consumers to file claims against AI companies. Meanwhile, some policymakers havebecome less worriedabout AI than they were during the early drafting of the AI Act.Why it matters:It’s unlikely that all nations – or evenstateswithin nations – will ever agree fully on rules and regulations that govern AI companies that do business within their borders, or protections from flaws such as model bias. But AI companies including Meta,OpenAI, andothersargue that a more uniform regulatory environment will make it easier to serve users worldwide.We’re thinking:The EU overreached with the AI Act. Fortunately, the legislation provides enough flexibility to pull back. Clearer rules will help European teams innovate and European and international companies better serve EU citizens.Memory Layers for More-Factual OutputImproving a large language model’s factual accuracy typically requires making it bigger, which in turn, involves more computation. Researchers devised an architecture that enables models to recall relevant details without significantly increasing the amount of computation required.What’s new:Vincent-Pierre Berges, Barlas Oğuz, and colleagues at Meta augmented transformers with trainablememory layersthat efficiently store and retrieve information related to a prompt. Thetraining codeis available under a CC BY-NClicense, which permits noncommercial uses.Memory layer basics:Memory layers wereintroducedin 2015 and wereapplied to transformersa few years later. They compute vectors, which may capture details like names or dates that were learned through training, and retrieve them according to a given input. Computing the output of a memory layer is similar to computing that of a self-attention layer. Both describe vectors that represent queries, keys, and values, and both compute the similarity between queries and keys and then weight the values by that similarity. However, while a self-attention layer computes queries, keys, and values from linear transformations of the input, a memory layer (which computes queries the same way) learns keys and a corresponding value for each key through training.Key insight:Memory layers can be scaled to millions of keys, but computing the similarity between a query and so many keys is computationally expensive. One solution is to represent each key as a combination of two half-keys drawn from two much smaller sets. For example, two sets of 1,000 half-keys each can represent 1 million possible keys. Comparing a query to these smaller sets is much more efficient, making it practical to scale up memory layers dramatically.How it works:The authors pretrained Llama-style models of several sizes (from 134 million to 8 billion parameters) on data similar to Llama 2’s and Llama 3’s pretraining datasets. They replaced the fully connected layers with memory layers in three transformer blocks. These layers shared parameters and held up to 16 million values (an extra 64 billion parameters total). The memory layers performed these steps:Given a query (a prompt that has been embedded by preceding transformer layers), split it into two vectors half the size.Compute similarity scores between each half-query to and each half-key drawn from two sets of half keys. Identify thekhighest-scoring half-keys.Concatenate the highest-scoring half keys to producek2full keys.Sum the similarity scores of the two half keys that make up each full key. Choose the k highest-scoring full keys.Compute the index of each full key based on the indices of the corresponding half-keys.Retrieve the values that correspond to the full keys.Output the summed values weighted by the similarity scores.Results:The authors compared a model (8 billion parameters) with memory layers to a similar model without memory layers, both trained on 1 trillion tokens.They used nine question-answering datasets for evaluation. The model with memory layers achieved higher performance on seven of them. For example, onMMLU, the memory model achieved 63.04 percent accuracy, while the unmodified transformer achieved 59.68 percent accuracy.In general, the memory model performed worse than Llama 3.1 8B trained on 15 trillion tokens. For example, Llama 3.1 8B achieved 66 percent accuracy on MMLU.Why it matters:Memory layers didn’t catch on in the early days of large language models (LLMs), but they can improve the output of today’s much bigger models. LLMs outfitted with memory layers require less data and computation for pretraining than conventional models to achieve the same result, at least with respect to answering factual questions.We’re thinking:While retrieval-augmented generation can help LLMs deliver more-factual output by retrieving facts from a database, the authors add trainable parameters for this purpose.A MESSAGE FROM DEEPLEARNING.AIBuild AI applications that access tools, data, and prompt templates using Model Context Protocol (MCP), an open standard developed by Anthropic. In “MCP: Build Rich-Context AI Apps with Anthropic,” you’ll build and deploy an MCP server, make an MCP-compatible chatbot, and connect applications to multiple third-party servers.Sign up now\nLoading theElevenlabs Text to SpeechAudioNative Player...\n\n\nShare\nShare\n\nShare\n",
    "image_urls": [
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--88-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/05/The-Batch-ads-and-exclusive-banners---2025-05-13T182227.067.png",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--89-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--90-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--63--1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--91-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/05/The-Batch-ads-and-exclusive-banners---2025-05-13T182128.677.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2024/10/DE-Vertical-2.png"
    ]
  }
]